{
  
    
        "post0": {
            "title": "Generalizing Attention in NLP and Understanding Self-Attention",
            "content": "Introduction . Attention is one of the most important and ubiquitous concepts in NLP and deep learning. There are many blog posts out there explaining different flavors of attention. This blog post however, introduces attention as a general concept in NLP and then goes onto explain some of the most important methods of calculating attention vectors. Each sub-topic or concept is concretely supported by real-world intuitions. Lastly, this post explains the idea of multiheaded self-attention in a unique by using the idea of linear projections and concludes by showing stepwise how to translate all the intuitions to code. . This blog post is a small excerpt from my work on paper-annotations for the task of question answering. This repo contains a collection of important question-answering papers, implemented from scratch in pytorch with detailed explanation of various concepts/components introduced in the respective papers. . Introduction to Attention in NLP . Attention as a concept in NLP was introduced in 2015 by Bahdanau et al. to improve neural machine translation systems. Before this paper, NMT systems were largely based on seq2seq architectures which had an encoder to encode a representation of the source language and a decoder which to decode this representation into the target language. Such models were trained on large quantities of parallel text data of two languages. One major drawback of this architecture was that it didn&#39;t work well for longer documents/sequences. This is because the entire information in the source sentence was being crammed into a single vector. If this vector fails to capture the important information from the source language, the system is going to perform poorly. . When we claim that these neural nets mimic the human brain, this is certainly not how the human brain works. While learning about some topic, we do not simply read 2-3 pages of content and expect our brain to remember all the details in the first go. We usually revisit various concepts, recollect and refer to the material back and forth before mastering it. The attention mechanism in NMT was designed to do this. While decoding at any particular time step, encoder hidden states from all the time-steps are made available to the decoder. The decoder then can look back at the encoder hidden states or the source language and make a more informed prediction at a particular time-step. This alieviates the problem of all the information from source language being crammed into a single vector. To illustrate this with equations, consider that the hidden states of the encoder RNN are represented by $H$ = {$h_{1}, h_{2}, h_{3},...,h_{t}$}. While decoding the token at position $t$, the input to the decoder unit is hidden state from previous unit $s_{t-1}$ and an attention vector which is a selective summary of the encoder hidden states and helps the decoder to pay more attention to a particular encoder state. The similarity between the encoder hidden states $H$ and the decoder hidden state so far $s_{t-1}$ is computed by, $$ alpha = tanh (W [H ; s_{t-1}]) $$ . $ alpha$ is then passed through a softmax layer to obtain attention distribution such that $ sum_{t} alpha_{t}$ = 1. The final step is calculating the attention vector by taking a weighted sum of the encoder hidden states, $$ sum_{t} alpha_{t} h_{t} $$ . The following diagram illustrates this process. . . Since then, many different forms of attention have been proposed and used in the literature. Attention is not limited to NMT systems and has evolved into a more general concept in NLP. At the heart of it attention is about summarizing a particular entity/representation by attending to the important parts of this representation. A more general definition of attention is as follows: . Given a set of vectors values, and a single vector query, attention is a method to calculate a weighted sum of the values, where the query determines which values to focus on. . It is a way to obtain a fixed size representation of an arbitrary set of representations (values), dependent on some other representation (query). . In our earlier NMT example, the encoder hidden states {$h_{1}, h_{2}, h_{3},...,h_{t}$} are the values and the decoder hidden state $s_{t-1}$ is the query. . Generalizing Attention . In general there are 3 steps when calculating the attention. Consider that values are represented by {$h_{1}, h_{2}, h_{3},..h_{n}$} and query is $s$. Then attention always involves, . Calculating the energy $e$ or attention scores between these 2 vectors, $e$ $ epsilon$ $ R^{N} $ | Taking softmax to get an attention distribution $ alpha$, $ alpha$ $ epsilon$ $R^{N}$ | $$ alpha = softmax(e)$$ $$ sum_{t}^{N} alpha_{t} = 1 $$ . Taking the weighted sum of the values by using $ alpha$ $$ a = sum_{t}^{N} alpha_{t}h_{t} $$ | Now there are different ways to calculate the energy between query and values. . Basic Dot Product Attention . $$ e_{t} = s^{T}h_{t}$$ . Additive Attention . $$ e_{t} = v^{T} tanh (W [h_{t};s])$$ This is nothing but the Bahdanau attention attention first proposed for NMT systems. . Bilinear Attention . $$ e_{t} = s^{T} W h_{t}$$ where $W$ is a trainable weight vector. This has been used extensively in Question Answering systems like the Stanford Attentive Reader and DrQA . Scaled Dot Product Attention . $$ e_{t} = s^{T}h_{t}/ sqrt n$$ where $n$ is the model size. A modified version of this proposed in the Transformers paper by Vaswani et al. is now employed in almost every NLP system. The following section explains this attention in more detail. . Multiheaded Self Attention . Idea of Linear Projections . Consider a system of an online book store like kindle, which lets you rent, buy and read books on its platform. Such platforms usually have a recommendation system (recsys) in place that enables them to understand their users&#39; taste and preferences over time. This helps them in making personalized recommendations to users and in turn improve their revenue. For simplicity, let&#39;s assume that there are 10,000 books available on the platform and the system maintain a simple binary vector of size 10,000 for each user. If a user has read a particular book, the position in the vector corresponding to the book&#39;s id is 1 and 0 otherwise. A books-read vector for a user looks like, $$ [1,0,0,1,1,0,0,0,0,1,1,...,1] $$ . Now assume a projection matrix of dimension 10,000 X 100. When we multiply any user&#39;s books vector, we get a new low dimensional vector of size 100. This vector is totally different from the previous one and now represents the user&#39;s taste or preferences in books. It basically represents a user-profile for the recommendation system. Calculating this user-taster vector for different users enables the application to find users with similar taste and recommend books that they might like simply based on what the other &quot;similar&quot; user has read. The weights or values of this projection matrix can be thought as representing certain features or properties that a book might possess. It might capture various genres like science, philsophy, fantasy novels, etc. The question that still remains however, is how do we get such a projection matrix in the first place that can transform a represenation from one vector space to another that is somehow related to the original vector but has an entirely different interpretation. This is exactly what deep learning is about. Neural networks work as this universal function approximators that helps in learning such transformations. The weights of such projection matrices are learned via backpropagation. We also need a lot of training data to achieve this. . Self Attention . Much of what will follow is heavily derived from Jay Alammar&#39;s famous blog post: The Illustrated Transformer. The intuition and visualizations can be directly converted into code and that&#39;s my main motive here. To understand the details, we&#39;ll first look at self attention using vectors at a granular level. We&#39;ll then show how actually these computations are made using matrices which directly correspond to the code. For convenience, we&#39;ll explain how self attention works in the transformer model. The input to the self attention layer is an embedding vector. The central idea of attention is the same as discussed in the first notebook. Even here we&#39;ll calculate the measure of similarity between two representations, convert them into an attention distribution and take a weighted sum with the values. However, there are certain details involved that need to be addressed. Following steps involved in calculating self-attention. . The first step is to project the input into 3 different vector spaces: key space, query space and value space. These projections give us a key vector, a query vector and a value vector. The weights of these projection matrices are learnt via backpropagation during training. The projection matrices for key, query and value are $W^{K}$, $W^{Q}$, $W^{V}$ respectively. These projections are exactly what we discussed above. Their values depend a lot on the training procedure and the training data. . | The next step is to calculate attention scores. This is basically the part where we determine how similar are two input vectors and hence how much attention/focus needs to be paid on one vector while summarizing the other. . The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position. . There are different ways to determine this. In this paper, a dot product between the query and the key is used. Consider the phrase &quot;Thinking Machines&quot;. For the word &quot;Thinking&quot;, we need to calculate a score with each word in the sentence including &quot;Thinking&quot; itself. Therefore, score for first position would be, $$ q_{1} . k_{1} $$ The result of this product represents the amount of attention we need to pay to &quot;Thinking&quot; itself while encoding &quot;Thinking&quot;. The score for next position would be, $$ q_{1} . k_{2} $$ which captures the importance of &quot;Machines&quot; while encoding &quot;Thinking&quot;. . | We then divide the scores calculated in the previous step by $ sqrt d_{k}$, where $d_{k}$ is the dimension of key vectors. This scaling was done to ensure that the gradients are stable during training. Next, these scores are passed through a softmax function to get an attention distribution. This means that for a sentence of length $n$, if $ alpha_{t}$ represents the score at $t$-th position, then $$ sum_{t=1}^{n} alpha_{t} = 1$$ | The last step is to multiply the softmax output with the value vector at respective position and sum these products up. In effect this computes a weighted sum. For a sentence of length n, $$ sum_{t=1}^{n} alpha_{t} v_{t}$$ | All the steps explained above can be summarized as, . Multiheaded attention and Implementation . The above steps are usually performed using matrices instead of vectors. This is also where we&#39;ll see how and why multihead attention is implemented. . The first step is to calculate the query, key and value matrices by projecting them using trainable weights. In code, these weights correspond to linear layers. $W^{Q}$ corresponds to fc_q, $W^{K}$ to fc_k and $W^{V}$ to fc_v. Projecting these gives us $Q$, $K$ and $V$ as seen in code too. | Similar representations for value and key are also calculated. The dimensions of the above matrices will be explained below. . Calculation of scores can be easily visualized as follows, In code this is achieved by calculating the energy of $K$ and $Q$ using torch.matmul. | The final step is to scale, take softmax of the scores and multiply the matrix by the value matrix. scale is calculated by taking the square root of head_dim. After scaling the energy tensor or the scores at different positions, we apply softmax to this tensor and multiply it with $V$ using torch.matmul once again. | In the original transformer model, the input embedding size is 512. Before projecting these embeddings, we split them into 8 parts which brings us to multihead attention. This paper uses 8 attention heads. Multiheaded attention expands the model&#39;s ability to focus on different positions. . It gives the attention layer mutiple &quot;representation subspaces.&quot; . These subspaces are nothing but different projection matrices. Instead of having just one projection matrix $W^{Q}$ for query, we&#39;ll have 8 projection matrices for query, key and value. Weights for each of these &quot;subspaces&quot; are learnt via backpropagation during training. An analogy for this can be the use of multiple convolutional filters to learn unique features from the image. Therefore, now the dimension of key, query and value matrices would be 64 (512/8). In code, splitting weight matrices for multiple attention heads is done right after getting $K$, $Q$ and $V$. This is done by first calculating the head_dimension and then splitting the tensors using the view function. . . The above image shows projection matrices for 2 attention heads. There are 8 such heads. This would give us 8 $Z$ matrices in the end. The output dimension of the self attention layer should be same as the input dimension. Hence, we need to recombine the results of all the attention heads before passing the output to the next layer. To combine them, in code, we simply use view to drop the head dimension and further make a projection using fc_o to ensure that the input dimension is same as the output dimension. . from torch import nn class MultiheadAttentionLayer(nn.Module): def __init__(self, hid_dim, num_heads, device): super().__init__() self.num_heads = num_heads self.device = device self.hid_dim = hid_dim self.head_dim = self.hid_dim // self.num_heads self.fc_q = nn.Linear(hid_dim, hid_dim) self.fc_k = nn.Linear(hid_dim, hid_dim) self.fc_v = nn.Linear(hid_dim, hid_dim) self.fc_o = nn.Linear(hid_dim, hid_dim) self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device) def forward(self, x, mask): # x = [bs, len_x, hid_dim] # mask = [bs, len_x] batch_size = x.shape[0] Q = self.fc_q(x) K = self.fc_k(x) V = self.fc_v(x) # Q = K = V = [bs, len_x, hid_dim] Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3) K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3) V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3) # [bs, len_x, num_heads, head_dim ] =&gt; [bs, num_heads, len_x, head_dim] K = K.permute(0,1,3,2) # [bs, num_heads, head_dim, len_x] energy = torch.matmul(Q, K) / self.scale # (bs, num_heads){[len_x, head_dim] * [head_dim, len_x]} =&gt; [bs, num_heads, len_x, len_x] mask = mask.unsqueeze(1).unsqueeze(2) # [bs, 1, 1, len_x] #print(&quot;Mask: &quot;, mask) #print(&quot;Energy: &quot;, energy) energy = energy.masked_fill(mask == 1, -1e10) #print(&quot;energy after masking: &quot;, energy) alpha = torch.softmax(energy, dim=-1) # [bs, num_heads, len_x, len_x] #print(&quot;energy after smax: &quot;, alpha) alpha = F.dropout(alpha, p=0.1) a = torch.matmul(alpha, V) # [bs, num_heads, len_x, head_dim] a = a.permute(0,2,1,3) # [bs, len_x, num_heads, hid_dim] a = a.contiguous().view(batch_size, -1, self.hid_dim) # [bs, len_x, hid_dim] a = self.fc_o(a) # [bs, len_x, hid_dim] #print(&quot;Multihead output: &quot;, a.shape) return a . References . Attention is All You Need https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf | https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html | The Illustrated Transformer:http://jalammar.github.io/illustrated-transformer/. This is an excellent piece of writing with amazing easy-to-understand visualizations. Must read. | https://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources/. Chris McCormick&#39;s BERT research series is another great resource to learn about self attention and various other details about BERT. He has a blog as well as youtube video series on the same. | https://nlp.seas.harvard.edu/2018/04/03/attention.html. The annotated Transformer. | https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture10.pdf. | https://github.com/bentrevett/pytorch-seq2seq. A great series of notebooks on Machine Translation using PyTorch which includes all the the methods discussed above in greater detail and context. | .",
            "url": "https://kushalj001.github.io/black-box-ml/attention/bahdanau/self%20attention/bahdanau%20attention/multihead%20attention/pytorch-implemention/2020/07/06/Generalizing-Attention-in-NLP-and-Understanding-Self-Attention.html",
            "relUrl": "/attention/bahdanau/self%20attention/bahdanau%20attention/multihead%20attention/pytorch-implemention/2020/07/06/Generalizing-Attention-in-NLP-and-Understanding-Self-Attention.html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Character Embeddings and Highway Layers in NLP",
            "content": "Introduction . Character embeddings and Highway Layers are the trademark components of many NLP systems. They have been used extensively in literature to reduce the parameters in models, deal with Out of Vocabulary or OOV words and help in faster training of neural networks. This blog post introduces these 2 topics, explains the intuitions with illustrations and then translates everything into code. . This blog post is a small excerpt from my work on paper-annotations for the task of question answering. This repo contains a collection of important question-answering papers, implemented from scratch in pytorch with detailed explanation of various concepts/components introduced in the respective papers. . Character Embedding . A character embedding is calculated for each context and query word. This is done by using convolutions. . It maps each word to a vector space using character-level CNNs. . Using CNNs in NLP was first proposed by Yoon Kim in his paper titled Convolutional Neural Networks for Sentence Classification. This paper tries to use CNNs in NLP as they are used in vision. Most of the state-of-the-art results in CV at that time were achieved by transfer learning from larger models pretrained on ImageNet. In this paper, they train a simple CNN with one layer of convolution on top of pretrained word vectors and hypothesized that these pretrained word vectors could work as a universal feature extractors for various classification tasks. This is analogous to the earlier layers of vision models like VGG and Inception working as generic feature extractors. The idea of character embeddings was also used in the paper titled Character-Aware Neural Language Models by the same author. The intuition is simple over here. Just as convolutional filters learn various features in an image by operating on its pixels, here they&#39;ll do so by operating on characters of words. Let&#39;s get into the working of this layer. . We first pass each word through an embedding layer to get a fixed size vector. Let the embedding dimension be $d$. Let $C$ represent a matrix representation of word of length $l$. Therefore $C$ is a matrix with dimensions $d$ x $l$. . Let $H$ represent a convolutional filter with dimensions $d$ x $w$, where $d$ is the embedding dimension and $w$ is the width or the window size of the filter. The weights of this filter are randomly initialized and learnt parallelly via backpropogation. We convolve this filter $H$ over our word representation $C$ as shown below. . The convolution operation is simply the inner product of the filter $H$ and matrix $C$. The convolution operations can be visualized as follows: . The result of the above operation is a feature vector. A single filter is usually associated with a unique feature that it captures from the image/matrix. To get the most representative value related to the feature, we perform max pooling over the dimension of this vector. . The above process was described for a single filter. This same process is repeated with $N$ number of filters. Each of these filters captures a different property of word. In an image, for example, if one filter captures the edges, another filter will capture the texture and another one the shapes in the image and so on. $N$ is also the size of the desired character embedding. In this paper authors have trained the model with $N$ = 100. . Implementation . The implementation of this layer is fairly straightforward. The input to this layer is of dimension [batch_size, seq_len, word_len] where seq_len and word_len are the lengths of largest sequence and word respectively within a given batch . We first embed the character tokens into a fixed size vector using an embedding layer. This gives a vector of dimension [batch_size, seq_len, word_len, emb_dim]. We then convert this tensor into a format that closely resembles an image, of type [ $N$, $C_{in}$, $H_{in}$, $W_{in}$]. The number of input channels, $C_{in}$ would be 1 and the output channels would be the desired embedding size which is 100. This is then passed through the convolution layer which gives an output of shape [ $N$, $C_{out}$, $H_{out}$, $W_{out}$]. Here, . . If padding = [0,0], kernel_size (or filter-size) = [$H_{in}$, $w$], dilation = [1,1], stride = [1,1]. as visible in images above, then, $H_{out}$ = 1, and $W_{out}$ = $W_{in}$ - $w$ - 1. . Since $H_{out}$ = 1, we squeeze that dimension and perform max pooling with a kernel-size = $L_{in}$. The value of $L_{in}$ = $W_{in}$ - $w$ - 1. . . If the kernel size = $L_{in}$, we get $L_{out}$ = 1 if other values are default. This dimension is again squeezed to finally give us a tensor of dimension [batch_size, seq_len, output_channels (or 100)]. . class CharacterEmbeddingLayer(nn.Module): def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size): super().__init__() self.char_emb_dim = char_emb_dim self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1) self.char_convolution = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=kernel_size) self.relu = nn.ReLU() self.dropout = nn.Dropout(0.2) def forward(self, x): # x = [bs, seq_len, word_len] # returns : [batch_size, seq_len, num_output_channels] # the output can be thought of as another feature embedding of dim 100. batch_size = x.shape[0] x = self.dropout(self.char_embedding(x)) # x = [bs, seq_len, word_len, char_emb_dim] # following three operations manipulate x in such a way that # it closely resembles an image. this format is important before # we perform convolution on the character embeddings. x = x.permute(0,1,3,2) # x = [bs, seq_len, char_emb_dim, word_len] x = x.view(-1, self.char_emb_dim, x.shape[3]) # x = [bs*seq_len, char_emb_dim, word_len] x = x.unsqueeze(1) # x = [bs*seq_len, 1, char_emb_dim, word_len] # x is now in a format that can be accepted by a conv layer. # think of the tensor above in terms of an image of dimension # (N, C_in, H_in, W_in). x = self.relu(self.char_convolution(x)) # x = [bs*seq_len, out_channels, H_out, W_out] x = x.squeeze() # x = [bs*seq_len, out_channels, W_out] x = F.max_pool1d(x, x.shape[2]).squeeze() # x = [bs*seq_len, out_channels, 1] =&gt; [bs*seq_len, out_channels] x = x.view(batch_size, -1, x.shape[-1]) # x = [bs, seq_len, out_channels] # x = [bs, seq_len, features] = [bs, seq_len, 100] return x . Highway Networks . Highway networks were originally introduced to ease the training of deep neural networks. While researchers had cracked the code for optimizing shallow neural networks, training deep networks was still a challenging task owing to problems such as vanishing gradients etc. Quoting the paper, . We present a novel architecture that enables the optimization of networks with virtually arbitrary depth. This is accomplished through the use of a learned gating mechanism for regulating information ﬂow which is inspired by Long Short Term Memory recurrent neural networks. Due to this gating mechanism, a neural network can have paths along which information can ﬂow across several layers without attenuation. We call such paths information highways, and such networks highway networks. . This paper takes the key idea of learned gating mechanism from LSTMs which process information internally through a sequence of learned gates. The purpose of this layer is to learn to pass relevant information from the input. A highway network is a series of feed-forward or linear layers with a gating mechanism. The gating is implemented by using a sigmoid function which decides what amount of information should be transformed and what should be passed as it is. . A plain feed-forward layer is associated with a linear transform $H$ parameterized by ($W_{H}, b_{H}$), such that for input $x$, the output $y$ is . $$ y = g(W_{H}.x + b_{H})$$ where $g$ is a non-linear activation. For highway networks, two additional linear transforms are defined viz. $T$ ($W_{T},b_{T}$) and $C$ ($W_{C}$,$b_{C}$). Then, . $$ y = T(x) . H(x) + x . C(x) $$ . We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by transforming the input and carrying it, respectively. For simplicity, in this paper we set C = 1 − T. . $$ y = T(x) . H(x) + x . (1 - T(x)) $$ . $$ y = T(x) . g(W_{H}.x + b_{H}) + x . (1 - T(x)) $$ where $T(x)$ = $ sigma$ ($W_{T}$ . $x$ + $b_{T}$) and $g$ is relu activation. . The input to this layer is the concatenation of word and character embeddings of each word. To implement this we use nn.ModuleList to add multiple linear layers. This is done for the gate layer as well as for a normal linear transform. In code the flow_layer is the same as linear transform $H$ discussed above and gate_layer is $T$. In the forward method we loop through each layer and compute the output according to the highway equation described above. . The output of this layer for context is $X$ $ epsilon$ $R^{ d X T}$ and for query is $Q$ $ epsilon$ $R^{ d X J}$, where $d$ is hidden size of the LSTM, $T$ is the context length, $J$ is the query length. . Importance in NLP systems . The structure discussed so far is a recurring pattern in many NLP systems. Although this might be out of favor now with the advent of transformers and large pretrained language models, you will find this pattern in many NLP systems before transformers came into being. The idea behind this is that adding highway layers enables the network to make more efficient use of character embeddings. If a particular word is not found in the pretrained word vector vocabulary (OOV word), it will most likely be initialized with a zero vector. It then makes much more sense to look at the character embedding of that word rather than the word embedding. The soft gating mechanism in highway layers helps the model to achieve this. . class HighwayNetwork(nn.Module): def __init__(self, input_dim, num_layers=2): super().__init__() self.num_layers = num_layers self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)]) self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)]) def forward(self, x): for i in range(self.num_layers): flow_value = F.relu(self.flow_layer[i](x)) gate_value = torch.sigmoid(self.gate_layer[i](x)) x = gate_value * flow_value + (1-gate_value) * x return x . References . Character-Aware Neural Language Models: https://arxiv.org/abs/1508.06615 | Convolutional Neural Networks for Sentence Classification: https://arxiv.org/abs/1408.5882 | Highway Networks: https://arxiv.org/abs/1505.00387 | https://nlp.seas.harvard.edu/slides/aaai16.pdf. A great resource for character embeddings. The figures in the character embedding section are taken from here. | .",
            "url": "https://kushalj001.github.io/black-box-ml/cnns/highway%20layers/nlp/cnns%20for%20nlp/2020/04/22/Character-Embeddings-and-Highway-Layers-in-NLP.html",
            "relUrl": "/cnns/highway%20layers/nlp/cnns%20for%20nlp/2020/04/22/Character-Embeddings-and-Highway-Layers-in-NLP.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Understanding Depth-wise Separable Convolutions",
            "content": "This blog post is a small excerpt from my work on paper-annotations for the task of question answering. This repo contains a collection of important question-answering papers, implemented from scratch in pytorch with detailed explanation of various concepts/components introduced in the respective papers. The illustrations in this blog post have been created by me using https://www.diagrams.net/. You can find the other references below. . Depthwise Separable Convolutions . Depthwise separable convolutions serve the same purpose as normal convolutions with the only difference being that they are faster because they reduce the number of multiplication operations. This is done by breaking the convolution operation into two parts: depthwise convolution and pointwise convolution. . Depthwise separable convolutions are used rather than traditional ones, as we observe that it is memory efﬁcient and has better generalization. . Let&#39;s understand why depthwise convolutions are faster than traditional convolution. Traditional convolution can be visualized as, . . Let&#39;s count the number of multiplications in a traditional convolution operation. The number of multiplications for a single convolution operation is the number of elements inside the kernel. This is $D_{K}$ X $D_{K}$ X $M$ = $D_{K}^{2}$ X $M$. To get the output feature map, we slide or convolve this kernel over the input. Given the output dimensions, we perform $D_{O}$ covolutions along the width and the height of the input image. Therefore, the number of multiplications per kernel are $D_{O}^{2}$ X $D_{K}^{2}$ X $M$. These calculations are for a single kernel. In convolutional neural networks, we usually use multiple kernels. Each kernel is expected to extract a unique feature from the input. If we use $N$ such filters, then number of multiplications become $N$ X $D_{O}^{2}$ X $D_{K}^{2}$ X $M$. . Depthwise convolution . . In depthwise convolution we perform convolution using kernels of dimension $D_{K}$ X $D_{K}$ X 1. Therefore the number of multiplications in a single convolution operation would be $D_{K}^{2}$ X $1$. If the output dimension is $D_{O}$, then the number of multiplications per kernel are $D_{K}^{2}$ X $D_{O}^{2}$. If there are $M$ input channels, we need to use $M$ such kernels, one kernel for each input channel to get the all the features. For $M$ kernels, we then get $D_{K}^{2}$ X $D_{O}^{2}$ X $M$ multiplications. . Pointwise convolution . . This part takes the output from depthwise convolution and performs convolution operation with a kernel of size 1 X 1 X $N$, where $N$ is the desired number of output features/channels. Here similarly, Multiplications per 1 convolution operation = 1 X 1 X $M$ Multiplications per kernel = $D_{O}^{2}$ X $M$ For N output features = $N$ X $D_{O}^{2}$ X $M$ . Adding up the number of multiplications from both the phases, we get, . $$ = N . D_{O}^{2} . M + D_{K}^{2} . D_{O}^{2} . M $$ $$ = D_{O}^{2} . M (N + D_{K}^{2}) $$ . Comparing this with traditional convolutions, . $$ = frac {D_{O}^{2} . M (N + D_{K}^{2})} {D_{O}^{2} . M . D_{K}^{2} . N}$$ . $$ = frac{1}{D_{K}^{2}} + frac{1}{N} $$ . This clearly shows that the number of computations in depthwise separable convolutions are lesser than traditional ones. In code, the depthwise phase of the convolution is done by assigning groups as in_channels. According to the documentation, . At groups= in_channels, each `nput channel is convolved with its own set of filters, of size:$ left lfloor frac{out _channels}{in _channels} right rfloor$ . Implementation . Following is an implementation for the layer discussed above. This is a standalone implementation of the layer and can be plugged into any application/larger model where it is used as a component. . import torch from torch import nn class DepthwiseSeparableConvolution(nn.Module): def __init__(self, in_channels, out_channels, kernel_size): super().__init__() self.depthwise_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, groups=in_channels, padding=kernel_size//2) self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0) def forward(self, x): # Interpretations # x = [bs, seq_len, emb_dim] for NLP applications # x = [C_in, H_in, W_in] for CV applications x = self.pointwise_conv(self.depthwise_conv(x)) return x . References . The QANet paper: https://arxiv.org/abs/1804.09541 | Convolutional Neural Networks for Sentence Classification: https://arxiv.org/abs/1408.5882 | https://www.youtube.com/watch?v=T7o3xvJLuHk. Easy explanation of depthwise separable convolutions. | https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728. Another amazing blog for depthwise separable convolutions. | .",
            "url": "https://kushalj001.github.io/black-box-ml/convolutions/depthwise%20separable%20convolutions/pytorch-implementation/pytorch/2020/03/20/Understanding-Depth-wise-Separable-Convolutions.html",
            "relUrl": "/convolutions/depthwise%20separable%20convolutions/pytorch-implementation/pytorch/2020/03/20/Understanding-Depth-wise-Separable-Convolutions.html",
            "date": " • Mar 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Building Sequential Models in PyTorch",
            "content": "Introduction . The aim of this post is to enable beginners to get started with building sequential models in PyTorch. PyTorch is one of the most widely used deep learning libraries and is an extremely popular choice among researchers due to the amount of control it provides to its users and its pythonic layout. I am writing this primarily as a resource that I can refer to in future. This post will help in brushing up all the basics of PyTorch and also provide a detailed explanation of how to use some important torch.nn modules. We will be implementing a common NLP task - sentiment analysis using PyTorch and torchText. We will be building an LSTM network for the task by using the IMDB dataset. Let&#39;s get started! . A Tensor based approach . Another motivation to write this post is to introduce neural nets from tensors&#39; persepective. Neural nets are, ultimately, a series of matrix/tensor multiplications. Each layer in a neural net expects an input in a specified format and gives the output in a specified format. These input and output formats are tensors of different shapes. The content of these tensors are well defined by the documentation of the concerned library. As such, while implementing neural nets, it becomes very important to understand the input and output shapes of tensors for all layers. The following tutorial is written based on this approach. . A short Introduction to NLP pipeline . I do not intend to go into details about all the preprocessing steps that are required for NLP tasks but for the sake of completeness, I&#39;ll summarize some important conceptual points. . Textual data usually requires some amount of cleaning before they can be fed to neural nets. Important cleaning steps include removal of HTML tags, punctuation, stopwords, numbers etc. Stopwords are high frequency words in a dataset that do not convey significant information to the network (e.g. and, of, the, a). | Lemmatization can be performed to improve results of the network. Lemmatization converts words to their root form or to their lemma which can be found in a dictionary. For example &quot;goes&quot; $-&gt;$ &quot;go&quot;. | Neural nets do not understand natural language. In fact the only thing they understand and can process are numbers. So for any NLP task, we need to convert out text data into a numerical format (numericalization). Each word in the dataset is assigned a numerical value. These words need to be represented as a vector. | There are two options to represent a word or a token as a feature vector. One-hot encoding: Here, the size of the feature vector equals that of the vocabulary of dataset. All the values are 0 except for the index that equals the numerical value of the word. | Embeddings: These can either be pre-trained(GloVe, word2vec) or be trained in parallel with the main task. The dimensions of such vectors are usually around 100-300. These transform the features of the word into a dense vector. | | . TorchText basics . Some key points about the structure of the library will serve as a good introduction and also help in following along the rest of the tutorial. . The most important class of torchtext is the Field class. The structure of datasets for different NLP tasks is different. For example, in a classification task we have text reviews that are sequential in nature and a sentiment label corresponding to each review which is binary in nature (+ or -). In machine translation or summarization, both the input and output are sequential. The Field class handles all such types of datasets. Therefore, we initialize Field objects for each type of data format present in our dataset. . In sentiment analysis we need two Field instances - one for text review and other for labels. For labels we use LabelField which inherits from Field. Following are some important parameters you might need while initializing a Field class. . tokenize : function used to tokenize the text. This can either be a custom function or passing &#39;spacy&#39; uses the SpaCy tokenizer. | init_token : prepends this token in the beginning of each example. e.g. &lt; sos &gt;. | eos_token : appends this token in the end of each example. e.g. &lt; eos &gt;. | fix_length : fixed, predefined length to which all the examples in field will be padded. | batch_first: gives data in tensors that have batch dimension as the first dimension. | . Some important methods: . pad() : This method pads the examples to fix_length if provided as a parameter. If not, it calculates the length of the largest example in a batch and pads the sequences to that length. | build_vocab(): The Field class holds an instance of Vocab class. This class is responsible for creating a vocabulary from the field data and creating mappings stoi (string to int) and itos (int to string) for each word. | . To summarize, the Field class numericalizes the text data and provides it in form of tensors so that they can be used easily with neural nets. . import torchtext from torchtext import data, datasets from torch import nn import torch import torch.optim as optim . # creating field objects for text and labels review = data.Field(tokenize=&#39;spacy&#39;, batch_first=True) sentiment = data.LabelField(dtype=torch.float, batch_first=True) # loading the IMDB dataset train_data, test_data = datasets.IMDB.splits(text_field=review, label_field=sentiment) . print(vars(train_data.examples[4])) . {&#39;text&#39;: [&#39;This&#39;, &#39;is&#39;, &#39;not&#39;, &#39;the&#39;, &#39;typical&#39;, &#39;Mel&#39;, &#39;Brooks&#39;, &#39;film&#39;, &#39;.&#39;, &#39;It&#39;, &#39;was&#39;, &#39;much&#39;, &#39;less&#39;, &#39;slapstick&#39;, &#39;than&#39;, &#39;most&#39;, &#39;of&#39;, &#39;his&#39;, &#39;movies&#39;, &#39;and&#39;, &#39;actually&#39;, &#39;had&#39;, &#39;a&#39;, &#39;plot&#39;, &#39;that&#39;, &#39;was&#39;, &#39;followable&#39;, &#39;.&#39;, &#39;Leslie&#39;, &#39;Ann&#39;, &#39;Warren&#39;, &#39;made&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;,&#39;, &#39;she&#39;, &#39;is&#39;, &#39;such&#39;, &#39;a&#39;, &#39;fantastic&#39;, &#39;,&#39;, &#39;under&#39;, &#39;-&#39;, &#39;rated&#39;, &#39;actress&#39;, &#39;.&#39;, &#39;There&#39;, &#39;were&#39;, &#39;some&#39;, &#39;moments&#39;, &#39;that&#39;, &#39;could&#39;, &#39;have&#39;, &#39;been&#39;, &#39;fleshed&#39;, &#39;out&#39;, &#39;a&#39;, &#39;bit&#39;, &#39;more&#39;, &#39;,&#39;, &#39;and&#39;, &#39;some&#39;, &#39;scenes&#39;, &#39;that&#39;, &#39;could&#39;, &#39;probably&#39;, &#39;have&#39;, &#39;been&#39;, &#39;cut&#39;, &#39;to&#39;, &#39;make&#39;, &#39;the&#39;, &#39;room&#39;, &#39;to&#39;, &#39;do&#39;, &#39;so&#39;, &#39;,&#39;, &#39;but&#39;, &#39;all&#39;, &#39;in&#39;, &#39;all&#39;, &#39;,&#39;, &#39;this&#39;, &#39;is&#39;, &#39;worth&#39;, &#39;the&#39;, &#39;price&#39;, &#39;to&#39;, &#39;rent&#39;, &#39;and&#39;, &#39;see&#39;, &#39;it&#39;, &#39;.&#39;, &#39;The&#39;, &#39;acting&#39;, &#39;was&#39;, &#39;good&#39;, &#39;overall&#39;, &#39;,&#39;, &#39;Brooks&#39;, &#39;himself&#39;, &#39;did&#39;, &#39;a&#39;, &#39;good&#39;, &#39;job&#39;, &#39;without&#39;, &#39;his&#39;, &#39;characteristic&#39;, &#39;speaking&#39;, &#39;to&#39;, &#39;directly&#39;, &#39;to&#39;, &#39;the&#39;, &#39;audience&#39;, &#39;.&#39;, &#39;Again&#39;, &#39;,&#39;, &#39;Warren&#39;, &#39;was&#39;, &#39;the&#39;, &#39;best&#39;, &#39;actor&#39;, &#39;in&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;,&#39;, &#39;but&#39;, &#39;&#34;&#39;, &#39;Fume&#39;, &#39;&#34;&#39;, &#39;and&#39;, &#39;&#34;&#39;, &#39;Sailor&#39;, &#39;&#34;&#39;, &#39;both&#39;, &#39;played&#39;, &#39;their&#39;, &#39;parts&#39;, &#39;well&#39;, &#39;.&#39;], &#39;label&#39;: [&#39;pos&#39;]} . # dividing the training set further into a train and validation set train_data, valid_data = train_data.split() . # some important parameters VOCAB_SIZE = 25000 BATCH_SIZE = 64 device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) # build vocabulary for the fields review.build_vocab(train_data, max_size=VOCAB_SIZE) sentiment.build_vocab(train_data) # create iterators for the dataset. iterators enable looping through the dataset easily train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits( (train_data, valid_data, test_data), batch_size = BATCH_SIZE, device = device) . x = next(iter(train_iterator)) print(x.text.shape) print(x.label.shape) . torch.Size([64, 1201]) torch.Size([64]) . Implementing the model . Let&#39;s begin by understanding the layers that are going to be used in this model. We need to know 3 things about each layer in PyTorch - . parameters : used to instantiate the layer. These are the keyword args required to create an object of the class. | inputs : tensors passed to instantiated layer during model.forward() call | outputs : output of the layer | . Embedding layer (nn.Embedding) . This layer acts as a lookup table or a matrix which maps each token to its embedding or feature vector. This module is often used to store word embeddings and retrieve them using indices. . Parameters . num_embeddings: size of vocabulary of the dataset. Number of words in the vocab. | embedding_dim : size of embedding vector for each word. 300 for word2vec. Each word will be mapped to a 300 (say) dimensional vector | . Inputs and outputs . Embedding layer can accept tensors of aribitary shape, denoted by [ * ] and the output tensor&#39;s shape is [ * ,H], where H is the embedding dimension of the layer. For example in case of sentiment analysis, the input will be of shape [batch_size, seq_len] and the output shape will be [ batch_size, seq_len, embedding_dim ]. Intuitively, it replaces each word of each example in the batch by an embedding vector. . LSTM Layer (nn.LSTM) . Parameters . input_size : The number of expected features in input. This means the dimension of the feature vector that will be input to an LSTM unit. For most NLP tasks, this is the embedding_dim because the words which are the input are represented by a vector of size embedding_dim. | hidden_size : Number of features you want the LSTM to learn about the pattern of your data. | num_layers : Number of layers in the LSTM network. If num_layers = 2, it means that you&#39;re stacking 2 LSTM layers. The input to the first LSTM layer would be the output of embedding layer whereas the input for second LSTM layer would be the output of first LSTM layer. | batch_first : If True then the input and output tensors are provided as (batch_size, seq_len, feature). | dropout : If provided, applied between consecutive LSTM layers except the last layer. | bidirectional : If True, it becomes a bidirectional LSTM. That is it reads the sequence from both the directions. The forward direction starts from $x_{0}$ and goes till $x_{n}$ and backward direction goes from $x_{n}$ to $x_{0}$. | . seq_len mentioned above is the length of the input sentence. This will be the same for all the examples within a single batch. For the rest of this post we are going to take batch_first = True . Inputs . input : Shape of tensor is [batch_size, seq_len input_size] if batch_first = True. This is usually the output from the embedding layer for most NLP tasks. | h_0 : [batch_size, num_layers * num_directions, hidden_size] Tensor containing initial hidden state for each element in batch. | c_0 : [batch_size, num_layers * num_directions, hidden_size] Tensor containing initial cell state for each element in batch. | . Outputs . output : [batch_size, seq_len, num_directions * hidden_size] Tensor containing the output features (h_t) from the last layer of the LSTM, for each t. | h_n : [num_layers * num_directions, batch, hidden_size]: tensor containing the hidden state for t = seq_len. | c_n : [num_layers * num_directions, batch, hidden_size]: tensor containing the cell state for t = seq_len. | . Understanding the outputs of the LSTM can be a bit difficult initially. The following diagram clearly explains what each of the outputs mean. The following figure shows a general case of LSTM implementation. . The horizontal axis or the time axis determines the sequence length or the inputs at various time-steps. The vertical axis determines how many LSTM layers have been stacked together. Beginning from the first layer at the bottom, adding each layer increases the depth of the network. The number of layers are denoted by w in this figure. | . As evident, for each time-step t the LSTM unit takes in $h_{t-1}$, $c_{t-1}$ and $x_{t}$ and gives $h_{t}$ and $c_{t}$. The newly calculated $h_{t}$ and $c_{t}$ are passed to next LSTM unit as hidden and cell state of the sequnce seen so far. Simultaneously $h_{t}$ is also passed as the output for that time-step. This is used as the input for stacked layers above the current layer and finally to calculate predictions. Therefore the output of LSTM layer actually contains the hidden states of all time-steps passed through all the layers. Basically, it holds ( $h^{w}_{1}$, $h^{w}_{2}$, ... $h^{w}_{n}$). | . The final hidden and cell states denoted by $h_{n}$ and $c_{n}$ for all the layers are stacked in (h_n, c_n) output of the LSTM layer. Therefore (h_n, c_n) hold (($h^{1}_{n}$, $c^{1}_{n}$) , ($h^{2}_{n}$, $c^{2}_{n}$) ... ($h^{w}_{n}$, $c^{w}_{n}$)) where w is number of layers stacked in the LSTM. | . INPUT_SIZE = len(review.vocab) HIDDEN_SIZE = 128 EMBEDDING_DIM = 100 DROPOUT = 0.4 NUM_LAYERS = 1 BIDIRECTIONAL = False BATCH_SIZE = 64 OUTPUT_DIM = 1 . class SentimentLSTM(nn.Module): def __init__(self, input_size, hidden_size, embedding_dim, dropout, num_layers, output_dim): super().__init__() self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_dim) self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) self.dropout = nn.Dropout(p=dropout) self.linear = nn.Linear(in_features=hidden_size, out_features=output_dim) def forward(self, x): # x = [batch_size, seq_len] = [64, seq_len] as seq_len depends on the batch. embed = self.embedding(x) # embed = [batch_size, seq_len, embedding_dim] = [64, seq_len, 100] # These can be intuitively interpreted as: each example in the batch # has a length of seq_len and each word in the sequence is represented # by a vector of size 100. output, (hidden, cell) = self.lstm(embed) # output = [batch_size, seq_len, hidden_size] = [64, seq_len, 128] # hidden = [num_layers*num_directions, batch_size, hidden_size] = [1, 64, 128] # cell = [num_layers*num_directions, batch_size, hidden_size] = [1, 64, 128] # output is the concatenation of the hidden state from every time step, # whereas hidden is simply the final hidden state. # We verify this using the assert statement. output = output.permute(1,0,2) # hidden = [1, 64, 128] # output = [seq_len, 64, 128] assert torch.equal(output[-1,:,:], hidden.squeeze(0)) preds = self.linear(output[-1,:,:]) # preds = [64, 1] return preds . model = SentimentLSTM(INPUT_SIZE, HIDDEN_SIZE, EMBEDDING_DIM, DROPOUT, NUM_LAYERS, OUTPUT_DIM) . Training the model . A basic training loop involves the following steps . forward pass, i.e. multiplication of inputs with randomly initialized weights and carrying this on for all the layers. | calculate the loss of the predictions with the given target/ground truth. | calculate the gradient of the loss function and propagate it backwards to calculate gradients of all the layers | updating the weights of all the layers using the gradients so that the objective/loss function converges | . def accuracy(preds, y): &#39;&#39;&#39; Returns accuracy of the model.&#39;&#39;&#39; rounded_preds = torch.round(torch.sigmoid(preds)) correct = (rounded_preds == y).float() acc = correct.sum() / len(correct) return acc . # define the loss function criterion = nn.BCEWithLogitsLoss() # determine the gradient descent algorithm to be used for updating weights optimizer = optim.SGD(model.parameters(), lr = 1e-3) . # put model and loss on GPU model = model.to(device) criterion = criterion.to(device) . The following function performs the training and evaluation simultaneously. Each line has been well documented. . def fit(model, criterion, optimizer, train_iterator, valid_iterator): # define number of epochs epochs = 3 # go through each epoch for epoch in range(epochs): # put the model in training mode model.train() # initialize losses and accuaracy for every epoch train_loss = 0. train_acc = 0. valid_loss = 0. valid_acc = 0. print(&quot;Epoch: &quot;, epoch) # go through each batch from the dataset for batch in train_iterator: # calculate model predictions. squeeze(1) is done because the output of model is [64,1]. # criterion expects it to be of dimension [64]. preds = model(batch.text).squeeze(1) # calculate loss for the batch loss = criterion(preds, batch.label) # add batch loss to total loss for the epoch train_loss += loss.item() # calculate accuracy for the batch train_acc += accuracy(preds, batch.label).item() # backward prop for loss function and calc gradients of all the layers in the net loss.backward() # update the weights optimizer.step() # make the gradients zero before next step so that they don&#39;t accumulate optimizer.zero_grad() # print recorded results. Divide the total epoch loss/accuracy by the number of examples. print(&#39;Training loss: &#39;,train_loss / len(train_iterator)) print(&#39;Training accuracy: &#39;, train_acc / len(train_iterator)) # put the model in evaluation mode model.eval() # ensures that gradients are not calculated. Takes less time. with torch.no_grad(): # loop through the valid iterator for batch in valid_iterator: preds = model(batch.text).squeeze(1) loss = criterion(preds, batch.label) valid_loss += loss.item() valid_acc += accuracy(preds, batch.label).item() print(&#39;Validation loss: &#39;, valid_loss / len(valid_iterator)) print(&#39;Validation accuracy: &#39;, valid_acc / len(valid_iterator)) print(&#39;-&#39;) . fit(model, criterion, optimizer, train_iterator, valid_iterator) . Epoch: 0 Training loss: 0.6931361860602442 Training accuracy: 0.5035274374659044 Validation loss: 0.6933359392618729 Validation accuracy: 0.4931585452819275 - Epoch: 1 Training loss: 0.6932326938114027 Training accuracy: 0.5008553832116789 Validation loss: 0.693308207948329 Validation accuracy: 0.4962040961293851 - Epoch: 2 Training loss: 0.6932031631904797 Training accuracy: 0.5027046403745665 Validation loss: 0.6933137026883788 Validation accuracy: 0.4962040961293851 - . References and Acknowledgements . https://pytorch.org/docs/stable/nn.html | https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm | https://github.com/udacity/deep-learning-v2-pytorch | https://github.com/bentrevett |",
            "url": "https://kushalj001.github.io/black-box-ml/lstm/pytorch/torchtext/nlp/sentiment-analysis/2020/01/10/Building-Sequential-Models-In-PyTorch.html",
            "relUrl": "/lstm/pytorch/torchtext/nlp/sentiment-analysis/2020/01/10/Building-Sequential-Models-In-PyTorch.html",
            "date": " • Jan 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Understanding LSTMs",
            "content": "Introduction . Why another blog post on LSTMs? . LSTMs or the Long Short Term Memory have been around for a long time and there are many resources that do a great job in explaining the concept and its working in detail. So why another blog post? Multiple reasons. First, as a personal resource. I have read many blog posts on LSTMs and most of the times the same ones simply because I tend to forget the details after sometime. Clearly, my brain cannot handle long-term dependencies. Second, I am trying to put a lot of details in this post. I have come across blogs that explain LSTMs with the help of equations, some do it with text only and some with animations (hands down the best ones). In this post, I have attempted to include explanations for each component, equations, figures and an implementation of the LSTM layer from scratch. I hope this helps everyone get a wholesome understanding of the topic. . Recurrent Neural Nets . RNNs are one of the key flavours of deep neural networks. Unlike artificial neural networks that have multiple layers of neurons stacked one after another, it is not easily evident as to how recurrent nets are deep. Below is the figure of a rolled RNN. . The input $x_{t}$ is processed by the RNN for each time-step t and outputs a hidden state that captures and maintains the information of all the previous time-steps. We&#39;ll see this again as a for loop when we implement an LSTM layer. This for loop is exactly what makes RNNs deep. The unrolled version of the network is more widely used in literature and is shown below: . This deep nature is precisely the reason why such networks cannot practically model long-term dependencies. As the length of the input sequence increases, the number of matrix multiplications within the network increase. The weight updates of the earlier layers suffer as the gradients tend to vanish for them. Intuitively, think of this as multiplying a number less than zero with itself. The values become low exponentially. On the other hand if gradient values are larger than 1, these explode into large numbers that the computer can no longer make sense of. Consider this for intuition: . To deal with such issues, we need a mechanism that enables the networks to forget the irrelevant information and hold on to the relevant one. Enter LSTMs. . Understanding the LSTM cell . Before we get into the abstract details of the LSTM, it is important to understand what the black box actually contains. The LSTM cell is nothing but a pack of 3-4 mini neural networks. These networks are comprised of linear layers that are parameterized by weight matrices and biases. The values of these weights are learnt by backpropagation. The following figure shows an LSTM cell with labelled gates and all the computations that take place inside the cell. Each cell has 3 inputs: the current token $x_{t}$, previous hidden state $h_{t-1}$ and the previous cell state $c_{t-1}$ and 2 outputs: the updated hidden state $h_{t}$ and cell state $c_{t}$. . The forget gate . This gate takes in the current input $x_{t}$ and the previous hidden state $h_{t-1}$, multiplies them with a weight matrix of the forget gate $W_{f}$ (also adds a bias $b_{f}$) and applies a sigmoid activation. From an implementational point of view, $W_{f}$ and $b_{f}$ are the values associated with a simple linear layer. The sigmoid function restricts the input values between 0 and 1. This output is then multiplied with the cell state. Intuitively, given the current input this gate tells the cell to remove or forget some information if the sigmoid output is close to 0 and keep the information if the sigmoid output is close to 1. The equation for this gate is: . The input and the cell gate . The input gate is used to decide that given the current input what information is important and should be stored in the cell state. The calculations of this gate are similar to those of the forget gate. The cell gate and the input gate work closely together to perform a very specific function. This function is to update the previous cell state. To do so, the cell gate proposes an update candidate. You can think of this update candidate as a proposed new cell state. To calculate the cell gate output, a tanh activation is used (more on this later). The equation for the cell state is: . Cell update . We cannot simply replace the new proposed cell state and eliminate the previous cell state. This is because the previous state might contain some important information about the previous inputs the LSTM layer has seen. This is basically the main purpose of recurrent networks - to hold on to relevant information from the past. Hence, we adopt a very elegant approach to update the cell state. From the previous blocks, we know the following: Given the current input, the forget gate decides what information from the previous cell state can be forgotten. This is done by multiplying the forget gate output $f_{t}$ with the previous cell state $c_{t-1}$. Also, the input gate determines what information from the current input is relevant. Product of $i_{t}$ and the proposed cell state would give us important parts from the proposed cell state. We thus combine these two to yeild the following cell update equation: . Output gate . This gate is used to calculate the hidden state for the next time step. Again the current input $x_{t}$ and previous hidden state $h_{t-1}$ are multiplied by a weight matrix $W_{o}$ and passed through sigmoid activation. To update the hidden state of the cell, it might be a good idea to incorporate some information from the newly updated cell state. Therefore, the result of the output gate $o_{t}$ is multiplied with the updated cell state $c_{t}$ after passing $c_{t}$ through tanh activation (explained later). The equation for this gate is: . Why do we use tanh for calculating cell state? . We left this part rather abruptly while talking about the cell gate. The question of as to why tanh has been preferred over sigmoid and even ReLU has been a hotly debated one. My research led me to multiple sources and reasons. . One of the key reasons as to why tanh is preferred is its range [-1, 1] and the fact that it is zero-centered. These properties enable the neural net to converge faster and hence train faster. Yann LeCun in his paper called Efficient BackProp explains such factors that affect the backpropagation algorithm in neural networks. To understand this consider the following. Assume that all the values in a weight matrix are positive. These weights are updated during backprop by say a factor d which can be positive or negative. As a result, these weights can only all decrease or all increase together for a given input pattern. Thus, if a weight vector must change direction it can only do so by zigzagging which is inefficient and thus very slow. | Another reason for using tanh is the relatively larger value of its derivative. Backprop computations result in multiplication of derivatives of the activation function multiple times depending upon the number of layers in the network. The maximum value for the derivative of a sigmoid function is 0.25 whereas that for tanh is 1. Hence, if the network is reasonably deep, the gradients of sigmoid are more likely to vanish than those of tanh. | . For sigmoid the graph is . . Why do we use tanh while calculating output gate values? . Although this is not that important but in case this question comes to your head, here&#39;s the answer. If you carefully look at the cell update equations, we have already applied a tanh activation in order to calculate the cell update candidate. Yet we again pass the cell state through tanh to get the new hidden state $h_{t}$. This is done to ensure that the values of $h_{t}$ lie between -1 and 1 because there&#39;s a chance that the values of the updated cell state $c_{t}$ might have exceeded 1 in previous additive computation. . Implementation . As pointed out earlier, the LSTM cell is a collection of 4 neural nets. In order to parallelize our computations and make use of a GPU it is better to compute values of the gates all at once. We need to linear layers: one for the current input and one for the previous hidden state. | So in the init method we initialize two linear layers. The out_features value for both these layers is 4 * hidden_dim owing to the number of gates. | You can relate this implementation with the equations above by expanding the multiplication of weights with the inputs. Assume that the weights associated with these layers are $W&#39;_{i}$ for current input and $W&#39;_{h}$ for previous hidden state. We perform the following computation initially and then break this computation into 4 matrices, one for each gate. | . W = $W&#39;_{i}$ $*$ $x_{t}$ + $W&#39;_{h}$ $*$ $h_{t-1}$ + $b&#39;_{i}$ + $b&#39;_{h}$ . The W matrix gets divided into 4 equal tensors $W_{i}$, $W_{f}$, $W_{c}$, $W_{o}$ which have been used in the equations above. This split is performed using torch.chunk. | . The rest of the code for LSTM cell is just converting the equations to code. . import torch from torch import nn . class LSTMCell(nn.Module): def __init__(self, input_dim, hidden_dim): super().__init__() self.input_layer = nn.Linear(in_features=input_dim, out_features=4 * hidden_dim) self.hidden_layer = nn.Linear(in_features=hidden_dim, out_features=4 * hidden_dim) def forward(self, current_input, previous_state): previous_hidden_state, previous_cell_state = previous_state weights = self.input_layer(current_input) + self.hidden_layer(previous_hidden) gates = weights.chunk(4,1) input_gate = torch.sigmoid(gates[0]) forget_gate = torch.sigmoid(gates[1]) output_gate = torch.sigmoid(gates[2]) cell_gate = torch.tanh(gates[3]) new_cell = (forget_gate * previous_cell_state) + (input_gate * cell_gate) new_hidden = output_gate * torch.tanh(new_cell) return new_hidden, (new_hidden, new_cell) . The following snippet basically takes an LSTMCell instance and calculates the output for the input sequence by applying a for loop. This is the same loop we had talked about initially in the post. . class LSTMLayer(nn.Module): def __init__(self, cell, *cell_args): super().__init__() self.cell = cell(*cell_args) def forward(self, input, state): inputs = input.unbind(1) outputs = [] for i in range(len(inputs)): output, state = self.cell(inputs[i], state) outputs += [output] return torch.stack(outputs, dim=1), state . lstm = LSTMLayer(LSTMCell, 100, 100) . Acknowledgements and References . This blog post is merely a combination of a variety of great resources available on the internet. The code is heavily drawn from the fastai foundations notebook by Jeremy Howard who does a great job in explaining the inner workings of each component. Figures are majorly taken from Chris Olah&#39;s evergreen post on LSTMs. All the references and links have been listed below to the best of my knowledge. Thank you! . https://colah.github.io/posts/2015-08-Understanding-LSTMs/ | https://github.com/fastai | https://github.com/emadRad/lstm-gru-pytorch/blob/master/lstm_gru.ipynb | https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21 | https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e | https://www.researchgate.net/figure/5-Activation-functions-in-comparison-Red-curves-stand-for-respectively-sigmoid_fig10_317679065 | https://stats.stackexchange.com/questions/368576/why-do-we-need-second-tanh-in-lstm-cell | https://www.quora.com/In-an-LSTM-unit-what-is-the-reason-behind-the-use-of-a-tanh-activation | https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm | https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function | https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function | http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf |",
            "url": "https://kushalj001.github.io/black-box-ml/lstm/pytorch/gates/vanishing%20gradient/2019/12/28/Understanding-LSTMs.html",
            "relUrl": "/lstm/pytorch/gates/vanishing%20gradient/2019/12/28/Understanding-LSTMs.html",
            "date": " • Dec 28, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Understanding Word Embeddings",
            "content": "Introduction . Words are an inseparable part of human lives. We communicate verbally, write letters and mail, and even think using words. We can do all of this because we have an understanding of the intrinsic meaning that each word has. Computers on the other hand are not good at understanding words and hence natural language. But they are good at processing large matrices/tensors of floating point values. Hence, humans have always tried to encode the words using numbers in some way that the computers can comprehend and work on challenging tasks. This encoding is usually called a word embedding. Its basically a vector that represents various features or characteristics of the word across its dimensions. In this post, I intend to explain in detail the working of two techniques that have been used widely to calculate such word embeddings viz. word2vec and GloVe. There are multiple reasons to write this post. . Most of us know how to use these word embeddings in code and build complex architectures using them. However, we often ignore the details of the model and how these vectors were trained in the first place. It is equally important to understand and appreciate them. | For a long time I had the misconception that word2vec and GloVe are somewhat similar with some tweaks here and there. But there are a lot of fundamental differences in their approaches to calculate word-vectors. | In this post I&#39;ll briefly explain word2vec and then move on to GloVe dive deep into the model details. . Word2Vec . word2vec uses a pseudo neural network to calculate continuous vector representations for words. Why pseudo? Because we don&#39;t actually use the outputs of the trained neural network. The neural network is trained to perform a pseudo or fake task. The task is formulated as done below. . The network is trained using word pairs from large text corpus. For each word in a sentence, we generate word pairs by looking at a fixed number of words before and after the current word (or the input word). This fixed number of words is also known as the window size. This can be 2, 3, 4 or 5. Window size of 3 means that we look at 3 words before and 3 words after the current word $w_{t}$. Which means the context words are ($w_{t-3}$, $w_{t-2}$, $w_{t-1}$, $w_{t+1}$, $w_{t+2}$, $w_{t+3}$). The word-pairs hence generated would be [($w_{t}$,$w_{t-3}$), ($w_{t}$,$w_{t-2}$), ... ($w_{t}$,$w_{t+3}$)]. The word-pairs generated are used to train the network. That is, $w_{t}$ is given as input to the model and the output is one of the words from the context. | . Assume that the vocabulary size of the corpus is 10,000. This means that there are 10,000 unique words in the corpus. The input to the network is a one-hot encoded vector representing the input word. The network has one hidden layer with 300 neurons. The output layer has 10,000 neurons, one for each word, with a softmax activation. No activation is used in the hidden layer. The presence of softmax means that the model will actually output probabilities for 10,000 words. This probability is the probability of the word at that index being the nearbuy or a context word for the input/current word. Intuitively, words that occur near the input word multiple times in the corpus will have a larger probability than others. | For example, consider the word &quot;Obama&quot;. It is more likely to be surrounded by words like &quot;Barack&quot;, &quot;USA&quot;, &quot;President&quot; etc. rather than words like &quot;juice&quot;, &quot;rabbit&quot; etc. The network hence captures various features of the word statistically by looking at the local context of the word. | Now coming back to the pseudo task. Once this network is trained for all the word pairs in the corpus, we simply remove the output layer. The hidden layer is associated with a matrix of size (10,000 X 300), one 300 dimensional vector for each word in the corpus. After backpropagation, the values of this matrix represent our word vectors. So the proposed task was fake because we never used the output layer. We just wanted to learn representations of the words. | . The model discussed above is the skip gram model in which we are predicting the context given the input. The authors also proposed another model in the same paper called continuous bag-of-words (CBOW) model where we predict the input word from the context. A schematic diagram for both these models as given in the paper is shown below . Subsampling . word2vec has been trained on a large corpus extracted from Google News that contains around 100 billion tokens. In such a large corpus, there are bound to exist very high frequency words that contribute very little to the training process. For exxample words like &quot;the&quot;, &quot;and&quot;, &quot;a&quot; etc. might occur in many context windows and hence be a part of many word-pairs. Thus the authors introduced subsampling that deletes words with high frequency from the corpus. Basically, each word is assigned a probability of whether it will be kept or dropped from the text. For example, if &quot;the&quot; is deleted from a sentence, it will generate fewer word-pairs for training and hence reduce training time. . Negative Sampling . word2vec model has 300 X 10,000 weight values in total. For each training sample, all these weights will get tweaked very slightly. This will happen for all the word-pairs generated in our text. That is, only for one word (ground truth) the output should be 1 and for the rest of the thousand words it should be 0. This would make the training very slow and is not even required since each training sample would not affect the a large fraction of weights significantly. Hence, for each training sample, we choose 5 negative words that we are not present in the input word&#39;s context. Weights are tweaked only for the label or the ground truth present in the word pair and these 5 negative words. . Global Vectors (GloVe) . GloVe follows a more principled approach in calculating word-embeddings. The major difference between word2vec and GloVe is that the latter does not use a neural net for the task. The authors develop a strong mathematical model to learn the embeddings. GloVe also overcomes the drawbacks of previous techniques used to calculate word-embeddings. Before word2vec, statistical methods like Latent Semantic Analysis (LSA) were used to approximate embeddings for terms in a document. These methods took into account global count statistics of the dataset. However, the vectors derived from such methods did not capture the meaning of the words like word2vec does and hence performed poorly in tasks like syntactic and semantic similarity and word analogies like &quot;king - queen + woman = man&quot;. Meanwhile word2vec took into consideration the local context of words but failed to account for the global count satistics of the dataset. The main aim of GloVe was to combine the two approaches to learn word-vectors. Quoting from the paper, . The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word representations, and although many such methods now exist, the question still remains as to how meaning is generated from these statistics, and how the resulting word vectors might represent that meaning. . Our model efﬁciently leverages statistical information by training only on the non-zero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. . As highlighted above, GloVe derives its training data by calculating a word-word co-occurence matrix. We&#39;ll show how this is calculated below with a toy example. But its important to understand the difference between this method and that used by word2vec. Word2vec generated training samples by forming word-pairs for all the words in a local context window completely ignoring the global statistics of the words that occur in the context window. . Co-occurence matrix . Consider the sentence - &quot;Winter is coming and it is here.&quot;. We construct a co-occurence matrix for this sentence with a context window size of 2. For &quot;winter&quot;, the context words are &quot;is&quot; and &quot;coming&quot;. Hence, we put 1 in the respective box. We also assume that the word itself is part of its context and hence increment the count in the diagonal of the matrix. . Derivation . We&#39;ll now get into the derivation of the GloVe model. The following section contains equations taken from the paper directly. Although I&#39;ll try my best to explain the significance and meaning of each equation, the derivation can get a bit daunting. Let&#39;s start by defining some notations. . Let the word-word co-occurrence table be denoted by $X$. Each entry in the table $X_{ij}$ denotes the number of times word $j$ occurs in the context of word $i$. . | Let $X_{i}$ be the number of times any word appears in the context of word $i$. That is the total number of distinct words throughout the corpus that appear in word $i$&#39;s context. Therefore, $X_{i}$ = $ sum_{k}$$X_{ik}$ where $k$ are the different words that appear in $i$&#39;s context throughout the dataset. . | Let $P_{ij}$ = $P (j | i)$ = $X_{ij}$ / $X_{i}$. This defines the probability that word $j$ appears in the context of word $i$. $P_{ij}$ is also called as the co-occurrence probability. . By calculating this probability, we are actually incorporating the global statistics of the dataset. Word2vec ignores the denominator of $P_{ij}$ calculated above. . | . Consider the following calculations for a particular dataset. The following example will help us understand how can we derive meaning of words by the ratio of probabilities calculated above. . The above matrix shows raw probabilities and ratios for two words viz. ice and steam. These words are topically concerned with the thermodynamic phases of water. The first 2 rows show calculations of the the probability $P_{ik}$ where $k$ are some words from the dataset called probe words. The last row shows the ratio of probabilities calculated in the first two rows. The hypothesis is as follows: . The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$. . Among the probe words, &quot;solid&quot; is closely related to the word &quot;ice&quot; (word $i$) and &quot;gas&quot; is closely related to &quot;steam&quot; (word $j$). As can be seen in the table above, the ratio $P_{ik}$/$P_{jk}$ is much greater than 1 (8.9) for words that are similar to &quot;ice&quot; ($i$) and much smaller than 1 (0.085) for words that are related to &quot;steam&quot; ($j$). Moreover, for words that are equally related to both &quot;ice&quot; and &quot;steam&quot; like &quot;water&quot; and words that not related to any off the two words (like &quot;fashion&quot;) have the ratio around 1. Therefore, the hypothesis stated above is true in the sense that ratios of co-occurrence probabilities gives a better sense about the meaning of a word than raw probabilities. . Therefore, this seems to be a good starting point for learning word vectors. We need to learn a function $F$, parameterized by 3 word vectors $w_{i}$, $w_{j}$ and $ tilde w_{k}$ such that, | . $w_{i}$, $w_{j}$ and $ tilde w_{k}$ belong to $R^{ d}$. $ tilde w_{k}$ represent separate context words or the probe words as discussed above. . There are a large number of possibilities for the function $F$. So lets impose some constraints on the model. The purpose of this model is to learn embeddings or feature vectors for words. Once learned, these vectors can be projected into a vector space just like we project 2 and 3 dimensional vectors in a cartesian plane. These vector spaces are inherently linear in nature because a vector is ultimately just a line. The above discussion leads us to our first constraint. The most common way to compare two linear structures is to calculate their difference. Thus we can restrict our function as, | . In the above equation, the right hand side is a ratio which is scalar and the arguments are vectors. While, learning a complex function $F$ that does the above mapping is possible but that would introduce non-linearities in our model because that&#39;s what neural nets do. We don&#39;t wish to obscure the linear structure that our model captures. Therefore, we take a dot product of the arguments on the left hand side to make it a scalar. | . The next constraint is that of symmetry. The distinction between a word and the context words is arbitary and hence they can be exchanged with each other. This is easy to understand. If &quot;water&quot; and &quot;gas&quot; can be used as context words for &quot;steam&quot;, then &quot;steam&quot; can also be used as a context word for &quot;water&quot; and &quot;gas&quot;. This symmetry is also evident in our co-occurrence matrix $X$ which is symmetric about the diagonal. Symmetry implies that $w$ $&lt;-&gt;$ $ tilde w$. To restore this symmetry, we require that the function $F$ is a homomorphism between groups $(R, +)$ and $(R_{&gt;0}, *)$. | . Let&#39;s define a group and homomorphism. A group $(G, *)$ is set of elements, that is closed under a particular operation $*$. That is if $x$ and $y$ belong to $G$ then, $z$ = $x$ $y$ also belongs to $G$. Each group has an inverse such that for all $x$, $x^{-1}x$ = $e$. Also, for all elements in a group, there exists an identity such that $xe = ex$ = $x$. Function $F$ is a homomorphism between 2 groups $(G, )$ and $(H, @)$ such that $F:G -&gt;H$ if $F(xy) = F(x) @ F(y)$. . Coming back to the equation of the model, homomorphism for $F$ can be explained as . begin{equation} F((w^{T}_{i} - w^{T}_{j}) tilde w_{k}) = F(w^{T}_{i} tilde w_{k} + (-w^{T}_{j} tilde w_{k})) end{equation} begin{equation} F(w^{T}_{i} tilde w_{k} + (-w^{T}_{j} tilde w_{k})) = F(w_{i}^{T} tilde w_{k}) * F(-w_{j}^{T} tilde w_{k}) end{equation} begin{equation} F(w_{i}^{T} tilde w_{k}) * F(-w_{j}^{T} tilde w_{k}) = F(w_{i}^{T} tilde w_{k}) * F(w_{j}^{T} tilde w_{k})^{-1} end{equation} The above equation gives the following result, . The above equation is solved by equation $(3)$, . And the exponential function is the solution for equation $(4)$, therefore we have the following, . . he symmetry is still not restored because of the $log(X_{i})$ term on the right hand side. Since, this term is independent of $k$, we can absorb it into bias $b_{i}$. Also introducing a bias $ tilde b_{k}$ for $ tilde w_{k}$, we finally have the following equation, . . The above equation gives us a cost function for a new weighted least squares regression model. The weighting function $f(X_{ij})$ takes into consideration the co-occurrences of words. . References and Acknowledgements . This post is heavily derived from the respective research papers of the techniques. Figures and equations are taken from the papers and blogs referenced below. Chris McCormick&#39;s posts on word2vec are still the best explanations for the topic and I strongly recommend the reader to read those. . https://nlp.stanford.edu/pubs/glove.pdf | https://arxiv.org/abs/1301.3781 | https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf | http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ | http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ | https://math.stackexchange.com/questions/2580647/what-does-homomorphism-mean-in-the-glove-paper/2580648 | https://datascience.stackexchange.com/questions/27042/glove-vector-representation-homomorphism-question | https://www.youtube.com/watch?v=g7L_r6zw4-c | https://towardsdatascience.com/word-embedding-part-ii-intuition-and-some-maths-to-understand-end-to-end-glove-model-9b08e6bf5c06 |",
            "url": "https://kushalj001.github.io/black-box-ml/word2vec/glove/word-embeddings/nlp/2019/11/13/Understanding-Word-Embeddings.html",
            "relUrl": "/word2vec/glove/word-embeddings/nlp/2019/11/13/Understanding-Word-Embeddings.html",
            "date": " • Nov 13, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kushalj001.github.io/black-box-ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kushalj001.github.io/black-box-ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}