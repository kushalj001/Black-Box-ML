{
  
    
        "post0": {
            "title": "Building Sequential Models in PyTorch",
            "content": "Introduction . The aim of this post is to enable beginners to get started with building sequential models in PyTorch. PyTorch is one of the most widely used deep learning libraries and is an extremely popular choice among researchers due to the amount of control it provides to its users and its pythonic layout. I am writing this primarily as a resource that I can refer to in future. This post will help in brushing up all the basics of PyTorch and also provide a detailed explanation of how to use some important torch.nn modules. We will be implementing a common NLP task - sentiment analysis using PyTorch and torchText. We will be building an LSTM network for the task by using the IMDB dataset. Let&#39;s get started! . A Tensor based approach . Another motivation to write this post is to introduce neural nets from tensors&#39; persepective. Neural nets are, ultimately, a series of matrix/tensor multiplications. Each layer in a neural net expects an input in a specified format and gives the output in a specified format. These input and output formats are tensors of different shapes. The content of these tensors are well defined by the documentation of the concerned library. As such, while implementing neural nets, it becomes very important to understand the input and output shapes of tensors for all layers. The following tutorial is written based on this approach. . A short Introduction to NLP pipeline . I do not intend to go into details about all the preprocessing steps that are required for NLP tasks but for the sake of completeness, I&#39;ll summarize some important conceptual points. . Textual data usually requires some amount of cleaning before they can be fed to neural nets. Important cleaning steps include removal of HTML tags, punctuation, stopwords, numbers etc. Stopwords are high frequency words in a dataset that do not convey significant information to the network (e.g. and, of, the, a). | Lemmatization can be performed to improve results of the network. Lemmatization converts words to their root form or to their lemma which can be found in a dictionary. For example &quot;goes&quot; $-&gt;$ &quot;go&quot;. | Neural nets do not understand natural language. In fact the only thing they understand and can process are numbers. So for any NLP task, we need to convert out text data into a numerical format (numericalization). Each word in the dataset is assigned a numerical value. These words need to be represented as a vector. | There are two options to represent a word or a token as a feature vector. One-hot encoding: Here, the size of the feature vector equals that of the vocabulary of dataset. All the values are 0 except for the index that equals the numerical value of the word. | Embeddings: These can either be pre-trained(GloVe, word2vec) or be trained in parallel with the main task. The dimensions of such vectors are usually around 100-300. These transform the features of the word into a dense vector. | | . TorchText basics . Some key points about the structure of the library will serve as a good introduction and also help in following along the rest of the tutorial. . The most important class of torchtext is the Field class. The structure of datasets for different NLP tasks is different. For example, in a classification task we have text reviews that are sequential in nature and a sentiment label corresponding to each review which is binary in nature (+ or -). In machine translation or summarization, both the input and output are sequential. The Field class handles all such types of datasets. Therefore, we initialize Field objects for each type of data format present in our dataset. . In sentiment analysis we need two Field instances - one for text review and other for labels. For labels we use LabelField which inherits from Field. Following are some important parameters you might need while initializing a Field class. . tokenize : function used to tokenize the text. This can either be a custom function or passing &#39;spacy&#39; uses the SpaCy tokenizer. | init_token : prepends this token in the beginning of each example. e.g. &lt; sos &gt;. | eos_token : appends this token in the end of each example. e.g. &lt; eos &gt;. | fix_length : fixed, predefined length to which all the examples in field will be padded. | batch_first: gives data in tensors that have batch dimension as the first dimension. | . Some important methods: . pad() : This method pads the examples to fix_length if provided as a parameter. If not, it calculates the length of the largest example in a batch and pads the sequences to that length. | build_vocab(): The Field class holds an instance of Vocab class. This class is responsible for creating a vocabulary from the field data and creating mappings stoi (string to int) and itos (int to string) for each word. | . To summarize, the Field class numericalizes the text data and provides it in form of tensors so that they can be used easily with neural nets. . import torchtext from torchtext import data, datasets from torch import nn import torch import torch.optim as optim . # creating field objects for text and labels review = data.Field(tokenize=&#39;spacy&#39;, batch_first=True) sentiment = data.LabelField(dtype=torch.float, batch_first=True) # loading the IMDB dataset train_data, test_data = datasets.IMDB.splits(text_field=review, label_field=sentiment) . print(vars(train_data.examples[4])) . {&#39;text&#39;: [&#39;This&#39;, &#39;is&#39;, &#39;not&#39;, &#39;the&#39;, &#39;typical&#39;, &#39;Mel&#39;, &#39;Brooks&#39;, &#39;film&#39;, &#39;.&#39;, &#39;It&#39;, &#39;was&#39;, &#39;much&#39;, &#39;less&#39;, &#39;slapstick&#39;, &#39;than&#39;, &#39;most&#39;, &#39;of&#39;, &#39;his&#39;, &#39;movies&#39;, &#39;and&#39;, &#39;actually&#39;, &#39;had&#39;, &#39;a&#39;, &#39;plot&#39;, &#39;that&#39;, &#39;was&#39;, &#39;followable&#39;, &#39;.&#39;, &#39;Leslie&#39;, &#39;Ann&#39;, &#39;Warren&#39;, &#39;made&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;,&#39;, &#39;she&#39;, &#39;is&#39;, &#39;such&#39;, &#39;a&#39;, &#39;fantastic&#39;, &#39;,&#39;, &#39;under&#39;, &#39;-&#39;, &#39;rated&#39;, &#39;actress&#39;, &#39;.&#39;, &#39;There&#39;, &#39;were&#39;, &#39;some&#39;, &#39;moments&#39;, &#39;that&#39;, &#39;could&#39;, &#39;have&#39;, &#39;been&#39;, &#39;fleshed&#39;, &#39;out&#39;, &#39;a&#39;, &#39;bit&#39;, &#39;more&#39;, &#39;,&#39;, &#39;and&#39;, &#39;some&#39;, &#39;scenes&#39;, &#39;that&#39;, &#39;could&#39;, &#39;probably&#39;, &#39;have&#39;, &#39;been&#39;, &#39;cut&#39;, &#39;to&#39;, &#39;make&#39;, &#39;the&#39;, &#39;room&#39;, &#39;to&#39;, &#39;do&#39;, &#39;so&#39;, &#39;,&#39;, &#39;but&#39;, &#39;all&#39;, &#39;in&#39;, &#39;all&#39;, &#39;,&#39;, &#39;this&#39;, &#39;is&#39;, &#39;worth&#39;, &#39;the&#39;, &#39;price&#39;, &#39;to&#39;, &#39;rent&#39;, &#39;and&#39;, &#39;see&#39;, &#39;it&#39;, &#39;.&#39;, &#39;The&#39;, &#39;acting&#39;, &#39;was&#39;, &#39;good&#39;, &#39;overall&#39;, &#39;,&#39;, &#39;Brooks&#39;, &#39;himself&#39;, &#39;did&#39;, &#39;a&#39;, &#39;good&#39;, &#39;job&#39;, &#39;without&#39;, &#39;his&#39;, &#39;characteristic&#39;, &#39;speaking&#39;, &#39;to&#39;, &#39;directly&#39;, &#39;to&#39;, &#39;the&#39;, &#39;audience&#39;, &#39;.&#39;, &#39;Again&#39;, &#39;,&#39;, &#39;Warren&#39;, &#39;was&#39;, &#39;the&#39;, &#39;best&#39;, &#39;actor&#39;, &#39;in&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;,&#39;, &#39;but&#39;, &#39;&#34;&#39;, &#39;Fume&#39;, &#39;&#34;&#39;, &#39;and&#39;, &#39;&#34;&#39;, &#39;Sailor&#39;, &#39;&#34;&#39;, &#39;both&#39;, &#39;played&#39;, &#39;their&#39;, &#39;parts&#39;, &#39;well&#39;, &#39;.&#39;], &#39;label&#39;: [&#39;pos&#39;]} . # dividing the training set further into a train and validation set train_data, valid_data = train_data.split() . # some important parameters VOCAB_SIZE = 25000 BATCH_SIZE = 64 device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) # build vocabulary for the fields review.build_vocab(train_data, max_size=VOCAB_SIZE) sentiment.build_vocab(train_data) # create iterators for the dataset. iterators enable looping through the dataset easily train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits( (train_data, valid_data, test_data), batch_size = BATCH_SIZE, device = device) . x = next(iter(train_iterator)) print(x.text.shape) print(x.label.shape) . torch.Size([64, 1201]) torch.Size([64]) . Implementing the model . Let&#39;s begin by understanding the layers that are going to be used in this model. We need to know 3 things about each layer in PyTorch - . parameters : used to instantiate the layer. These are the keyword args required to create an object of the class. | inputs : tensors passed to instantiated layer during model.forward() call | outputs : output of the layer | . Embedding layer (nn.Embedding) . This layer acts as a lookup table or a matrix which maps each token to its embedding or feature vector. This module is often used to store word embeddings and retrieve them using indices. . Parameters . num_embeddings: size of vocabulary of the dataset. Number of words in the vocab. | embedding_dim : size of embedding vector for each word. 300 for word2vec. Each word will be mapped to a 300 (say) dimensional vector | . Inputs and outputs . Embedding layer can accept tensors of aribitary shape, denoted by [ * ] and the output tensor&#39;s shape is [ * ,H], where H is the embedding dimension of the layer. For example in case of sentiment analysis, the input will be of shape [batch_size, seq_len] and the output shape will be [ batch_size, seq_len, embedding_dim ]. Intuitively, it replaces each word of each example in the batch by an embedding vector. . LSTM Layer (nn.LSTM) . Parameters . input_size : The number of expected features in input. This means the dimension of the feature vector that will be input to an LSTM unit. For most NLP tasks, this is the embedding_dim because the words which are the input are represented by a vector of size embedding_dim. | hidden_size : Number of features you want the LSTM to learn about the pattern of your data. | num_layers : Number of layers in the LSTM network. If num_layers = 2, it means that you&#39;re stacking 2 LSTM layers. The input to the first LSTM layer would be the output of embedding layer whereas the input for second LSTM layer would be the output of first LSTM layer. | batch_first : If True then the input and output tensors are provided as (batch_size, seq_len, feature). | dropout : If provided, applied between consecutive LSTM layers except the last layer. | bidirectional : If True, it becomes a bidirectional LSTM. That is it reads the sequence from both the directions. The forward direction starts from $x_{0}$ and goes till $x_{n}$ and backward direction goes from $x_{n}$ to $x_{0}$. | . seq_len mentioned above is the length of the input sentence. This will be the same for all the examples within a single batch. For the rest of this post we are going to take batch_first = True . Inputs . input : Shape of tensor is [batch_size, seq_len input_size] if batch_first = True. This is usually the output from the embedding layer for most NLP tasks. | h_0 : [batch_size, num_layers * num_directions, hidden_size] Tensor containing initial hidden state for each element in batch. | c_0 : [batch_size, num_layers * num_directions, hidden_size] Tensor containing initial cell state for each element in batch. | . Outputs . output : [batch_size, seq_len, num_directions * hidden_size] Tensor containing the output features (h_t) from the last layer of the LSTM, for each t. | h_n : [num_layers * num_directions, batch, hidden_size]: tensor containing the hidden state for t = seq_len. | c_n : [num_layers * num_directions, batch, hidden_size]: tensor containing the cell state for t = seq_len. | . Understanding the outputs of the LSTM can be a bit difficult initially. The following diagram clearly explains what each of the outputs mean. The following figure shows a general case of LSTM implementation. . The horizontal axis or the time axis determines the sequence length or the inputs at various time-steps. The vertical axis determines how many LSTM layers have been stacked together. Beginning from the first layer at the bottom, adding each layer increases the depth of the network. The number of layers are denoted by w in this figure. | . As evident, for each time-step t the LSTM unit takes in $h_{t-1}$, $c_{t-1}$ and $x_{t}$ and gives $h_{t}$ and $c_{t}$. The newly calculated $h_{t}$ and $c_{t}$ are passed to next LSTM unit as hidden and cell state of the sequnce seen so far. Simultaneously $h_{t}$ is also passed as the output for that time-step. This is used as the input for stacked layers above the current layer and finally to calculate predictions. Therefore the output of LSTM layer actually contains the hidden states of all time-steps passed through all the layers. Basically, it holds ( $h^{w}_{1}$, $h^{w}_{2}$, ... $h^{w}_{n}$). | . The final hidden and cell states denoted by $h_{n}$ and $c_{n}$ for all the layers are stacked in (h_n, c_n) output of the LSTM layer. Therefore (h_n, c_n) hold (($h^{1}_{n}$, $c^{1}_{n}$) , ($h^{2}_{n}$, $c^{2}_{n}$) ... ($h^{w}_{n}$, $c^{w}_{n}$)) where w is number of layers stacked in the LSTM. | . INPUT_SIZE = len(review.vocab) HIDDEN_SIZE = 128 EMBEDDING_DIM = 100 DROPOUT = 0.4 NUM_LAYERS = 1 BIDIRECTIONAL = False BATCH_SIZE = 64 OUTPUT_DIM = 1 . class SentimentLSTM(nn.Module): def __init__(self, input_size, hidden_size, embedding_dim, dropout, num_layers, output_dim): super().__init__() self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_dim) self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) self.dropout = nn.Dropout(p=dropout) self.linear = nn.Linear(in_features=hidden_size, out_features=output_dim) def forward(self, x): # x = [batch_size, seq_len] = [64, seq_len] as seq_len depends on the batch. embed = self.embedding(x) # embed = [batch_size, seq_len, embedding_dim] = [64, seq_len, 100] # These can be intuitively interpreted as: each example in the batch # has a length of seq_len and each word in the sequence is represented # by a vector of size 100. output, (hidden, cell) = self.lstm(embed) # output = [batch_size, seq_len, hidden_size] = [64, seq_len, 128] # hidden = [num_layers*num_directions, batch_size, hidden_size] = [1, 64, 128] # cell = [num_layers*num_directions, batch_size, hidden_size] = [1, 64, 128] # output is the concatenation of the hidden state from every time step, # whereas hidden is simply the final hidden state. # We verify this using the assert statement. output = output.permute(1,0,2) # hidden = [1, 64, 128] # output = [seq_len, 64, 128] assert torch.equal(output[-1,:,:], hidden.squeeze(0)) preds = self.linear(output[-1,:,:]) # preds = [64, 1] return preds . model = SentimentLSTM(INPUT_SIZE, HIDDEN_SIZE, EMBEDDING_DIM, DROPOUT, NUM_LAYERS, OUTPUT_DIM) . Training the model . A basic training loop involves the following steps . forward pass, i.e. multiplication of inputs with randomly initialized weights and carrying this on for all the layers. | calculate the loss of the predictions with the given target/ground truth. | calculate the gradient of the loss function and propagate it backwards to calculate gradients of all the layers | updating the weights of all the layers using the gradients so that the objective/loss function converges | . def accuracy(preds, y): &#39;&#39;&#39; Returns accuracy of the model.&#39;&#39;&#39; rounded_preds = torch.round(torch.sigmoid(preds)) correct = (rounded_preds == y).float() acc = correct.sum() / len(correct) return acc . # define the loss function criterion = nn.BCEWithLogitsLoss() # determine the gradient descent algorithm to be used for updating weights optimizer = optim.SGD(model.parameters(), lr = 1e-3) . # put model and loss on GPU model = model.to(device) criterion = criterion.to(device) . The following function performs the training and evaluation simultaneously. Each line has been well documented. . def fit(model, criterion, optimizer, train_iterator, valid_iterator): # define number of epochs epochs = 3 # go through each epoch for epoch in range(epochs): # put the model in training mode model.train() # initialize losses and accuaracy for every epoch train_loss = 0. train_acc = 0. valid_loss = 0. valid_acc = 0. print(&quot;Epoch: &quot;, epoch) # go through each batch from the dataset for batch in train_iterator: # calculate model predictions. squeeze(1) is done because the output of model is [64,1]. # criterion expects it to be of dimension [64]. preds = model(batch.text).squeeze(1) # calculate loss for the batch loss = criterion(preds, batch.label) # add batch loss to total loss for the epoch train_loss += loss.item() # calculate accuracy for the batch train_acc += accuracy(preds, batch.label).item() # backward prop for loss function and calc gradients of all the layers in the net loss.backward() # update the weights optimizer.step() # make the gradients zero before next step so that they don&#39;t accumulate optimizer.zero_grad() # print recorded results. Divide the total epoch loss/accuracy by the number of examples. print(&#39;Training loss: &#39;,train_loss / len(train_iterator)) print(&#39;Training accuracy: &#39;, train_acc / len(train_iterator)) # put the model in evaluation mode model.eval() # ensures that gradients are not calculated. Takes less time. with torch.no_grad(): # loop through the valid iterator for batch in valid_iterator: preds = model(batch.text).squeeze(1) loss = criterion(preds, batch.label) valid_loss += loss.item() valid_acc += accuracy(preds, batch.label).item() print(&#39;Validation loss: &#39;, valid_loss / len(valid_iterator)) print(&#39;Validation accuracy: &#39;, valid_acc / len(valid_iterator)) print(&#39;-&#39;) . fit(model, criterion, optimizer, train_iterator, valid_iterator) . Epoch: 0 Training loss: 0.6931361860602442 Training accuracy: 0.5035274374659044 Validation loss: 0.6933359392618729 Validation accuracy: 0.4931585452819275 - Epoch: 1 Training loss: 0.6932326938114027 Training accuracy: 0.5008553832116789 Validation loss: 0.693308207948329 Validation accuracy: 0.4962040961293851 - Epoch: 2 Training loss: 0.6932031631904797 Training accuracy: 0.5027046403745665 Validation loss: 0.6933137026883788 Validation accuracy: 0.4962040961293851 - . References and Acknowledgements . https://pytorch.org/docs/stable/nn.html | https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm | https://github.com/udacity/deep-learning-v2-pytorch | https://github.com/bentrevett |",
            "url": "https://kushalj001.github.io/black-box-ml/lstm/pytorch/torchtext/nlp/sentiment-analysis/2020/01/10/Building-Sequential-Models-In-PyTorch.html",
            "relUrl": "/lstm/pytorch/torchtext/nlp/sentiment-analysis/2020/01/10/Building-Sequential-Models-In-PyTorch.html",
            "date": " • Jan 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Understanding LSTMs",
            "content": "Introduction . Why another blog post on LSTMs? . LSTMs or the Long Short Term Memory have been around for a long time and there are many resources that do a great job in explaining the concept and its working in detail. So why another blog post? Multiple reasons. First, as a personal resource. I have read many blog posts on LSTMs and most of the times the same ones simply because I tend to forget the details after sometime. Clearly, my brain cannot handle long-term dependencies. Second, I am trying to put a lot of details in this post. I have come across blogs that explain LSTMs with the help of equations, some do it with text only and some with animations (hands down the best ones). In this post, I have attempted to include explanations for each component, equations, figures and an implementation of the LSTM layer from scratch. I hope this helps everyone get a wholesome understanding of the topic. . Recurrent Neural Nets . RNNs are one of the key flavours of deep neural networks. Unlike artificial neural networks that have multiple layers of neurons stacked one after another, it is not easily evident as to how recurrent nets are deep. Below is the figure of a rolled RNN. . The input $x_{t}$ is processed by the RNN for each time-step t and outputs a hidden state that captures and maintains the information of all the previous time-steps. We&#39;ll see this again as a for loop when we implement an LSTM layer. This for loop is exactly what makes RNNs deep. The unrolled version of the network is more widely used in literature and is shown below: . This deep nature is precisely the reason why such networks cannot practically model long-term dependencies. As the length of the input sequence increases, the number of matrix multiplications within the network increase. The weight updates of the earlier layers suffer as the gradients tend to vanish for them. Intuitively, think of this as multiplying a number less than zero with itself. The values become low exponentially. On the other hand if gradient values are larger than 1, these explode into large numbers that the computer can no longer make sense of. Consider this for intuition: . To deal with such issues, we need a mechanism that enables the networks to forget the irrelevant information and hold on to the relevant one. Enter LSTMs. . Understanding the LSTM cell . Before we get into the abstract details of the LSTM, it is important to understand what the black box actually contains. The LSTM cell is nothing but a pack of 3-4 mini neural networks. These networks are comprised of linear layers that are parameterized by weight matrices and biases. The values of these weights are learnt by backpropagation. The following figure shows an LSTM cell with labelled gates and all the computations that take place inside the cell. Each cell has 3 inputs: the current token $x_{t}$, previous hidden state $h_{t-1}$ and the previous cell state $c_{t-1}$ and 2 outputs: the updated hidden state $h_{t}$ and cell state $c_{t}$. . The forget gate . This gate takes in the current input $x_{t}$ and the previous hidden state $h_{t-1}$, multiplies them with a weight matrix of the forget gate $W_{f}$ (also adds a bias $b_{f}$) and applies a sigmoid activation. From an implementational point of view, $W_{f}$ and $b_{f}$ are the values associated with a simple linear layer. The sigmoid function restricts the input values between 0 and 1. This output is then multiplied with the cell state. Intuitively, given the current input this gate tells the cell to remove or forget some information if the sigmoid output is close to 0 and keep the information if the sigmoid output is close to 1. The equation for this gate is: . The input and the cell gate . The input gate is used to decide that given the current input what information is important and should be stored in the cell state. The calculations of this gate are similar to those of the forget gate. The cell gate and the input gate work closely together to perform a very specific function. This function is to update the previous cell state. To do so, the cell gate proposes an update candidate. You can think of this update candidate as a proposed new cell state. To calculate the cell gate output, a tanh activation is used (more on this later). The equation for the cell state is: . Cell update . We cannot simply replace the new proposed cell state and eliminate the previous cell state. This is because the previous state might contain some important information about the previous inputs the LSTM layer has seen. This is basically the main purpose of recurrent networks - to hold on to relevant information from the past. Hence, we adopt a very elegant approach to update the cell state. From the previous blocks, we know the following: Given the current input, the forget gate decides what information from the previous cell state can be forgotten. This is done by multiplying the forget gate output $f_{t}$ with the previous cell state $c_{t-1}$. Also, the input gate determines what information from the current input is relevant. Product of $i_{t}$ and the proposed cell state would give us important parts from the proposed cell state. We thus combine these two to yeild the following cell update equation: . Output gate . This gate is used to calculate the hidden state for the next time step. Again the current input $x_{t}$ and previous hidden state $h_{t-1}$ are multiplied by a weight matrix $W_{o}$ and passed through sigmoid activation. To update the hidden state of the cell, it might be a good idea to incorporate some information from the newly updated cell state. Therefore, the result of the output gate $o_{t}$ is multiplied with the updated cell state $c_{t}$ after passing $c_{t}$ through tanh activation (explained later). The equation for this gate is: . Why do we use tanh for calculating cell state? . We left this part rather abruptly while talking about the cell gate. The question of as to why tanh has been preferred over sigmoid and even ReLU has been a hotly debated one. My research led me to multiple sources and reasons. . One of the key reasons as to why tanh is preferred is its range [-1, 1] and the fact that it is zero-centered. These properties enable the neural net to converge faster and hence train faster. Yann LeCun in his paper called Efficient BackProp explains such factors that affect the backpropagation algorithm in neural networks. To understand this consider the following. Assume that all the values in a weight matrix are positive. These weights are updated during backprop by say a factor d which can be positive or negative. As a result, these weights can only all decrease or all increase together for a given input pattern. Thus, if a weight vector must change direction it can only do so by zigzagging which is inefficient and thus very slow. | Another reason for using tanh is the relatively larger value of its derivative. Backprop computations result in multiplication of derivatives of the activation function multiple times depending upon the number of layers in the network. The maximum value for the derivative of a sigmoid function is 0.25 whereas that for tanh is 1. Hence, if the network is reasonably deep, the gradients of sigmoid are more likely to vanish than those of tanh. | . For sigmoid the graph is . . Why do we use tanh while calculating output gate values? . Although this is not that important but in case this question comes to your head, here&#39;s the answer. If you carefully look at the cell update equations, we have already applied a tanh activation in order to calculate the cell update candidate. Yet we again pass the cell state through tanh to get the new hidden state $h_{t}$. This is done to ensure that the values of $h_{t}$ lie between -1 and 1 because there&#39;s a chance that the values of the updated cell state $c_{t}$ might have exceeded 1 in previous additive computation. . Implementation . As pointed out earlier, the LSTM cell is a collection of 4 neural nets. In order to parallelize our computations and make use of a GPU it is better to compute values of the gates all at once. We need to linear layers: one for the current input and one for the previous hidden state. | So in the init method we initialize two linear layers. The out_features value for both these layers is 4 * hidden_dim owing to the number of gates. | You can relate this implementation with the equations above by expanding the multiplication of weights with the inputs. Assume that the weights associated with these layers are $W&#39;_{i}$ for current input and $W&#39;_{h}$ for previous hidden state. We perform the following computation initially and then break this computation into 4 matrices, one for each gate. | . W = $W&#39;_{i}$ $*$ $x_{t}$ + $W&#39;_{h}$ $*$ $h_{t-1}$ + $b&#39;_{i}$ + $b&#39;_{h}$ . The W matrix gets divided into 4 equal tensors $W_{i}$, $W_{f}$, $W_{c}$, $W_{o}$ which have been used in the equations above. This split is performed using torch.chunk. | . The rest of the code for LSTM cell is just converting the equations to code. . import torch from torch import nn . class LSTMCell(nn.Module): def __init__(self, input_dim, hidden_dim): super().__init__() self.input_layer = nn.Linear(in_features=input_dim, out_features=4 * hidden_dim) self.hidden_layer = nn.Linear(in_features=hidden_dim, out_features=4 * hidden_dim) def forward(self, current_input, previous_state): previous_hidden_state, previous_cell_state = previous_state weights = self.input_layer(current_input) + self.hidden_layer(previous_hidden) gates = weights.chunk(4,1) input_gate = torch.sigmoid(gates[0]) forget_gate = torch.sigmoid(gates[1]) output_gate = torch.sigmoid(gates[2]) cell_gate = torch.tanh(gates[3]) new_cell = (forget_gate * previous_cell_state) + (input_gate * cell_gate) new_hidden = output_gate * torch.tanh(new_cell) return new_hidden, (new_hidden, new_cell) . The following snippet basically takes an LSTMCell instance and calculates the output for the input sequence by applying a for loop. This is the same loop we had talked about initially in the post. . class LSTMLayer(nn.Module): def __init__(self, cell, *cell_args): super().__init__() self.cell = cell(*cell_args) def forward(self, input, state): inputs = input.unbind(1) outputs = [] for i in range(len(inputs)): output, state = self.cell(inputs[i], state) outputs += [output] return torch.stack(outputs, dim=1), state . lstm = LSTMLayer(LSTMCell, 100, 100) . Acknowledgements and References . This blog post is merely a combination of a variety of great resources available on the internet. The code is heavily drawn from the fastai foundations notebook by Jeremy Howard who does a great job in explaining the inner workings of each component. Figures are majorly taken from Chris Olah&#39;s evergreen post on LSTMs. All the references and links have been listed below to the best of my knowledge. Thank you! . https://colah.github.io/posts/2015-08-Understanding-LSTMs/ | https://github.com/fastai | https://github.com/emadRad/lstm-gru-pytorch/blob/master/lstm_gru.ipynb | https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21 | https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e | https://www.researchgate.net/figure/5-Activation-functions-in-comparison-Red-curves-stand-for-respectively-sigmoid_fig10_317679065 | https://stats.stackexchange.com/questions/368576/why-do-we-need-second-tanh-in-lstm-cell | https://www.quora.com/In-an-LSTM-unit-what-is-the-reason-behind-the-use-of-a-tanh-activation | https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm | https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function | https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function | http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf |",
            "url": "https://kushalj001.github.io/black-box-ml/lstm/pytorch/gates/vanishing%20gradient/2019/12/28/Understanding-LSTMs.html",
            "relUrl": "/lstm/pytorch/gates/vanishing%20gradient/2019/12/28/Understanding-LSTMs.html",
            "date": " • Dec 28, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Understanding Word Embeddings",
            "content": "Introduction . Words are an inseparable part of human lives. We communicate verbally, write letters and mail, and even think using words. We can do all of this because we have an understanding of the intrinsic meaning that each word has. Computers on the other hand are not good at understanding words and hence natural language. But they are good at processing large matrices/tensors of floating point values. Hence, humans have always tried to encode the words using numbers in some way that the computers can comprehend and work on challenging tasks. This encoding is usually called a word embedding. Its basically a vector that represents various features or characteristics of the word across its dimensions. In this post, I intend to explain in detail the working of two techniques that have been used widely to calculate such word embeddings viz. word2vec and GloVe. There are multiple reasons to write this post. . Most of us know how to use these word embeddings in code and build complex architectures using them. However, we often ignore the details of the model and how these vectors were trained in the first place. It is equally important to understand and appreciate them. | For a long time I had the misconception that word2vec and GloVe are somewhat similar with some tweaks here and there. But there are a lot of fundamental differences in their approaches to calculate word-vectors. | In this post I&#39;ll briefly explain word2vec and then move on to GloVe dive deep into the model details. . Word2Vec . word2vec uses a pseudo neural network to calculate continuous vector representations for words. Why pseudo? Because we don&#39;t actually use the outputs of the trained neural network. The neural network is trained to perform a pseudo or fake task. The task is formulated as done below. . The network is trained using word pairs from large text corpus. For each word in a sentence, we generate word pairs by looking at a fixed number of words before and after the current word (or the input word). This fixed number of words is also known as the window size. This can be 2, 3, 4 or 5. Window size of 3 means that we look at 3 words before and 3 words after the current word $w_{t}$. Which means the context words are ($w_{t-3}$, $w_{t-2}$, $w_{t-1}$, $w_{t+1}$, $w_{t+2}$, $w_{t+3}$). The word-pairs hence generated would be [($w_{t}$,$w_{t-3}$), ($w_{t}$,$w_{t-2}$), ... ($w_{t}$,$w_{t+3}$)]. The word-pairs generated are used to train the network. That is, $w_{t}$ is given as input to the model and the output is one of the words from the context. | . Assume that the vocabulary size of the corpus is 10,000. This means that there are 10,000 unique words in the corpus. The input to the network is a one-hot encoded vector representing the input word. The network has one hidden layer with 300 neurons. The output layer has 10,000 neurons, one for each word, with a softmax activation. No activation is used in the hidden layer. The presence of softmax means that the model will actually output probabilities for 10,000 words. This probability is the probability of the word at that index being the nearbuy or a context word for the input/current word. Intuitively, words that occur near the input word multiple times in the corpus will have a larger probability than others. | For example, consider the word &quot;Obama&quot;. It is more likely to be surrounded by words like &quot;Barack&quot;, &quot;USA&quot;, &quot;President&quot; etc. rather than words like &quot;juice&quot;, &quot;rabbit&quot; etc. The network hence captures various features of the word statistically by looking at the local context of the word. | Now coming back to the pseudo task. Once this network is trained for all the word pairs in the corpus, we simply remove the output layer. The hidden layer is associated with a matrix of size (10,000 X 300), one 300 dimensional vector for each word in the corpus. After backpropagation, the values of this matrix represent our word vectors. So the proposed task was fake because we never used the output layer. We just wanted to learn representations of the words. | . The model discussed above is the skip gram model in which we are predicting the context given the input. The authors also proposed another model in the same paper called continuous bag-of-words (CBOW) model where we predict the input word from the context. A schematic diagram for both these models as given in the paper is shown below . Subsampling . word2vec has been trained on a large corpus extracted from Google News that contains around 100 billion tokens. In such a large corpus, there are bound to exist very high frequency words that contribute very little to the training process. For exxample words like &quot;the&quot;, &quot;and&quot;, &quot;a&quot; etc. might occur in many context windows and hence be a part of many word-pairs. Thus the authors introduced subsampling that deletes words with high frequency from the corpus. Basically, each word is assigned a probability of whether it will be kept or dropped from the text. For example, if &quot;the&quot; is deleted from a sentence, it will generate fewer word-pairs for training and hence reduce training time. . Negative Sampling . word2vec model has 300 X 10,000 weight values in total. For each training sample, all these weights will get tweaked very slightly. This will happen for all the word-pairs generated in our text. That is, only for one word (ground truth) the output should be 1 and for the rest of the thousand words it should be 0. This would make the training very slow and is not even required since each training sample would not affect the a large fraction of weights significantly. Hence, for each training sample, we choose 5 negative words that we are not present in the input word&#39;s context. Weights are tweaked only for the label or the ground truth present in the word pair and these 5 negative words. . Global Vectors (GloVe) . GloVe follows a more principled approach in calculating word-embeddings. The major difference between word2vec and GloVe is that the latter does not use a neural net for the task. The authors develop a strong mathematical model to learn the embeddings. GloVe also overcomes the drawbacks of previous techniques used to calculate word-embeddings. Before word2vec, statistical methods like Latent Semantic Analysis (LSA) were used to approximate embeddings for terms in a document. These methods took into account global count statistics of the dataset. However, the vectors derived from such methods did not capture the meaning of the words like word2vec does and hence performed poorly in tasks like syntactic and semantic similarity and word analogies like &quot;king - queen + woman = man&quot;. Meanwhile word2vec took into consideration the local context of words but failed to account for the global count satistics of the dataset. The main aim of GloVe was to combine the two approaches to learn word-vectors. Quoting from the paper, . The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word representations, and although many such methods now exist, the question still remains as to how meaning is generated from these statistics, and how the resulting word vectors might represent that meaning. . Our model efﬁciently leverages statistical information by training only on the non-zero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. . As highlighted above, GloVe derives its training data by calculating a word-word co-occurence matrix. We&#39;ll show how this is calculated below with a toy example. But its important to understand the difference between this method and that used by word2vec. Word2vec generated training samples by forming word-pairs for all the words in a local context window completely ignoring the global statistics of the words that occur in the context window. . Co-occurence matrix . Consider the sentence - &quot;Winter is coming and it is here.&quot;. We construct a co-occurence matrix for this sentence with a context window size of 2. For &quot;winter&quot;, the context words are &quot;is&quot; and &quot;coming&quot;. Hence, we put 1 in the respective box. We also assume that the word itself is part of its context and hence increment the count in the diagonal of the matrix. . Derivation . We&#39;ll now get into the derivation of the GloVe model. The following section contains equations taken from the paper directly. Although I&#39;ll try my best to explain the significance and meaning of each equation, the derivation can get a bit daunting. Let&#39;s start by defining some notations. . Let the word-word co-occurrence table be denoted by $X$. Each entry in the table $X_{ij}$ denotes the number of times word $j$ occurs in the context of word $i$. . | Let $X_{i}$ be the number of times any word appears in the context of word $i$. That is the total number of distinct words throughout the corpus that appear in word $i$&#39;s context. Therefore, $X_{i}$ = $ sum_{k}$$X_{ik}$ where $k$ are the different words that appear in $i$&#39;s context throughout the dataset. . | Let $P_{ij}$ = $P (j | i)$ = $X_{ij}$ / $X_{i}$. This defines the probability that word $j$ appears in the context of word $i$. $P_{ij}$ is also called as the co-occurrence probability. . By calculating this probability, we are actually incorporating the global statistics of the dataset. Word2vec ignores the denominator of $P_{ij}$ calculated above. . | . Consider the following calculations for a particular dataset. The following example will help us understand how can we derive meaning of words by the ratio of probabilities calculated above. . The above matrix shows raw probabilities and ratios for two words viz. ice and steam. These words are topically concerned with the thermodynamic phases of water. The first 2 rows show calculations of the the probability $P_{ik}$ where $k$ are some words from the dataset called probe words. The last row shows the ratio of probabilities calculated in the first two rows. The hypothesis is as follows: . The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$. . Among the probe words, &quot;solid&quot; is closely related to the word &quot;ice&quot; (word $i$) and &quot;gas&quot; is closely related to &quot;steam&quot; (word $j$). As can be seen in the table above, the ratio $P_{ik}$/$P_{jk}$ is much greater than 1 (8.9) for words that are similar to &quot;ice&quot; ($i$) and much smaller than 1 (0.085) for words that are related to &quot;steam&quot; ($j$). Moreover, for words that are equally related to both &quot;ice&quot; and &quot;steam&quot; like &quot;water&quot; and words that not related to any off the two words (like &quot;fashion&quot;) have the ratio around 1. Therefore, the hypothesis stated above is true in the sense that ratios of co-occurrence probabilities gives a better sense about the meaning of a word than raw probabilities. . Therefore, this seems to be a good starting point for learning word vectors. We need to learn a function $F$, parameterized by 3 word vectors $w_{i}$, $w_{j}$ and $ tilde w_{k}$ such that, | . $w_{i}$, $w_{j}$ and $ tilde w_{k}$ belong to $R^{ d}$. $ tilde w_{k}$ represent separate context words or the probe words as discussed above. . There are a large number of possibilities for the function $F$. So lets impose some constraints on the model. The purpose of this model is to learn embeddings or feature vectors for words. Once learned, these vectors can be projected into a vector space just like we project 2 and 3 dimensional vectors in a cartesian plane. These vector spaces are inherently linear in nature because a vector is ultimately just a line. The above discussion leads us to our first constraint. The most common way to compare two linear structures is to calculate their difference. Thus we can restrict our function as, | . In the above equation, the right hand side is a ratio which is scalar and the arguments are vectors. While, learning a complex function $F$ that does the above mapping is possible but that would introduce non-linearities in our model because that&#39;s what neural nets do. We don&#39;t wish to obscure the linear structure that our model captures. Therefore, we take a dot product of the arguments on the left hand side to make it a scalar. | . The next constraint is that of symmetry. The distinction between a word and the context words is arbitary and hence they can be exchanged with each other. This is easy to understand. If &quot;water&quot; and &quot;gas&quot; can be used as context words for &quot;steam&quot;, then &quot;steam&quot; can also be used as a context word for &quot;water&quot; and &quot;gas&quot;. This symmetry is also evident in our co-occurrence matrix $X$ which is symmetric about the diagonal. Symmetry implies that $w$ $&lt;-&gt;$ $ tilde w$. To restore this symmetry, we require that the function $F$ is a homomorphism between groups $(R, +)$ and $(R_{&gt;0}, *)$. | . Let&#39;s define a group and homomorphism. A group $(G, *)$ is set of elements, that is closed under a particular operation $*$. That is if $x$ and $y$ belong to $G$ then, $z$ = $x$ $y$ also belongs to $G$. Each group has an inverse such that for all $x$, $x^{-1}x$ = $e$. Also, for all elements in a group, there exists an identity such that $xe = ex$ = $x$. Function $F$ is a homomorphism between 2 groups $(G, )$ and $(H, @)$ such that $F:G -&gt;H$ if $F(xy) = F(x) @ F(y)$. . Coming back to the equation of the model, homomorphism for $F$ can be explained as . begin{equation} F((w^{T}_{i} - w^{T}_{j}) tilde w_{k}) = F(w^{T}_{i} tilde w_{k} + (-w^{T}_{j} tilde w_{k})) end{equation} begin{equation} F(w^{T}_{i} tilde w_{k} + (-w^{T}_{j} tilde w_{k})) = F(w_{i}^{T} tilde w_{k}) * F(-w_{j}^{T} tilde w_{k}) end{equation} begin{equation} F(w_{i}^{T} tilde w_{k}) * F(-w_{j}^{T} tilde w_{k}) = F(w_{i}^{T} tilde w_{k}) * F(w_{j}^{T} tilde w_{k})^{-1} end{equation} The above equation gives the following result, . The above equation is solved by equation $(3)$, . And the exponential function is the solution for equation $(4)$, therefore we have the following, . . he symmetry is still not restored because of the $log(X_{i})$ term on the right hand side. Since, this term is independent of $k$, we can absorb it into bias $b_{i}$. Also introducing a bias $ tilde b_{k}$ for $ tilde w_{k}$, we finally have the following equation, . . The above equation gives us a cost function for a new weighted least squares regression model. The weighting function $f(X_{ij})$ takes into consideration the co-occurrences of words. . References and Acknowledgements . This post is heavily derived from the respective research papers of the techniques. Figures and equations are taken from the papers and blogs referenced below. Chris McCormick&#39;s posts on word2vec are still the best explanations for the topic and I strongly recommend the reader to read those. . https://nlp.stanford.edu/pubs/glove.pdf | https://arxiv.org/abs/1301.3781 | https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf | http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ | http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ | https://math.stackexchange.com/questions/2580647/what-does-homomorphism-mean-in-the-glove-paper/2580648 | https://datascience.stackexchange.com/questions/27042/glove-vector-representation-homomorphism-question | https://www.youtube.com/watch?v=g7L_r6zw4-c | https://towardsdatascience.com/word-embedding-part-ii-intuition-and-some-maths-to-understand-end-to-end-glove-model-9b08e6bf5c06 |",
            "url": "https://kushalj001.github.io/black-box-ml/word2vec/glove/word-embeddings/nlp/2019/11/13/Understanding-Word-Embeddings.html",
            "relUrl": "/word2vec/glove/word-embeddings/nlp/2019/11/13/Understanding-Word-Embeddings.html",
            "date": " • Nov 13, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kushalj001.github.io/black-box-ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kushalj001.github.io/black-box-ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}