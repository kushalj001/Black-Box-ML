{
  
    
        "post0": {
            "title": "Generating Text with VAEs and Posterior Collapse",
            "content": "Introduction . This blog is about my recent research on generative modeling, variational autoencoders (VAEs) and, in particular, application of VAEs to text generation. VAEs work extremely well for images but have not seen the same success in text-generation. This is because VAE training for such models usually results in the model choosing a local optimum where it learns to completely ignore the latent variable. This phenomenon is referred to as the posterior collapse in literature. This blog assumes familiarity with the basics of latent variable models and VAEs. There are a lot of great resources that explain the ideas and the math behind VAEs. I begin with a small recap of latent variable models, the evidence lower bound (ELBO) and how VAEs model the probability distibutions of the ELBO. I then move on to the application of VAE models for text generation, optimization challenges and their proposed solutions. To do this, I explain 2 papers related to the topic which look at the problem in unique ways and propose different solutions for the same. I hope that by the end of this blog, the reader gains a solid understanding of the basics, motivations, challenges and the proposed solutions for training text-generating VAEs. . Latent Variable Models and Variational Autoencoders . Any dataset for a given problem comprises of datapoints $x_{i}$&#39;s. These datapoints are usually recorded as part of some data collection process or are simply generated as a byproduct of another process. For example, determining the average heights of people across different countries, weather predictions, classifying if a medical image shows a benign or a malignant tumor and so on. In all such cases, we would like to know more about the data generating process itself, rather than only having some samples of the process. However, this is not practically possible. Instead, we assume that the datapoints that we have come from some probability distribution and try to approximate it using the data. One can argue that by doing so, we are making a very strong assumption about the data coming from some well defined data generating process (probability distribution). But it would be very pessimistic to believe that everything is completely random. It would call for our nature being too dull (when we already know it is not) to not enforce some structure on the many happenings in our surroundings. Latent variable models (LVMs) assume that an observed datapoint $x_{i}$ is generated by a latent variable $z$ which we cannot observe. These latent variables capture some interesting structure about the data which is not obvious. A graphical model for the above idea can be described as $$z rightarrow x$$ . Evidence Lower Bound . VAEs are a class of LVMs that optimize the evidence lower bound (ELBO) using variational inference in a way that scales well for large datasets. This can be attributed to the reparameterization trick which enables us to use gradient optimization methods like stochastic gradient descent to optimize the lower bound. The lower bound equation after applying reparameterization can be written as . $$L( theta, phi) = E_{q_{ phi}(z|x)}[ log p_{ theta}( x | z)] - KL [q_{ phi}( z | x )) || p(z)]$$ . For the VAE, we have following definitions and assumptions, . $p(z)$ is the prior which is taken a standard multivariate gaussian, with mean ($ mu$) = 0 and variance $( sigma)$ = $I$. | $p_{ theta}(x | z)$ is a multivariate gaussian, parameterized by a neural net or a multi-layer perceptron (MLP). Given a multi-dimensional $z$, this MLP outputs the mean ($ mu_{ theta}$) and variance ($ sigma_{ theta}$) of the assumed gaussian. | $q_{ phi}(z | x)$ is the approximate posterior or the inference model that we use to estimate the true posterior $p_{ theta}(z | x)$. This is assumed to be a multivariate gaussian whose parameters are calculated by a neural net. It takes $x$ as input and outputs the mean ($ mu_{ phi}$) and the variance ($ sigma_{ phi}$) of the distribution. | . It is easy to see that the approximate posterior $q_{ phi}(z | x)$ acts as an encoder taking in a datapoint $x$ and giving out a latent representation of the input, $z$. Similarly, $p_{ theta}(x | z)$ acts as the decoder in VAE, taking in the encoded latent variable and again reconstructing the input example, $x$. . $$x xrightarrow{q_{ phi}(z | x)} z xrightarrow{p_{ theta}(x | z)} x$$ . VAE for Sentences . Language models assign probabilities to sequences of natural text and are used to predict the next best word based on previous context. The probability of a sequence with $m$ words can be modeled as, . $$ p(w_{1}, w_{2}... w_{m}) = prod_{i=1}^{m}p(w_{i} | w_{1}, ...w_{i-1}) $$ . The above equation can be simplified using some independence assumptions wherein we assume that the probability of next word does not depend on the entire sequence but only previous $n$ words. If $n=1$, we have a unigram language model, bigram if $n=2$ and so on. For a general n-gram model, we have the following approximation, . $$ prod_{i=1}^{m}p(w_{i} | w_{1}, w_{2}... w_{i-1}) approx prod_{i=1}^{n}p(w_{i}| w_{i-(n-1)}, ...w_{i-1}) $$ . Recurrent neural networks (RNNs) and its variants (LSTMs/GRUs) can model this factorization and work well for generating natural text. In fact, theoretically, RNNs can model any arbitrarily long and complex sequence representation without any independence assumptions mentioned above. They fail in practice however, due to a number of issues. Nevertheless, if RNNs work reasonably well, a natural argument would then be, why do we need latent variables to generate text? One of the motivations behind this is controlled generation of text. RNNs generate text at word or token level. They take a new input at each time step and emit out the next word based on the current input and several past inputs. It records the information from previous tokens in a continuously evolving hidden state vector. By doing so, it breaks the model structure into a series of next-word predictions and fails to capture sentence-level features like topic, high-level syntax and semantics. With VAE, we can use a latent variable to encode the latent space of sentences which captures interpretable high-level features and decode or generate words from that space. Following is an example that compares a standard encoder-decoder model and a VAE by interpolating vectors between two sentences (bold) vectors. We can see that halfway through, the standard model starts generating meaningless sentences because the decoding space for the model does not correspond to any reasonable semantics. Whereas, in case of the VAE, the intermediate sentences are semantically consistent. . Model . The model proposed by Bowman et al. is very similar in spirit to the original VAE. The only major difference is that the multivariate gaussians in the encoder and the decoder are parameterized by an LSTM model instead of a multi-layer perceptron. Like the VAE, they also use the reparameterization trick to enable backpropagation through the network. The proposed model can be explained by the figure below. . On the encoder side, we first get an embedding vector for each input token and pass it to an LSTM cell. We&#39;ll refer the tuple of hidden and cell state calculated by each LSTM unit as the hidden state for brevity. We use the hidden state vector of the final LSTM cell and consider it as the representation of the encoded sentence. This hidden state vector is then transformed by two linear layers, one to calculate the mean ($ mu_{ phi}$) and the other to calculate the variance ($ sigma_{ phi}$) of $q_{ phi}(z | x)$. The dimension of these linear layers is generally lesser than the hidden dimension of the LSTM cell. At this point, the reparameterization trick kicks in. To calculate $z$, we sample $ epsilon$ from a standard multivariate gaussian such that, . $$ epsilon sim mathcal{N}(0, I)$$ . $$ z = g_{ phi}( epsilon, x) $$ . $$ g_{ phi}( epsilon, x) = mu_{ phi}(x) + epsilon odot sigma_{ phi}(x) $$ . where $g_{ phi}$ is a differentiable transformation. By doing so, $z$ will also have the desired distribution as discussed earlier, . $$ q_{ phi}(z | x) sim mathcal{N}(z; mu_{ phi}(x), sigma_{ phi}(x))$$ . Once we have the latent variable vector, we use it to initialize the hidden state of the first LSTM cell in the decoder. The input transformations in the decoder are similar to those in encoder. At each time-step we decode a word by passing the LSTM output at that time-step to a linear layer with softmax whose output dimension equals the vocabulary size. This is not generally shown in diagrams and is understood implicitly. . Optimization challenges . Training the model proposed above is not as straightforward as it seems. Training VAE models with very powerful decoders like RNNs/LSTMs is difficult because they suffer with posterior collapse. Posterior collapse occurs when the approximate posterior $q_{ phi}(z | x)$ equals (or collapses into) the prior $p(z)$. This means that the KL term in the lower bound optimization goes to zero and you&#39;re essentially only optimizing the cross-entropy loss of the decoder. . $$ L( theta, phi) = E_{q_{ phi}(z|x)}[ log p_{ theta}( x | z)] - KL [q_{ phi}( z | x )) || p(z)] $$ . $$ L( theta, phi) = E_{q_{ phi}(z|x)}[ log p_{ theta}( x | z)] if q_{ phi}( z | x ) = p(z) $$ . This happens when the signal from the input $x$ to the posterior parameters is either too weak or too noisy. The decoder learns to ignore the information captured in the latent variable samples $z$ drawn from $q_{ phi}(z | x)$ altogether and results in very little signal passing from the encoder to the decoder. If the model learns to ignore $z$, the decoder is making predictions only based on the ground-truth inputs that we provide on the decoder side and is actually behaving as a general RNN language model (RNNLM). The decoding distribution can be described by the following equation, . $$ p(x | z) = prod_{i}p(x_{i} | z, x_{&lt;_{i}})$$ . As discussed previously, the RNN based decoder does not need $z$ to optimize the lower bound because it can model any arbitrary sequential representation based on past inputs. It is also important to understand the dynamics of the lower bound equation which can be broken into 2 parts: the data likelihood under the posterior (expressed as cross entropy) and the KL divergence of the posterior from the prior. The cross-entropy term is a negative value since we&#39;re calculating the expectation of the log of a probability distribution (which lies between 0 and 1). The KL (which is always &gt;=0) term is preceded with a negative sign which makes it negative too. Therefore the lower bound or the ELBO takes a negative value and maximizing it means that it should have a lower value. . So, technically, when the model suffers with posterior collapse, it is actually leading us to an easier optimization solution by making the KL term zero. But that kind of defeats the whole purpose of our premise! We do not want the model to learn only via the decoder. Instead we want the model to use the information captured in the latent variable to decode better representations that capture the high-level syntax and semantics of the text. Understanding the KL term can also be a bit counter-intuitive initially. The aim of the KL term is to drive the approximate posterior towards the prior, but that does not mean that we want them to be equal to each other. You can think of it as a regularization term. . Regularization is a technique used for tuning the function by adding an additional penalty term in the error function. The additional term controls the excessively fluctuating function such that the coefficients don’t take extreme values. . In our context, a model that encodes useful information in the latent variable $z$ should have a non-zero KL divergence and relatively small cross-entropy term. The cross-entropy term is reported as negative log-likelihood (NLL) for language modeling tasks in all the papers. A smaller cross-entropy term and a non-zero KL term would take care of all of our concerns. Hope is to capture enough useful information in the latent variable which helps the decoder to predict correctly and lower the cross-entropy term to a degree where even upon adding the non-zero KL term, it is still lesser than the local optimum attained via posterior collapse. $$ NLL + KL &lt; NLL (with posterior collapse) $$ . Proposed Solutions . There are two major categories of solutions to deal with posterior collapse in literature, . Alter the training procedure, wherein you change the training dyanmics to make sure that the latent variable is considered by the decoder while decoding. | Weaken the decoder, which means that we reduce information directly available to the decoder, such that it is forced to consider the latent variable $z$ for making decisions. There are many ways of doing this and some of them will be discussed below. Bowman et al. proposed the following solutions. | . KL Cost Annealing . This technique can be put under the first catergory of solutions to deal with posterior collapse. The KL term is multiplied by a variable weight which is 0 during the initial stages of training. The weight is then increased progressively during training until it becomes 1 which is when it trains according to the normal VAE equation. Intuitively, not penalizing the KL term initially allows the encoder to pack as much information as possible into the latent variable $z$ for the decoder to use when we gradually begin penalizing KL. . The graph above, taken directly from the paper summarizes the behaviour of the KL term and the variable weight. In the initial stages, when the variable weight is 0, the KL term spikes because the encoder, $q_{ phi}(z | x)$, can take in a lot of information without incurring any penalty. This drives $q_{ phi}(z | x)$ away from the prior, $p(z)$, hence the rise in KL. Then, as we gradually start increasing the variable weight, the value of KL term drops once the weight reaches 1. Finally, the KL term increases slowly as it converges and learns to pack information in $z$. The rate of increasing the variable weight is tuned as a hyperparameter. . Word Dropout . In vanilla dropout, we drop off some percentage of neurons from the neural network by setting the weights of these neurons as zero for that particular forward and backward pass. The idea behind dropout is to remove any dependencies between neurons and enable them to learn more robust features. It can also be thought of as a regularization technique since we are penalizing the model by not allowing it to use all the neurons and hence learn more complex features from the data. Word dropout proposed in Bowman et al. is similar in spirit. It is a way of weakening the decoder by reducing the information provided to the decoder directly. One way to implement this would be to dynamically update the embedding layer in the decoder. However, partially updating weights in an embedding layer during training is not as straightforward as it seems. Instead, word dropout is parameterized by a word keep rate $k epsilon [0,1]$. At $k = 0$, the decoder has no inputs and predicts only based on the previously generated words and the latent variable $z$. The authors replace the ground-truth/label words with UNK tokens randomly which forces the decoder to look at the latent variable to predict the next word correctly. Intuitively, word dropout implements something that humans do quite naturally. If we already know about the text document we&#39;re going to read, we tend to skip a few words and are still able to make sense out of it. We&#39;re essentially relying on some latent information about the document within our brains, which is analogous to the idea behind word dropout. . Lagging Inference Networks . The paper discussed in this section by He et al. presents a different perspective of the posterior collapse as to why it happens and proposes a slightly altered training approach to avoid it. Unlike many other papers in literature, the authors specifically focus on the true model posterior $p_{ theta}(z | x)$ and the approximate posterior $q_{ phi}(z | x)$ and analyze how does their interaction throughout the training process determines whether the model will suffer with posterior collapse. The approximate posterior, $q_{ phi}(z | x)$ (also referred to as the inference network in this paper) is used to estimate the true posterior and hence the lower bound optimization tries to drive $q_{ phi}(z | x)$ towards $p_{ theta}(z | x)$. The authors hypothesize that during the initial stages of training, the inference network or $q_{ phi}(z | x)$ falls behind the true posterior (which it is supposed to approximate) which has multiple forces acting on it within the context of training dynamics. And before $q_{ phi}(z | x)$ can catch up with $p_{ theta}(z | x)$, the model learns to ignore $z$ and falls into a local optimum suffering with posterior collapse. We&#39;ll be going over the details of the core ideas in this paper. . Breaking down Posterior Collapse . Until now we have only discussed about the approximate posterior, $q_{ phi}(z | x)$ and prior $p(z)$ in the context of posterior collapse. However, the true model posterior, $p_{ theta}(z | x)$ also plays a key role in it, which the authors try to analyze and understand through various experiments. Formally, posterior collapse occurs when, . $$p_{ theta}(z | x) = q_{ phi}(z | x) = p(z)$$ . We can further break posterior collapse into two partial collapse states, . Model collapse, when $p_{ theta}(z | x) = p(z)$ | Inference collapse, when $q_{ phi}(z | x) = p(z)$ | . To conduct experiments and analyze the dynamics of the lower bound optimization, He et al. use a synthetic dataset, which is sequential in nature. They use an LSTM encoder and decoder since posterior collapse is more severe for such powerful autoregressive models. In order to observe how the true and approximate posterior change throughout the training process, we will keep track of their means. We have already discussed how we use neural networks to model multivariate gaussians. We will denote the true model posterior mean by $ mu_{x, theta}$ and that of the inference network by $ mu_{x, phi}$. The latent variable $z$ used in the experiments is a scalar, which will make it easier to visualize how these means change during training. Following figure, taken directly from the paper puts the above discussion into context. . The figure highlights the following, . The x-axis denotes the mean $ mu_{x, theta}$ of true model posterior, $p_{ theta}(z | x)$. | The y-axis denotes the mean $ mu_{x, phi}$, of inference network $q_{ phi}(z | x)$ | If the co-ordinates ( $ mu_{x, theta}$, $ mu_{x, phi}$) lie on x-axis, it means that the model has suffered with inference collapse because on x-axis, $ mu_{x, phi}$ = 0, which is the mean of our prior, $p(z)$. | Similarly, if the co-ordinates ( $ mu_{x, theta}$, $ mu_{x, phi}$) lie on y-axis, it means that the model has suffered with model collapse because on x-axis, $ mu_{x, theta}$ = 0, which is the mean of our prior, $p(z)$. | At the origin, we have posterior collapse where both the means collapse into 0. | And finally, we have the line $x = y$. If the datapoints are distributed along this line, we would have a attained a more desirable local optmimum as compared to the trivial one at the origin. If the approximate posterior has a non-zero value, it implies that the model has captured some useful information in the latent variable which will be used by the decoder. NOTE It is not possible to calculate the true model posterior since it involves an intractable integral. The authors approximate the true model posterior by using a descretization method. | . Visualizing Posterior Collapse . In this section, we&#39;ll be discussing the training dynamics of an alternate lower bound equation and analyze why do models suffer with posterior collapse. We can derive this alternate lower bound by looking at another optimization problem. We know that the evaluating true model posterior involves an intractable integral. Hence we try to approximate it using another probability distribution, $q_{ phi}(z | x)$. Our aim is to drive this towards the true model posterior and essentially we need to minimize the difference between these two distributions. . $$ min_{ phi, theta} KL [q_{ phi}(z | x) || p_{ theta}(z | x)]$$ . $$KL [q_{ phi}(z | x) || p_{ theta}(z | x)] = int_{z} q_{ phi}(z | x) log frac{q_{ phi}(z | x)}{p_{ theta}(z | x)}$$ . $$KL [q_{ phi}(z | x) || p_{ theta}(z | x)] = int_{z} q_{ phi}(z | x) log frac{q_{ phi}(z | x) p_{ theta}(x)}{p_{ theta}(z , x)}$$ . $$ KL [q_{ phi}(z | x) || p_{ theta}(z | x)] = int_{z} q_{ phi}(z | x) log frac{q_{ phi}(z | x)}{p_{ theta}(z , x)} + int_{z} q_{ phi}(z | x) log p_{ theta}(x)$$ . $$KL [q_{ phi}(z | x) || p_{ theta}(z | x)] = -L( theta, phi) + log p_{ theta}(x)$$ . Rearranging the above equation, we get the final ELBO as, . $$ boxed{L( theta, phi) = log( p_{ theta}(x) ) - KL [q_{ phi}(z | x) || p_{ theta}(z | x)]}$$ . According this equation, during training, there is only one force acting on $q_{ phi}(z | x)$ which is the KL term trying to drive it towards $p_{ theta}(z | x)$. However, there are two forces affecting the true posterior: the KL term and the marginal data likelihood. The following figures show what happens during different stages of training. These are the projections of 500 data samples generated from a synthetic dataset. . During the initial stages of training, $x$ and $z$ are independent under both $p_{ theta}(z | x)$ and $q_{ phi}(z | x)$. As shown in the figure at iter = 0, both the means are at 0 and since training has not begun yet, we say that all the datapoints are suffering with model collapse and inference collapse. As we start training, at around 200 iterations, the datapoints start spreading around the x-axis. Since $x$ and $z$ are independent under $p_{ theta}(z | x)$, the only remaining force acting on it is by the marginal data likelihood term. Hence, we can claim that $log p_{ theta}(x)$ is able to move the datapoints away from model collapse. However, if the datapoints are moving away from model collapse, that is $ mu_{x, theta}$ is not zero anymore, it means that our true model posterior has changed via $log p_{ theta}(x)$. If the true model posterior changes, it implies that $p_{ theta}(z | x)$ will start diverging from $q_{ phi}(z | x)$ and therefore increase the KL term. $q_{ phi}(z | x)$ cannot keep track of this because $x$ and $z$ are still independent under it. Even under $p_{ theta}(z | x)$, the dependence is brought by $log p_{ theta}(x)$. So, even though the datapoints initially move away from model collapse, they are still in the state of inference collapse. As training progresses, datapoints are again brought close to 0 by an increasing KL term, which is evident at iter = 2000 in the figure above. The training process, thus makes an effort to align the true model posterior and the inference network by setting both to the prior $p(z)$. Before $q_{ phi}(z | x)$ can catch up with $p_{ theta}(z | x)$, the model has already learned to ignore the latent variable forcing the model into posterior collapse. . Relation with KL Cost Annealing . If we now take a step back and analyze KL cost annealing discussed earlier, we can clearly see why it actually works. In KL annealing, we do not penalize the KL term during the initial stages of training and start penalizing it gradually as training progresses. We can directly relate this to the ideas presented in the previous section. During the initial stages of training, we saw that the marginal data likelihood manages to move the datapoints away from model collapse. However, this is suppressed by an increasing KL term. The KL term rises because $p_{ theta}(z | x)$ diverges from $q_{ phi}(z | x)$ which is lagging behind the true model posterior. So even before $x$ and $z$ become dependent under $q_{ phi}(z | x)$, training forces the model into posterior collapse. We need to control the KL term in a way such that it waits for $q_{ phi}(z | x)$ to become relevant. This is exactly what KL annealing does. It essentially buys time for $q_{ phi}(z | x)$ to cover the lag and catch up with the true posterior by not penalizing the KL term initially. . Aggressive Training of the Inference Network . The solution proposed by He et al. is also intuitive. Since the inference network is lagging behind during the initial stages of training, we should optimize it before we optimize the true model posterior. Formally, this can be written as, . $$ theta^{*} = arg max_{ theta} L(X; theta, phi^{*}), where phi^{*} = arg max_{ phi} L(X; theta, phi)$$ . Essentially, it means that we&#39;re optimizing $q_{ phi}(z | x)$ in an inner loop in the entire training process. Also, the model need not operate in this mode throughout the training process. We need to do this only until $z$ and $x$ become dependent under $q_{ phi}(z | x)$ and it catches up with the true model posterior. Empirical results show that we typically need to train in aggessive mode for 5 epochs and then revert to normal training. . The figure above shows the behaviour posterior means when trained following the proposed aggressive training approach. The aggressive training successfully pulls the datapoints away from inference collapse and moves them towards the desired diagonal. . Closing Remarks . This blog covers some of the key ideas in this research area and I hope it gives the reader a solid foundation to build upon. I&#39;ll list down some other interesting ideas and papers in this topic that build upon the content presented in this blog. As discussed in the blog, most of the ideas to avoid posterior collapse can be broadly categorized under two categories: one that alters the training procedure and one that weakens/changes the decoder. . Improved Variational Autoencoders for Text Modeling using Dilated Convolutions . by Yang et al. replaces the recurrent decoder with a decoder which involves dilated convolutions. Dilated convolutions enable the network to increase the receptive field without incurring additional computations. In 2D convolution with different dilation $d$ can be visualized as follows . As can be see, for $d = 1$, the receptive field is $3X3$, for $d=2$, it is $7X7$ and for $d=4$, it is $15X15$. In all these convolutions the filter size effectively remains the same since we are skipping pixels in between. This idea translates to 1D and NLP applications in a similar fashion. . The authors hypothesize that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. By changing the dilation structure of the decoder, we can control the contextual information from previously generated words. In extreme cases, the convolution decoder can act as a recurrent decoder (with less or no dilation) or as a simple MLP/bag-of-words model (with a big dilation factor). As such, we have a greater control over the amount of information provided to decoder via inputs. This can be thought of as a knob which allows us to change the complexity of the decoder from an LSTM to an MLP. LSTM decoders are strong enough to model an autoregressive factorization without using latent information. Hence, the authors believe that finding a sweet spot by turning this knob could lead us to better results which can take the advantage of both the latent and contextual information. . Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing . by Fu et al. proposes a different KL annealing strategy. The authors argue against the de facto monotonic (Bowman et al.) KL annealing schedule and propose a cyclic schedule. Their hypothesis is based on analyzing the quality of latent variable $z$ learnt by the encoder during the initial stages of training. . References and Acknowledgements . I would like to thank Akash Srivastava, Swapneel Mehta and Fenil Doshi for introducing me to generative modeling and VAEs and clearing my doubts on numerous occasions. I would also like to thank Kumar Shridhar for clearing my doubts related to this research topic and suggesting edits to some sections of this blog. Below is the list of papers and other resources that I referred to while learning about this research area and writing this blog. Most of the figures in this blog have been borrowed from the concerned paper itself or some other blog, all of which have been mentioned in the references below and some were created by me using https://www.diagrams.net/. If you find any mistakes or errors, kindly point it out in the comments below or reach out to me at kushalj001@gmail.com. Thank you! . https://arxiv.org/abs/1511.06349 | https://arxiv.org/abs/1901.05534 | https://arxiv.org/abs/1606.05908 | https://arxiv.org/pdf/1702.08139.pdf | https://beta.vu.nl/nl/Images/werkstuk-fischer_tcm235-927160.pdf | https://gokererdogan.github.io/2017/08/15/variational-autoencoder-explained/ | https://datascience.stackexchange.com/questions/48962/what-is-posterior-collapse-phenomenon | https://en.wikipedia.org/wiki/Language_model | https://www.aclweb.org/anthology/N19-1021.pdf | https://arxiv.org/pdf/1702.08139.pdf | https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215 | https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/ | https://www.youtube.com/watch?v=Z5knlb6MMOI&amp;list=PL8PYTP1V4I8AkaHEJ7lOOrlex-pcxS-XV&amp;index=20&amp;ab_channel=GrahamNeubig | http://phontron.com/class/nn4nlp2021/assets/slides/nn4nlp-20-latent.pdf | https://github.com/timbmg/Sentence-VAE | https://towardsdatascience.com/regularization-an-important-concept-in-machine-learning-5891628907ea#:~:text=Regularization%20is%20a%20technique%20used,don&#39;t%20take%20extreme%20values. |",
            "url": "https://kushalj001.github.io/black-box-ml/language%20modeling/text-generation/vaes/posterior%20collapse/nlp/elbo/2021/05/05/Generating-Text-with-VAEs-and-Posterior-Collapse.html",
            "relUrl": "/language%20modeling/text-generation/vaes/posterior%20collapse/nlp/elbo/2021/05/05/Generating-Text-with-VAEs-and-Posterior-Collapse.html",
            "date": " • May 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Generalizing Attention in NLP and Understanding Self-Attention",
            "content": "Introduction . Attention is one of the most important and ubiquitous concepts in NLP and deep learning. There are many blog posts out there explaining different flavors of attention. This blog post however, introduces attention as a general concept in NLP and then goes onto explain some of the most important methods of calculating attention vectors. Each sub-topic or concept is concretely supported by real-world intuitions. Lastly, this post explains the idea of multiheaded self-attention in a unique by using the idea of linear projections and concludes by showing stepwise how to translate all the intuitions to code. . This blog post is a small excerpt from my work on paper-annotations for the task of question answering. This repo contains a collection of important question-answering papers, implemented from scratch in pytorch with detailed explanation of various concepts/components introduced in the respective papers. . Introduction to Attention in NLP . Attention as a concept in NLP was introduced in 2015 by Bahdanau et al. to improve neural machine translation systems. Before this paper, NMT systems were largely based on seq2seq architectures which had an encoder to encode a representation of the source language and a decoder which to decode this representation into the target language. Such models were trained on large quantities of parallel text data of two languages. One major drawback of this architecture was that it didn&#39;t work well for longer documents/sequences. This is because the entire information in the source sentence was being crammed into a single vector. If this vector fails to capture the important information from the source language, the system is going to perform poorly. . When we claim that these neural nets mimic the human brain, this is certainly not how the human brain works. While learning about some topic, we do not simply read 2-3 pages of content and expect our brain to remember all the details in the first go. We usually revisit various concepts, recollect and refer to the material back and forth before mastering it. The attention mechanism in NMT was designed to do this. While decoding at any particular time step, encoder hidden states from all the time-steps are made available to the decoder. The decoder then can look back at the encoder hidden states or the source language and make a more informed prediction at a particular time-step. This alieviates the problem of all the information from source language being crammed into a single vector. To illustrate this with equations, consider that the hidden states of the encoder RNN are represented by $H$ = {$h_{1}, h_{2}, h_{3},...,h_{t}$}. While decoding the token at position $t$, the input to the decoder unit is hidden state from previous unit $s_{t-1}$ and an attention vector which is a selective summary of the encoder hidden states and helps the decoder to pay more attention to a particular encoder state. The similarity between the encoder hidden states $H$ and the decoder hidden state so far $s_{t-1}$ is computed by, $$ alpha = tanh (W [H ; s_{t-1}]) $$ . $ alpha$ is then passed through a softmax layer to obtain attention distribution such that $ sum_{t} alpha_{t}$ = 1. The final step is calculating the attention vector by taking a weighted sum of the encoder hidden states, $$ sum_{t} alpha_{t} h_{t} $$ . The following diagram illustrates this process. . . Since then, many different forms of attention have been proposed and used in the literature. Attention is not limited to NMT systems and has evolved into a more general concept in NLP. At the heart of it attention is about summarizing a particular entity/representation by attending to the important parts of this representation. A more general definition of attention is as follows: . Given a set of vectors values, and a single vector query, attention is a method to calculate a weighted sum of the values, where the query determines which values to focus on. . It is a way to obtain a fixed size representation of an arbitrary set of representations (values), dependent on some other representation (query). . In our earlier NMT example, the encoder hidden states {$h_{1}, h_{2}, h_{3},...,h_{t}$} are the values and the decoder hidden state $s_{t-1}$ is the query. . Generalizing Attention . In general there are 3 steps when calculating the attention. Consider that values are represented by {$h_{1}, h_{2}, h_{3},..h_{n}$} and query is $s$. Then attention always involves, . Calculating the energy $e$ or attention scores between these 2 vectors, $e$ $ epsilon$ $ R^{N} $ | Taking softmax to get an attention distribution $ alpha$, $ alpha$ $ epsilon$ $R^{N}$ | $$ alpha = softmax(e)$$ $$ sum_{t}^{N} alpha_{t} = 1 $$ . Taking the weighted sum of the values by using $ alpha$ $$ a = sum_{t}^{N} alpha_{t}h_{t} $$ | Now there are different ways to calculate the energy between query and values. . Basic Dot Product Attention . $$ e_{t} = s^{T}h_{t}$$ . Additive Attention . $$ e_{t} = v^{T} tanh (W [h_{t};s])$$ This is nothing but the Bahdanau attention attention first proposed for NMT systems. . Bilinear Attention . $$ e_{t} = s^{T} W h_{t}$$ where $W$ is a trainable weight vector. This has been used extensively in Question Answering systems like the Stanford Attentive Reader and DrQA . Scaled Dot Product Attention . $$ e_{t} = s^{T}h_{t}/ sqrt n$$ where $n$ is the model size. A modified version of this proposed in the Transformers paper by Vaswani et al. is now employed in almost every NLP system. The following section explains this attention in more detail. . Multiheaded Self Attention . Idea of Linear Projections . Consider a system of an online book store like kindle, which lets you rent, buy and read books on its platform. Such platforms usually have a recommendation system (recsys) in place that enables them to understand their users&#39; taste and preferences over time. This helps them in making personalized recommendations to users and in turn improve their revenue. For simplicity, let&#39;s assume that there are 10,000 books available on the platform and the system maintain a simple binary vector of size 10,000 for each user. If a user has read a particular book, the position in the vector corresponding to the book&#39;s id is 1 and 0 otherwise. A books-read vector for a user looks like, $$ [1,0,0,1,1,0,0,0,0,1,1,...,1] $$ . Now assume a projection matrix of dimension 10,000 X 100. When we multiply any user&#39;s books vector, we get a new low dimensional vector of size 100. This vector is totally different from the previous one and now represents the user&#39;s taste or preferences in books. It basically represents a user-profile for the recommendation system. Calculating this user-taster vector for different users enables the application to find users with similar taste and recommend books that they might like simply based on what the other &quot;similar&quot; user has read. The weights or values of this projection matrix can be thought as representing certain features or properties that a book might possess. It might capture various genres like science, philsophy, fantasy novels, etc. The question that still remains however, is how do we get such a projection matrix in the first place that can transform a represenation from one vector space to another that is somehow related to the original vector but has an entirely different interpretation. This is exactly what deep learning is about. Neural networks work as this universal function approximators that helps in learning such transformations. The weights of such projection matrices are learned via backpropagation. We also need a lot of training data to achieve this. . Self Attention . Much of what will follow is heavily derived from Jay Alammar&#39;s famous blog post: The Illustrated Transformer. The intuition and visualizations can be directly converted into code and that&#39;s my main motive here. To understand the details, we&#39;ll first look at self attention using vectors at a granular level. We&#39;ll then show how actually these computations are made using matrices which directly correspond to the code. For convenience, we&#39;ll explain how self attention works in the transformer model. The input to the self attention layer is an embedding vector. The central idea of attention is the same as discussed in the first notebook. Even here we&#39;ll calculate the measure of similarity between two representations, convert them into an attention distribution and take a weighted sum with the values. However, there are certain details involved that need to be addressed. Following steps involved in calculating self-attention. . The first step is to project the input into 3 different vector spaces: key space, query space and value space. These projections give us a key vector, a query vector and a value vector. The weights of these projection matrices are learnt via backpropagation during training. The projection matrices for key, query and value are $W^{K}$, $W^{Q}$, $W^{V}$ respectively. These projections are exactly what we discussed above. Their values depend a lot on the training procedure and the training data. . | The next step is to calculate attention scores. This is basically the part where we determine how similar are two input vectors and hence how much attention/focus needs to be paid on one vector while summarizing the other. . The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position. . There are different ways to determine this. In this paper, a dot product between the query and the key is used. Consider the phrase &quot;Thinking Machines&quot;. For the word &quot;Thinking&quot;, we need to calculate a score with each word in the sentence including &quot;Thinking&quot; itself. Therefore, score for first position would be, $$ q_{1} . k_{1} $$ The result of this product represents the amount of attention we need to pay to &quot;Thinking&quot; itself while encoding &quot;Thinking&quot;. The score for next position would be, $$ q_{1} . k_{2} $$ which captures the importance of &quot;Machines&quot; while encoding &quot;Thinking&quot;. . | We then divide the scores calculated in the previous step by $ sqrt d_{k}$, where $d_{k}$ is the dimension of key vectors. This scaling was done to ensure that the gradients are stable during training. Next, these scores are passed through a softmax function to get an attention distribution. This means that for a sentence of length $n$, if $ alpha_{t}$ represents the score at $t$-th position, then $$ sum_{t=1}^{n} alpha_{t} = 1$$ | The last step is to multiply the softmax output with the value vector at respective position and sum these products up. In effect this computes a weighted sum. For a sentence of length n, $$ sum_{t=1}^{n} alpha_{t} v_{t}$$ | All the steps explained above can be summarized as, . Multiheaded attention and Implementation . The above steps are usually performed using matrices instead of vectors. This is also where we&#39;ll see how and why multihead attention is implemented. . The first step is to calculate the query, key and value matrices by projecting them using trainable weights. In code, these weights correspond to linear layers. $W^{Q}$ corresponds to fc_q, $W^{K}$ to fc_k and $W^{V}$ to fc_v. Projecting these gives us $Q$, $K$ and $V$ as seen in code too. | Similar representations for value and key are also calculated. The dimensions of the above matrices will be explained below. . Calculation of scores can be easily visualized as follows, In code this is achieved by calculating the energy of $K$ and $Q$ using torch.matmul. | The final step is to scale, take softmax of the scores and multiply the matrix by the value matrix. scale is calculated by taking the square root of head_dim. After scaling the energy tensor or the scores at different positions, we apply softmax to this tensor and multiply it with $V$ using torch.matmul once again. | In the original transformer model, the input embedding size is 512. Before projecting these embeddings, we split them into 8 parts which brings us to multihead attention. This paper uses 8 attention heads. Multiheaded attention expands the model&#39;s ability to focus on different positions. . It gives the attention layer mutiple &quot;representation subspaces.&quot; . These subspaces are nothing but different projection matrices. Instead of having just one projection matrix $W^{Q}$ for query, we&#39;ll have 8 projection matrices for query, key and value. Weights for each of these &quot;subspaces&quot; are learnt via backpropagation during training. An analogy for this can be the use of multiple convolutional filters to learn unique features from the image. Therefore, now the dimension of key, query and value matrices would be 64 (512/8). In code, splitting weight matrices for multiple attention heads is done right after getting $K$, $Q$ and $V$. This is done by first calculating the head_dimension and then splitting the tensors using the view function. . . The above image shows projection matrices for 2 attention heads. There are 8 such heads. This would give us 8 $Z$ matrices in the end. The output dimension of the self attention layer should be same as the input dimension. Hence, we need to recombine the results of all the attention heads before passing the output to the next layer. To combine them, in code, we simply use view to drop the head dimension and further make a projection using fc_o to ensure that the input dimension is same as the output dimension. . from torch import nn class MultiheadAttentionLayer(nn.Module): def __init__(self, hid_dim, num_heads, device): super().__init__() self.num_heads = num_heads self.device = device self.hid_dim = hid_dim self.head_dim = self.hid_dim // self.num_heads self.fc_q = nn.Linear(hid_dim, hid_dim) self.fc_k = nn.Linear(hid_dim, hid_dim) self.fc_v = nn.Linear(hid_dim, hid_dim) self.fc_o = nn.Linear(hid_dim, hid_dim) self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device) def forward(self, x, mask): # x = [bs, len_x, hid_dim] # mask = [bs, len_x] batch_size = x.shape[0] Q = self.fc_q(x) K = self.fc_k(x) V = self.fc_v(x) # Q = K = V = [bs, len_x, hid_dim] Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3) K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3) V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0,2,1,3) # [bs, len_x, num_heads, head_dim ] =&gt; [bs, num_heads, len_x, head_dim] K = K.permute(0,1,3,2) # [bs, num_heads, head_dim, len_x] energy = torch.matmul(Q, K) / self.scale # (bs, num_heads){[len_x, head_dim] * [head_dim, len_x]} =&gt; [bs, num_heads, len_x, len_x] mask = mask.unsqueeze(1).unsqueeze(2) # [bs, 1, 1, len_x] #print(&quot;Mask: &quot;, mask) #print(&quot;Energy: &quot;, energy) energy = energy.masked_fill(mask == 1, -1e10) #print(&quot;energy after masking: &quot;, energy) alpha = torch.softmax(energy, dim=-1) # [bs, num_heads, len_x, len_x] #print(&quot;energy after smax: &quot;, alpha) alpha = F.dropout(alpha, p=0.1) a = torch.matmul(alpha, V) # [bs, num_heads, len_x, head_dim] a = a.permute(0,2,1,3) # [bs, len_x, num_heads, hid_dim] a = a.contiguous().view(batch_size, -1, self.hid_dim) # [bs, len_x, hid_dim] a = self.fc_o(a) # [bs, len_x, hid_dim] #print(&quot;Multihead output: &quot;, a.shape) return a . References . Attention is All You Need https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf | https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html | The Illustrated Transformer:http://jalammar.github.io/illustrated-transformer/. This is an excellent piece of writing with amazing easy-to-understand visualizations. Must read. | https://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources/. Chris McCormick&#39;s BERT research series is another great resource to learn about self attention and various other details about BERT. He has a blog as well as youtube video series on the same. | https://nlp.seas.harvard.edu/2018/04/03/attention.html. The annotated Transformer. | https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture10.pdf. | https://github.com/bentrevett/pytorch-seq2seq. A great series of notebooks on Machine Translation using PyTorch which includes all the the methods discussed above in greater detail and context. | .",
            "url": "https://kushalj001.github.io/black-box-ml/attention/bahdanau/self%20attention/bahdanau%20attention/multihead%20attention/pytorch-implemention/2020/07/06/Generalizing-Attention-in-NLP-and-Understanding-Self-Attention.html",
            "relUrl": "/attention/bahdanau/self%20attention/bahdanau%20attention/multihead%20attention/pytorch-implemention/2020/07/06/Generalizing-Attention-in-NLP-and-Understanding-Self-Attention.html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Character Embeddings and Highway Layers in NLP",
            "content": "Introduction . Character embeddings and Highway Layers are the trademark components of many NLP systems. They have been used extensively in literature to reduce the parameters in models, deal with Out of Vocabulary or OOV words and help in faster training of neural networks. This blog post introduces these 2 topics, explains the intuitions with illustrations and then translates everything into code. . This blog post is a small excerpt from my work on paper-annotations for the task of question answering. This repo contains a collection of important question-answering papers, implemented from scratch in pytorch with detailed explanation of various concepts/components introduced in the respective papers. . Character Embedding . It maps each word to a vector space using character-level CNNs. . Using CNNs in NLP was first proposed by Yoon Kim in his paper titled Convolutional Neural Networks for Sentence Classification. This paper tries to use CNNs in NLP as they are used in vision. Most of the state-of-the-art results in CV at that time were achieved by transfer learning from larger models pretrained on ImageNet. In this paper, they train a simple CNN with one layer of convolution on top of pretrained word vectors and hypothesized that these pretrained word vectors could work as a universal feature extractors for various classification tasks. This is analogous to the earlier layers of vision models like VGG and Inception working as generic feature extractors. The idea of character embeddings was also used in the paper titled Character-Aware Neural Language Models by the same author. The intuition is simple over here. Just as convolutional filters learn various features in an image by operating on its pixels, here they&#39;ll do so by operating on characters of words. Let&#39;s get into the working of this layer. . We first pass each word through an embedding layer to get a fixed size vector. Let the embedding dimension be $d$. Let $C$ represent a matrix representation of word of length $l$. Therefore $C$ is a matrix with dimensions $d$ x $l$. . Let $H$ represent a convolutional filter with dimensions $d$ x $w$, where $d$ is the embedding dimension and $w$ is the width or the window size of the filter. The weights of this filter are randomly initialized and learnt parallelly via backpropogation. We convolve this filter $H$ over our word representation $C$ as shown below. . The convolution operation is simply the inner product of the filter $H$ and matrix $C$. The convolution operations can be visualized as follows: . The result of the above operation is a feature vector. A single filter is usually associated with a unique feature that it captures from the image/matrix. To get the most representative value related to the feature, we perform max pooling over the dimension of this vector. . The above process was described for a single filter. This same process is repeated with $N$ number of filters. Each of these filters captures a different property of word. In an image, for example, if one filter captures the edges, another filter will capture the texture and another one the shapes in the image and so on. $N$ is also the size of the desired character embedding. In this paper authors have trained the model with $N$ = 100. . Implementation . The implementation of this layer is fairly straightforward. The input to this layer is of dimension [batch_size, seq_len, word_len] where seq_len and word_len are the lengths of largest sequence and word respectively within a given batch . We first embed the character tokens into a fixed size vector using an embedding layer. This gives a vector of dimension [batch_size, seq_len, word_len, emb_dim]. We then convert this tensor into a format that closely resembles an image, of type [ $N$, $C_{in}$, $H_{in}$, $W_{in}$]. The number of input channels, $C_{in}$ would be 1 and the output channels would be the desired embedding size which is 100. This is then passed through the convolution layer which gives an output of shape [ $N$, $C_{out}$, $H_{out}$, $W_{out}$]. Here, . . If padding = [0,0], kernel_size (or filter-size) = [$H_{in}$, $w$], dilation = [1,1], stride = [1,1]. as visible in images above, then, $H_{out}$ = 1, and $W_{out}$ = $W_{in}$ - $w$ - 1. . Since $H_{out}$ = 1, we squeeze that dimension and perform max pooling with a kernel-size = $L_{in}$. The value of $L_{in}$ = $W_{in}$ - $w$ - 1. . . If the kernel size = $L_{in}$, we get $L_{out}$ = 1 if other values are default. This dimension is again squeezed to finally give us a tensor of dimension [batch_size, seq_len, output_channels (or 100)]. . class CharacterEmbeddingLayer(nn.Module): def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size): super().__init__() self.char_emb_dim = char_emb_dim self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1) self.char_convolution = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=kernel_size) self.relu = nn.ReLU() self.dropout = nn.Dropout(0.2) def forward(self, x): # x = [bs, seq_len, word_len] # returns : [batch_size, seq_len, num_output_channels] # the output can be thought of as another feature embedding of dim 100. batch_size = x.shape[0] x = self.dropout(self.char_embedding(x)) # x = [bs, seq_len, word_len, char_emb_dim] # following three operations manipulate x in such a way that # it closely resembles an image. this format is important before # we perform convolution on the character embeddings. x = x.permute(0,1,3,2) # x = [bs, seq_len, char_emb_dim, word_len] x = x.view(-1, self.char_emb_dim, x.shape[3]) # x = [bs*seq_len, char_emb_dim, word_len] x = x.unsqueeze(1) # x = [bs*seq_len, 1, char_emb_dim, word_len] # x is now in a format that can be accepted by a conv layer. # think of the tensor above in terms of an image of dimension # (N, C_in, H_in, W_in). x = self.relu(self.char_convolution(x)) # x = [bs*seq_len, out_channels, H_out, W_out] x = x.squeeze() # x = [bs*seq_len, out_channels, W_out] x = F.max_pool1d(x, x.shape[2]).squeeze() # x = [bs*seq_len, out_channels, 1] =&gt; [bs*seq_len, out_channels] x = x.view(batch_size, -1, x.shape[-1]) # x = [bs, seq_len, out_channels] # x = [bs, seq_len, features] = [bs, seq_len, 100] return x . Highway Networks . Highway networks were originally introduced to ease the training of deep neural networks. While researchers had cracked the code for optimizing shallow neural networks, training deep networks was still a challenging task owing to problems such as vanishing gradients etc. Quoting the paper, . We present a novel architecture that enables the optimization of networks with virtually arbitrary depth. This is accomplished through the use of a learned gating mechanism for regulating information ﬂow which is inspired by Long Short Term Memory recurrent neural networks. Due to this gating mechanism, a neural network can have paths along which information can ﬂow across several layers without attenuation. We call such paths information highways, and such networks highway networks. . This paper takes the key idea of learned gating mechanism from LSTMs which process information internally through a sequence of learned gates. The purpose of this layer is to learn to pass relevant information from the input. A highway network is a series of feed-forward or linear layers with a gating mechanism. The gating is implemented by using a sigmoid function which decides what amount of information should be transformed and what should be passed as it is. . A plain feed-forward layer is associated with a linear transform $H$ parameterized by ($W_{H}, b_{H}$), such that for input $x$, the output $y$ is . $$ y = g(W_{H}.x + b_{H})$$ where $g$ is a non-linear activation. For highway networks, two additional linear transforms are defined viz. $T$ ($W_{T},b_{T}$) and $C$ ($W_{C}$,$b_{C}$). Then, . $$ y = T(x) . H(x) + x . C(x) $$ . We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by transforming the input and carrying it, respectively. For simplicity, in this paper we set C = 1 − T. . $$ y = T(x) . H(x) + x . (1 - T(x)) $$ . $$ y = T(x) . g(W_{H}.x + b_{H}) + x . (1 - T(x)) $$ where $T(x)$ = $ sigma$ ($W_{T}$ . $x$ + $b_{T}$) and $g$ is relu activation. . The input to this layer is the concatenation of word and character embeddings of each word. To implement this we use nn.ModuleList to add multiple linear layers. This is done for the gate layer as well as for a normal linear transform. In code the flow_layer is the same as linear transform $H$ discussed above and gate_layer is $T$. In the forward method we loop through each layer and compute the output according to the highway equation described above. . The output of this layer for context is $X$ $ epsilon$ $R^{ d X T}$ and for query is $Q$ $ epsilon$ $R^{ d X J}$, where $d$ is hidden size of the LSTM, $T$ is the context length, $J$ is the query length. . Importance in NLP systems . The structure discussed so far is a recurring pattern in many NLP systems. Although this might be out of favor now with the advent of transformers and large pretrained language models, you will find this pattern in many NLP systems before transformers came into being. The idea behind this is that adding highway layers enables the network to make more efficient use of character embeddings. If a particular word is not found in the pretrained word vector vocabulary (OOV word), it will most likely be initialized with a zero vector. It then makes much more sense to look at the character embedding of that word rather than the word embedding. The soft gating mechanism in highway layers helps the model to achieve this. . class HighwayNetwork(nn.Module): def __init__(self, input_dim, num_layers=2): super().__init__() self.num_layers = num_layers self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)]) self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)]) def forward(self, x): for i in range(self.num_layers): flow_value = F.relu(self.flow_layer[i](x)) gate_value = torch.sigmoid(self.gate_layer[i](x)) x = gate_value * flow_value + (1-gate_value) * x return x . References . Character-Aware Neural Language Models: https://arxiv.org/abs/1508.06615 | Convolutional Neural Networks for Sentence Classification: https://arxiv.org/abs/1408.5882 | Highway Networks: https://arxiv.org/abs/1505.00387 | https://nlp.seas.harvard.edu/slides/aaai16.pdf. A great resource for character embeddings. The figures in the character embedding section are taken from here. | .",
            "url": "https://kushalj001.github.io/black-box-ml/cnns/highway%20layers/nlp/cnns%20for%20nlp/2020/04/22/Character-Embeddings-and-Highway-Layers-in-NLP.html",
            "relUrl": "/cnns/highway%20layers/nlp/cnns%20for%20nlp/2020/04/22/Character-Embeddings-and-Highway-Layers-in-NLP.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Understanding Depth-wise Separable Convolutions",
            "content": "This blog post is a small excerpt from my work on paper-annotations for the task of question answering. This repo contains a collection of important question-answering papers, implemented from scratch in pytorch with detailed explanation of various concepts/components introduced in the respective papers. The illustrations in this blog post have been created by me using https://www.diagrams.net/. You can find the other references below. . Depthwise Separable Convolutions . Depthwise separable convolutions serve the same purpose as normal convolutions with the only difference being that they are faster because they reduce the number of multiplication operations. This is done by breaking the convolution operation into two parts: depthwise convolution and pointwise convolution. . Depthwise separable convolutions are used rather than traditional ones, as we observe that it is memory efﬁcient and has better generalization. . Let&#39;s understand why depthwise convolutions are faster than traditional convolution. Traditional convolution can be visualized as, . . Let&#39;s count the number of multiplications in a traditional convolution operation. The number of multiplications for a single convolution operation is the number of elements inside the kernel. This is $D_{K}$ X $D_{K}$ X $M$ = $D_{K}^{2}$ X $M$. To get the output feature map, we slide or convolve this kernel over the input. Given the output dimensions, we perform $D_{O}$ covolutions along the width and the height of the input image. Therefore, the number of multiplications per kernel are $D_{O}^{2}$ X $D_{K}^{2}$ X $M$. These calculations are for a single kernel. In convolutional neural networks, we usually use multiple kernels. Each kernel is expected to extract a unique feature from the input. If we use $N$ such filters, then number of multiplications become $N$ X $D_{O}^{2}$ X $D_{K}^{2}$ X $M$. . Depthwise convolution . . In depthwise convolution we perform convolution using kernels of dimension $D_{K}$ X $D_{K}$ X 1. Therefore the number of multiplications in a single convolution operation would be $D_{K}^{2}$ X $1$. If the output dimension is $D_{O}$, then the number of multiplications per kernel are $D_{K}^{2}$ X $D_{O}^{2}$. If there are $M$ input channels, we need to use $M$ such kernels, one kernel for each input channel to get the all the features. For $M$ kernels, we then get $D_{K}^{2}$ X $D_{O}^{2}$ X $M$ multiplications. . Pointwise convolution . . This part takes the output from depthwise convolution and performs convolution operation with a kernel of size 1 X 1 X $N$, where $N$ is the desired number of output features/channels. Here similarly, Multiplications per 1 convolution operation = 1 X 1 X $M$ Multiplications per kernel = $D_{O}^{2}$ X $M$ For N output features = $N$ X $D_{O}^{2}$ X $M$ . Adding up the number of multiplications from both the phases, we get, . $$ = N . D_{O}^{2} . M + D_{K}^{2} . D_{O}^{2} . M $$ $$ = D_{O}^{2} . M (N + D_{K}^{2}) $$ . Comparing this with traditional convolutions, . $$ = frac {D_{O}^{2} . M (N + D_{K}^{2})} {D_{O}^{2} . M . D_{K}^{2} . N}$$ . $$ = frac{1}{D_{K}^{2}} + frac{1}{N} $$ . This clearly shows that the number of computations in depthwise separable convolutions are lesser than traditional ones. In code, the depthwise phase of the convolution is done by assigning groups as in_channels. According to the documentation, . At groups= in_channels, each `nput channel is convolved with its own set of filters, of size:$ left lfloor frac{out _channels}{in _channels} right rfloor$ . Implementation . Following is an implementation for the layer discussed above. This is a standalone implementation of the layer and can be plugged into any application/larger model where it is used as a component. . import torch from torch import nn class DepthwiseSeparableConvolution(nn.Module): def __init__(self, in_channels, out_channels, kernel_size): super().__init__() self.depthwise_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, groups=in_channels, padding=kernel_size//2) self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0) def forward(self, x): # Interpretations # x = [bs, seq_len, emb_dim] for NLP applications # x = [C_in, H_in, W_in] for CV applications x = self.pointwise_conv(self.depthwise_conv(x)) return x . References . The QANet paper: https://arxiv.org/abs/1804.09541 | Convolutional Neural Networks for Sentence Classification: https://arxiv.org/abs/1408.5882 | https://www.youtube.com/watch?v=T7o3xvJLuHk. Easy explanation of depthwise separable convolutions. | https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728. Another amazing blog for depthwise separable convolutions. | .",
            "url": "https://kushalj001.github.io/black-box-ml/convolutions/depthwise%20separable%20convolutions/pytorch-implementation/pytorch/2020/03/20/Understanding-Depth-wise-Separable-Convolutions.html",
            "relUrl": "/convolutions/depthwise%20separable%20convolutions/pytorch-implementation/pytorch/2020/03/20/Understanding-Depth-wise-Separable-Convolutions.html",
            "date": " • Mar 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Building Sequential Models in PyTorch",
            "content": "Introduction . The aim of this post is to enable beginners to get started with building sequential models in PyTorch. PyTorch is one of the most widely used deep learning libraries and is an extremely popular choice among researchers due to the amount of control it provides to its users and its pythonic layout. I am writing this primarily as a resource that I can refer to in future. This post will help in brushing up all the basics of PyTorch and also provide a detailed explanation of how to use some important torch.nn modules. We will be implementing a common NLP task - sentiment analysis using PyTorch and torchText. We will be building an LSTM network for the task by using the IMDB dataset. Let&#39;s get started! . A Tensor based approach . Another motivation to write this post is to introduce neural nets from tensors&#39; persepective. Neural nets are, ultimately, a series of matrix/tensor multiplications. Each layer in a neural net expects an input in a specified format and gives the output in a specified format. These input and output formats are tensors of different shapes. The content of these tensors are well defined by the documentation of the concerned library. As such, while implementing neural nets, it becomes very important to understand the input and output shapes of tensors for all layers. The following tutorial is written based on this approach. . A short Introduction to NLP pipeline . I do not intend to go into details about all the preprocessing steps that are required for NLP tasks but for the sake of completeness, I&#39;ll summarize some important conceptual points. . Textual data usually requires some amount of cleaning before they can be fed to neural nets. Important cleaning steps include removal of HTML tags, punctuation, stopwords, numbers etc. Stopwords are high frequency words in a dataset that do not convey significant information to the network (e.g. and, of, the, a). | Lemmatization can be performed to improve results of the network. Lemmatization converts words to their root form or to their lemma which can be found in a dictionary. For example &quot;goes&quot; $-&gt;$ &quot;go&quot;. | Neural nets do not understand natural language. In fact the only thing they understand and can process are numbers. So for any NLP task, we need to convert out text data into a numerical format (numericalization). Each word in the dataset is assigned a numerical value. These words need to be represented as a vector. | There are two options to represent a word or a token as a feature vector. One-hot encoding: Here, the size of the feature vector equals that of the vocabulary of dataset. All the values are 0 except for the index that equals the numerical value of the word. | Embeddings: These can either be pre-trained(GloVe, word2vec) or be trained in parallel with the main task. The dimensions of such vectors are usually around 100-300. These transform the features of the word into a dense vector. | | . TorchText basics . Some key points about the structure of the library will serve as a good introduction and also help in following along the rest of the tutorial. . The most important class of torchtext is the Field class. The structure of datasets for different NLP tasks is different. For example, in a classification task we have text reviews that are sequential in nature and a sentiment label corresponding to each review which is binary in nature (+ or -). In machine translation or summarization, both the input and output are sequential. The Field class handles all such types of datasets. Therefore, we initialize Field objects for each type of data format present in our dataset. . In sentiment analysis we need two Field instances - one for text review and other for labels. For labels we use LabelField which inherits from Field. Following are some important parameters you might need while initializing a Field class. . tokenize : function used to tokenize the text. This can either be a custom function or passing &#39;spacy&#39; uses the SpaCy tokenizer. | init_token : prepends this token in the beginning of each example. e.g. &lt; sos &gt;. | eos_token : appends this token in the end of each example. e.g. &lt; eos &gt;. | fix_length : fixed, predefined length to which all the examples in field will be padded. | batch_first: gives data in tensors that have batch dimension as the first dimension. | . Some important methods: . pad() : This method pads the examples to fix_length if provided as a parameter. If not, it calculates the length of the largest example in a batch and pads the sequences to that length. | build_vocab(): The Field class holds an instance of Vocab class. This class is responsible for creating a vocabulary from the field data and creating mappings stoi (string to int) and itos (int to string) for each word. | . To summarize, the Field class numericalizes the text data and provides it in form of tensors so that they can be used easily with neural nets. . import torchtext from torchtext import data, datasets from torch import nn import torch import torch.optim as optim . # creating field objects for text and labels review = data.Field(tokenize=&#39;spacy&#39;, batch_first=True) sentiment = data.LabelField(dtype=torch.float, batch_first=True) # loading the IMDB dataset train_data, test_data = datasets.IMDB.splits(text_field=review, label_field=sentiment) . print(vars(train_data.examples[4])) . {&#39;text&#39;: [&#39;This&#39;, &#39;is&#39;, &#39;not&#39;, &#39;the&#39;, &#39;typical&#39;, &#39;Mel&#39;, &#39;Brooks&#39;, &#39;film&#39;, &#39;.&#39;, &#39;It&#39;, &#39;was&#39;, &#39;much&#39;, &#39;less&#39;, &#39;slapstick&#39;, &#39;than&#39;, &#39;most&#39;, &#39;of&#39;, &#39;his&#39;, &#39;movies&#39;, &#39;and&#39;, &#39;actually&#39;, &#39;had&#39;, &#39;a&#39;, &#39;plot&#39;, &#39;that&#39;, &#39;was&#39;, &#39;followable&#39;, &#39;.&#39;, &#39;Leslie&#39;, &#39;Ann&#39;, &#39;Warren&#39;, &#39;made&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;,&#39;, &#39;she&#39;, &#39;is&#39;, &#39;such&#39;, &#39;a&#39;, &#39;fantastic&#39;, &#39;,&#39;, &#39;under&#39;, &#39;-&#39;, &#39;rated&#39;, &#39;actress&#39;, &#39;.&#39;, &#39;There&#39;, &#39;were&#39;, &#39;some&#39;, &#39;moments&#39;, &#39;that&#39;, &#39;could&#39;, &#39;have&#39;, &#39;been&#39;, &#39;fleshed&#39;, &#39;out&#39;, &#39;a&#39;, &#39;bit&#39;, &#39;more&#39;, &#39;,&#39;, &#39;and&#39;, &#39;some&#39;, &#39;scenes&#39;, &#39;that&#39;, &#39;could&#39;, &#39;probably&#39;, &#39;have&#39;, &#39;been&#39;, &#39;cut&#39;, &#39;to&#39;, &#39;make&#39;, &#39;the&#39;, &#39;room&#39;, &#39;to&#39;, &#39;do&#39;, &#39;so&#39;, &#39;,&#39;, &#39;but&#39;, &#39;all&#39;, &#39;in&#39;, &#39;all&#39;, &#39;,&#39;, &#39;this&#39;, &#39;is&#39;, &#39;worth&#39;, &#39;the&#39;, &#39;price&#39;, &#39;to&#39;, &#39;rent&#39;, &#39;and&#39;, &#39;see&#39;, &#39;it&#39;, &#39;.&#39;, &#39;The&#39;, &#39;acting&#39;, &#39;was&#39;, &#39;good&#39;, &#39;overall&#39;, &#39;,&#39;, &#39;Brooks&#39;, &#39;himself&#39;, &#39;did&#39;, &#39;a&#39;, &#39;good&#39;, &#39;job&#39;, &#39;without&#39;, &#39;his&#39;, &#39;characteristic&#39;, &#39;speaking&#39;, &#39;to&#39;, &#39;directly&#39;, &#39;to&#39;, &#39;the&#39;, &#39;audience&#39;, &#39;.&#39;, &#39;Again&#39;, &#39;,&#39;, &#39;Warren&#39;, &#39;was&#39;, &#39;the&#39;, &#39;best&#39;, &#39;actor&#39;, &#39;in&#39;, &#39;the&#39;, &#39;movie&#39;, &#39;,&#39;, &#39;but&#39;, &#39;&#34;&#39;, &#39;Fume&#39;, &#39;&#34;&#39;, &#39;and&#39;, &#39;&#34;&#39;, &#39;Sailor&#39;, &#39;&#34;&#39;, &#39;both&#39;, &#39;played&#39;, &#39;their&#39;, &#39;parts&#39;, &#39;well&#39;, &#39;.&#39;], &#39;label&#39;: [&#39;pos&#39;]} . # dividing the training set further into a train and validation set train_data, valid_data = train_data.split() . # some important parameters VOCAB_SIZE = 25000 BATCH_SIZE = 64 device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) # build vocabulary for the fields review.build_vocab(train_data, max_size=VOCAB_SIZE) sentiment.build_vocab(train_data) # create iterators for the dataset. iterators enable looping through the dataset easily train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits( (train_data, valid_data, test_data), batch_size = BATCH_SIZE, device = device) . x = next(iter(train_iterator)) print(x.text.shape) print(x.label.shape) . torch.Size([64, 1201]) torch.Size([64]) . Implementing the model . Let&#39;s begin by understanding the layers that are going to be used in this model. We need to know 3 things about each layer in PyTorch - . parameters : used to instantiate the layer. These are the keyword args required to create an object of the class. | inputs : tensors passed to instantiated layer during model.forward() call | outputs : output of the layer | . Embedding layer (nn.Embedding) . This layer acts as a lookup table or a matrix which maps each token to its embedding or feature vector. This module is often used to store word embeddings and retrieve them using indices. . Parameters . num_embeddings: size of vocabulary of the dataset. Number of words in the vocab. | embedding_dim : size of embedding vector for each word. 300 for word2vec. Each word will be mapped to a 300 (say) dimensional vector | . Inputs and outputs . Embedding layer can accept tensors of aribitary shape, denoted by [ * ] and the output tensor&#39;s shape is [ * ,H], where H is the embedding dimension of the layer. For example in case of sentiment analysis, the input will be of shape [batch_size, seq_len] and the output shape will be [ batch_size, seq_len, embedding_dim ]. Intuitively, it replaces each word of each example in the batch by an embedding vector. . LSTM Layer (nn.LSTM) . Parameters . input_size : The number of expected features in input. This means the dimension of the feature vector that will be input to an LSTM unit. For most NLP tasks, this is the embedding_dim because the words which are the input are represented by a vector of size embedding_dim. | hidden_size : Number of features you want the LSTM to learn about the pattern of your data. | num_layers : Number of layers in the LSTM network. If num_layers = 2, it means that you&#39;re stacking 2 LSTM layers. The input to the first LSTM layer would be the output of embedding layer whereas the input for second LSTM layer would be the output of first LSTM layer. | batch_first : If True then the input and output tensors are provided as (batch_size, seq_len, feature). | dropout : If provided, applied between consecutive LSTM layers except the last layer. | bidirectional : If True, it becomes a bidirectional LSTM. That is it reads the sequence from both the directions. The forward direction starts from $x_{0}$ and goes till $x_{n}$ and backward direction goes from $x_{n}$ to $x_{0}$. | . seq_len mentioned above is the length of the input sentence. This will be the same for all the examples within a single batch. For the rest of this post we are going to take batch_first = True . Inputs . input : Shape of tensor is [batch_size, seq_len input_size] if batch_first = True. This is usually the output from the embedding layer for most NLP tasks. | h_0 : [batch_size, num_layers * num_directions, hidden_size] Tensor containing initial hidden state for each element in batch. | c_0 : [batch_size, num_layers * num_directions, hidden_size] Tensor containing initial cell state for each element in batch. | . Outputs . output : [batch_size, seq_len, num_directions * hidden_size] Tensor containing the output features (h_t) from the last layer of the LSTM, for each t. | h_n : [num_layers * num_directions, batch, hidden_size]: tensor containing the hidden state for t = seq_len. | c_n : [num_layers * num_directions, batch, hidden_size]: tensor containing the cell state for t = seq_len. | . Understanding the outputs of the LSTM can be a bit difficult initially. The following diagram clearly explains what each of the outputs mean. The following figure shows a general case of LSTM implementation. . The horizontal axis or the time axis determines the sequence length or the inputs at various time-steps. The vertical axis determines how many LSTM layers have been stacked together. Beginning from the first layer at the bottom, adding each layer increases the depth of the network. The number of layers are denoted by w in this figure. | . As evident, for each time-step t the LSTM unit takes in $h_{t-1}$, $c_{t-1}$ and $x_{t}$ and gives $h_{t}$ and $c_{t}$. The newly calculated $h_{t}$ and $c_{t}$ are passed to next LSTM unit as hidden and cell state of the sequnce seen so far. Simultaneously $h_{t}$ is also passed as the output for that time-step. This is used as the input for stacked layers above the current layer and finally to calculate predictions. Therefore the output of LSTM layer actually contains the hidden states of all time-steps passed through all the layers. Basically, it holds ( $h^{w}_{1}$, $h^{w}_{2}$, ... $h^{w}_{n}$). | . The final hidden and cell states denoted by $h_{n}$ and $c_{n}$ for all the layers are stacked in (h_n, c_n) output of the LSTM layer. Therefore (h_n, c_n) hold (($h^{1}_{n}$, $c^{1}_{n}$) , ($h^{2}_{n}$, $c^{2}_{n}$) ... ($h^{w}_{n}$, $c^{w}_{n}$)) where w is number of layers stacked in the LSTM. | . INPUT_SIZE = len(review.vocab) HIDDEN_SIZE = 128 EMBEDDING_DIM = 100 DROPOUT = 0.4 NUM_LAYERS = 1 BIDIRECTIONAL = False BATCH_SIZE = 64 OUTPUT_DIM = 1 . class SentimentLSTM(nn.Module): def __init__(self, input_size, hidden_size, embedding_dim, dropout, num_layers, output_dim): super().__init__() self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_dim) self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) self.dropout = nn.Dropout(p=dropout) self.linear = nn.Linear(in_features=hidden_size, out_features=output_dim) def forward(self, x): # x = [batch_size, seq_len] = [64, seq_len] as seq_len depends on the batch. embed = self.embedding(x) # embed = [batch_size, seq_len, embedding_dim] = [64, seq_len, 100] # These can be intuitively interpreted as: each example in the batch # has a length of seq_len and each word in the sequence is represented # by a vector of size 100. output, (hidden, cell) = self.lstm(embed) # output = [batch_size, seq_len, hidden_size] = [64, seq_len, 128] # hidden = [num_layers*num_directions, batch_size, hidden_size] = [1, 64, 128] # cell = [num_layers*num_directions, batch_size, hidden_size] = [1, 64, 128] # output is the concatenation of the hidden state from every time step, # whereas hidden is simply the final hidden state. # We verify this using the assert statement. output = output.permute(1,0,2) # hidden = [1, 64, 128] # output = [seq_len, 64, 128] assert torch.equal(output[-1,:,:], hidden.squeeze(0)) preds = self.linear(output[-1,:,:]) # preds = [64, 1] return preds . model = SentimentLSTM(INPUT_SIZE, HIDDEN_SIZE, EMBEDDING_DIM, DROPOUT, NUM_LAYERS, OUTPUT_DIM) . Training the model . A basic training loop involves the following steps . forward pass, i.e. multiplication of inputs with randomly initialized weights and carrying this on for all the layers. | calculate the loss of the predictions with the given target/ground truth. | calculate the gradient of the loss function and propagate it backwards to calculate gradients of all the layers | updating the weights of all the layers using the gradients so that the objective/loss function converges | . def accuracy(preds, y): &#39;&#39;&#39; Returns accuracy of the model.&#39;&#39;&#39; rounded_preds = torch.round(torch.sigmoid(preds)) correct = (rounded_preds == y).float() acc = correct.sum() / len(correct) return acc . # define the loss function criterion = nn.BCEWithLogitsLoss() # determine the gradient descent algorithm to be used for updating weights optimizer = optim.SGD(model.parameters(), lr = 1e-3) . # put model and loss on GPU model = model.to(device) criterion = criterion.to(device) . The following function performs the training and evaluation simultaneously. Each line has been well documented. . def fit(model, criterion, optimizer, train_iterator, valid_iterator): # define number of epochs epochs = 3 # go through each epoch for epoch in range(epochs): # put the model in training mode model.train() # initialize losses and accuaracy for every epoch train_loss = 0. train_acc = 0. valid_loss = 0. valid_acc = 0. print(&quot;Epoch: &quot;, epoch) # go through each batch from the dataset for batch in train_iterator: # calculate model predictions. squeeze(1) is done because the output of model is [64,1]. # criterion expects it to be of dimension [64]. preds = model(batch.text).squeeze(1) # calculate loss for the batch loss = criterion(preds, batch.label) # add batch loss to total loss for the epoch train_loss += loss.item() # calculate accuracy for the batch train_acc += accuracy(preds, batch.label).item() # backward prop for loss function and calc gradients of all the layers in the net loss.backward() # update the weights optimizer.step() # make the gradients zero before next step so that they don&#39;t accumulate optimizer.zero_grad() # print recorded results. Divide the total epoch loss/accuracy by the number of examples. print(&#39;Training loss: &#39;,train_loss / len(train_iterator)) print(&#39;Training accuracy: &#39;, train_acc / len(train_iterator)) # put the model in evaluation mode model.eval() # ensures that gradients are not calculated. Takes less time. with torch.no_grad(): # loop through the valid iterator for batch in valid_iterator: preds = model(batch.text).squeeze(1) loss = criterion(preds, batch.label) valid_loss += loss.item() valid_acc += accuracy(preds, batch.label).item() print(&#39;Validation loss: &#39;, valid_loss / len(valid_iterator)) print(&#39;Validation accuracy: &#39;, valid_acc / len(valid_iterator)) print(&#39;-&#39;) . fit(model, criterion, optimizer, train_iterator, valid_iterator) . Epoch: 0 Training loss: 0.6931361860602442 Training accuracy: 0.5035274374659044 Validation loss: 0.6933359392618729 Validation accuracy: 0.4931585452819275 - Epoch: 1 Training loss: 0.6932326938114027 Training accuracy: 0.5008553832116789 Validation loss: 0.693308207948329 Validation accuracy: 0.4962040961293851 - Epoch: 2 Training loss: 0.6932031631904797 Training accuracy: 0.5027046403745665 Validation loss: 0.6933137026883788 Validation accuracy: 0.4962040961293851 - . References and Acknowledgements . https://pytorch.org/docs/stable/nn.html | https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm | https://github.com/udacity/deep-learning-v2-pytorch | https://github.com/bentrevett |",
            "url": "https://kushalj001.github.io/black-box-ml/lstm/pytorch/torchtext/nlp/sentiment-analysis/2020/01/10/Building-Sequential-Models-In-PyTorch.html",
            "relUrl": "/lstm/pytorch/torchtext/nlp/sentiment-analysis/2020/01/10/Building-Sequential-Models-In-PyTorch.html",
            "date": " • Jan 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Understanding LSTMs",
            "content": "Introduction . Why another blog post on LSTMs? . LSTMs or the Long Short Term Memory have been around for a long time and there are many resources that do a great job in explaining the concept and its working in detail. So why another blog post? Multiple reasons. First, as a personal resource. I have read many blog posts on LSTMs and most of the times the same ones simply because I tend to forget the details after sometime. Clearly, my brain cannot handle long-term dependencies. Second, I am trying to put a lot of details in this post. I have come across blogs that explain LSTMs with the help of equations, some do it with text only and some with animations (hands down the best ones). In this post, I have attempted to include explanations for each component, equations, figures and an implementation of the LSTM layer from scratch. I hope this helps everyone get a wholesome understanding of the topic. . Recurrent Neural Nets . RNNs are one of the key flavours of deep neural networks. Unlike artificial neural networks that have multiple layers of neurons stacked one after another, it is not easily evident as to how recurrent nets are deep. Below is the figure of a rolled RNN. . The input $x_{t}$ is processed by the RNN for each time-step t and outputs a hidden state that captures and maintains the information of all the previous time-steps. We&#39;ll see this again as a for loop when we implement an LSTM layer. This for loop is exactly what makes RNNs deep. The unrolled version of the network is more widely used in literature and is shown below: . This deep nature is precisely the reason why such networks cannot practically model long-term dependencies. As the length of the input sequence increases, the number of matrix multiplications within the network increase. The weight updates of the earlier layers suffer as the gradients tend to vanish for them. Intuitively, think of this as multiplying a number less than zero with itself. The values become low exponentially. On the other hand if gradient values are larger than 1, these explode into large numbers that the computer can no longer make sense of. Consider this for intuition: . To deal with such issues, we need a mechanism that enables the networks to forget the irrelevant information and hold on to the relevant one. Enter LSTMs. . Understanding the LSTM cell . Before we get into the abstract details of the LSTM, it is important to understand what the black box actually contains. The LSTM cell is nothing but a pack of 3-4 mini neural networks. These networks are comprised of linear layers that are parameterized by weight matrices and biases. The values of these weights are learnt by backpropagation. The following figure shows an LSTM cell with labelled gates and all the computations that take place inside the cell. Each cell has 3 inputs: the current token $x_{t}$, previous hidden state $h_{t-1}$ and the previous cell state $c_{t-1}$ and 2 outputs: the updated hidden state $h_{t}$ and cell state $c_{t}$. . The forget gate . This gate takes in the current input $x_{t}$ and the previous hidden state $h_{t-1}$, multiplies them with a weight matrix of the forget gate $W_{f}$ (also adds a bias $b_{f}$) and applies a sigmoid activation. From an implementational point of view, $W_{f}$ and $b_{f}$ are the values associated with a simple linear layer. The sigmoid function restricts the input values between 0 and 1. This output is then multiplied with the cell state. Intuitively, given the current input this gate tells the cell to remove or forget some information if the sigmoid output is close to 0 and keep the information if the sigmoid output is close to 1. The equation for this gate is: . The input and the cell gate . The input gate is used to decide that given the current input what information is important and should be stored in the cell state. The calculations of this gate are similar to those of the forget gate. The cell gate and the input gate work closely together to perform a very specific function. This function is to update the previous cell state. To do so, the cell gate proposes an update candidate. You can think of this update candidate as a proposed new cell state. To calculate the cell gate output, a tanh activation is used (more on this later). The equation for the cell state is: . Cell update . We cannot simply replace the new proposed cell state and eliminate the previous cell state. This is because the previous state might contain some important information about the previous inputs the LSTM layer has seen. This is basically the main purpose of recurrent networks - to hold on to relevant information from the past. Hence, we adopt a very elegant approach to update the cell state. From the previous blocks, we know the following: Given the current input, the forget gate decides what information from the previous cell state can be forgotten. This is done by multiplying the forget gate output $f_{t}$ with the previous cell state $c_{t-1}$. Also, the input gate determines what information from the current input is relevant. Product of $i_{t}$ and the proposed cell state would give us important parts from the proposed cell state. We thus combine these two to yeild the following cell update equation: . Output gate . This gate is used to calculate the hidden state for the next time step. Again the current input $x_{t}$ and previous hidden state $h_{t-1}$ are multiplied by a weight matrix $W_{o}$ and passed through sigmoid activation. To update the hidden state of the cell, it might be a good idea to incorporate some information from the newly updated cell state. Therefore, the result of the output gate $o_{t}$ is multiplied with the updated cell state $c_{t}$ after passing $c_{t}$ through tanh activation (explained later). The equation for this gate is: . Why do we use tanh for calculating cell state? . We left this part rather abruptly while talking about the cell gate. The question of as to why tanh has been preferred over sigmoid and even ReLU has been a hotly debated one. My research led me to multiple sources and reasons. . One of the key reasons as to why tanh is preferred is its range [-1, 1] and the fact that it is zero-centered. These properties enable the neural net to converge faster and hence train faster. Yann LeCun in his paper called Efficient BackProp explains such factors that affect the backpropagation algorithm in neural networks. To understand this consider the following. Assume that all the values in a weight matrix are positive. These weights are updated during backprop by say a factor d which can be positive or negative. As a result, these weights can only all decrease or all increase together for a given input pattern. Thus, if a weight vector must change direction it can only do so by zigzagging which is inefficient and thus very slow. | Another reason for using tanh is the relatively larger value of its derivative. Backprop computations result in multiplication of derivatives of the activation function multiple times depending upon the number of layers in the network. The maximum value for the derivative of a sigmoid function is 0.25 whereas that for tanh is 1. Hence, if the network is reasonably deep, the gradients of sigmoid are more likely to vanish than those of tanh. | . For sigmoid the graph is . . Why do we use tanh while calculating output gate values? . Although this is not that important but in case this question comes to your head, here&#39;s the answer. If you carefully look at the cell update equations, we have already applied a tanh activation in order to calculate the cell update candidate. Yet we again pass the cell state through tanh to get the new hidden state $h_{t}$. This is done to ensure that the values of $h_{t}$ lie between -1 and 1 because there&#39;s a chance that the values of the updated cell state $c_{t}$ might have exceeded 1 in previous additive computation. . Implementation . As pointed out earlier, the LSTM cell is a collection of 4 neural nets. In order to parallelize our computations and make use of a GPU it is better to compute values of the gates all at once. We need to linear layers: one for the current input and one for the previous hidden state. | So in the init method we initialize two linear layers. The out_features value for both these layers is 4 * hidden_dim owing to the number of gates. | You can relate this implementation with the equations above by expanding the multiplication of weights with the inputs. Assume that the weights associated with these layers are $W&#39;_{i}$ for current input and $W&#39;_{h}$ for previous hidden state. We perform the following computation initially and then break this computation into 4 matrices, one for each gate. | . W = $W&#39;_{i}$ $*$ $x_{t}$ + $W&#39;_{h}$ $*$ $h_{t-1}$ + $b&#39;_{i}$ + $b&#39;_{h}$ . The W matrix gets divided into 4 equal tensors $W_{i}$, $W_{f}$, $W_{c}$, $W_{o}$ which have been used in the equations above. This split is performed using torch.chunk. | . The rest of the code for LSTM cell is just converting the equations to code. . import torch from torch import nn . class LSTMCell(nn.Module): def __init__(self, input_dim, hidden_dim): super().__init__() self.input_layer = nn.Linear(in_features=input_dim, out_features=4 * hidden_dim) self.hidden_layer = nn.Linear(in_features=hidden_dim, out_features=4 * hidden_dim) def forward(self, current_input, previous_state): previous_hidden_state, previous_cell_state = previous_state weights = self.input_layer(current_input) + self.hidden_layer(previous_hidden) gates = weights.chunk(4,1) input_gate = torch.sigmoid(gates[0]) forget_gate = torch.sigmoid(gates[1]) output_gate = torch.sigmoid(gates[2]) cell_gate = torch.tanh(gates[3]) new_cell = (forget_gate * previous_cell_state) + (input_gate * cell_gate) new_hidden = output_gate * torch.tanh(new_cell) return new_hidden, (new_hidden, new_cell) . The following snippet basically takes an LSTMCell instance and calculates the output for the input sequence by applying a for loop. This is the same loop we had talked about initially in the post. . class LSTMLayer(nn.Module): def __init__(self, cell, *cell_args): super().__init__() self.cell = cell(*cell_args) def forward(self, input, state): inputs = input.unbind(1) outputs = [] for i in range(len(inputs)): output, state = self.cell(inputs[i], state) outputs += [output] return torch.stack(outputs, dim=1), state . lstm = LSTMLayer(LSTMCell, 100, 100) . Acknowledgements and References . This blog post is merely a combination of a variety of great resources available on the internet. The code is heavily drawn from the fastai foundations notebook by Jeremy Howard who does a great job in explaining the inner workings of each component. Figures are majorly taken from Chris Olah&#39;s evergreen post on LSTMs. All the references and links have been listed below to the best of my knowledge. Thank you! . https://colah.github.io/posts/2015-08-Understanding-LSTMs/ | https://github.com/fastai | https://github.com/emadRad/lstm-gru-pytorch/blob/master/lstm_gru.ipynb | https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21 | https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e | https://www.researchgate.net/figure/5-Activation-functions-in-comparison-Red-curves-stand-for-respectively-sigmoid_fig10_317679065 | https://stats.stackexchange.com/questions/368576/why-do-we-need-second-tanh-in-lstm-cell | https://www.quora.com/In-an-LSTM-unit-what-is-the-reason-behind-the-use-of-a-tanh-activation | https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm | https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function | https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function | http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf |",
            "url": "https://kushalj001.github.io/black-box-ml/lstm/pytorch/gates/vanishing%20gradient/2019/12/28/Understanding-LSTMs.html",
            "relUrl": "/lstm/pytorch/gates/vanishing%20gradient/2019/12/28/Understanding-LSTMs.html",
            "date": " • Dec 28, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Understanding Word Embeddings",
            "content": "Introduction . Words are an inseparable part of human lives. We communicate verbally, write letters and mail, and even think using words. We can do all of this because we have an understanding of the intrinsic meaning that each word has. Computers on the other hand are not good at understanding words and hence natural language. But they are good at processing large matrices/tensors of floating point values. Hence, humans have always tried to encode the words using numbers in some way that the computers can comprehend and work on challenging tasks. This encoding is usually called a word embedding. Its basically a vector that represents various features or characteristics of the word across its dimensions. In this post, I intend to explain in detail the working of two techniques that have been used widely to calculate such word embeddings viz. word2vec and GloVe. There are multiple reasons to write this post. . Most of us know how to use these word embeddings in code and build complex architectures using them. However, we often ignore the details of the model and how these vectors were trained in the first place. It is equally important to understand and appreciate them. | For a long time I had the misconception that word2vec and GloVe are somewhat similar with some tweaks here and there. But there are a lot of fundamental differences in their approaches to calculate word-vectors. | In this post I&#39;ll briefly explain word2vec and then move on to GloVe dive deep into the model details. . Word2Vec . word2vec uses a pseudo neural network to calculate continuous vector representations for words. Why pseudo? Because we don&#39;t actually use the outputs of the trained neural network. The neural network is trained to perform a pseudo or fake task. The task is formulated as done below. . The network is trained using word pairs from large text corpus. For each word in a sentence, we generate word pairs by looking at a fixed number of words before and after the current word (or the input word). This fixed number of words is also known as the window size. This can be 2, 3, 4 or 5. Window size of 3 means that we look at 3 words before and 3 words after the current word $w_{t}$. Which means the context words are ($w_{t-3}$, $w_{t-2}$, $w_{t-1}$, $w_{t+1}$, $w_{t+2}$, $w_{t+3}$). The word-pairs hence generated would be [($w_{t}$,$w_{t-3}$), ($w_{t}$,$w_{t-2}$), ... ($w_{t}$,$w_{t+3}$)]. The word-pairs generated are used to train the network. That is, $w_{t}$ is given as input to the model and the output is one of the words from the context. | . Assume that the vocabulary size of the corpus is 10,000. This means that there are 10,000 unique words in the corpus. The input to the network is a one-hot encoded vector representing the input word. The network has one hidden layer with 300 neurons. The output layer has 10,000 neurons, one for each word, with a softmax activation. No activation is used in the hidden layer. The presence of softmax means that the model will actually output probabilities for 10,000 words. This probability is the probability of the word at that index being the nearbuy or a context word for the input/current word. Intuitively, words that occur near the input word multiple times in the corpus will have a larger probability than others. | For example, consider the word &quot;Obama&quot;. It is more likely to be surrounded by words like &quot;Barack&quot;, &quot;USA&quot;, &quot;President&quot; etc. rather than words like &quot;juice&quot;, &quot;rabbit&quot; etc. The network hence captures various features of the word statistically by looking at the local context of the word. | Now coming back to the pseudo task. Once this network is trained for all the word pairs in the corpus, we simply remove the output layer. The hidden layer is associated with a matrix of size (10,000 X 300), one 300 dimensional vector for each word in the corpus. After backpropagation, the values of this matrix represent our word vectors. So the proposed task was fake because we never used the output layer. We just wanted to learn representations of the words. | . The model discussed above is the skip gram model in which we are predicting the context given the input. The authors also proposed another model in the same paper called continuous bag-of-words (CBOW) model where we predict the input word from the context. A schematic diagram for both these models as given in the paper is shown below . Subsampling . word2vec has been trained on a large corpus extracted from Google News that contains around 100 billion tokens. In such a large corpus, there are bound to exist very high frequency words that contribute very little to the training process. For exxample words like &quot;the&quot;, &quot;and&quot;, &quot;a&quot; etc. might occur in many context windows and hence be a part of many word-pairs. Thus the authors introduced subsampling that deletes words with high frequency from the corpus. Basically, each word is assigned a probability of whether it will be kept or dropped from the text. For example, if &quot;the&quot; is deleted from a sentence, it will generate fewer word-pairs for training and hence reduce training time. . Negative Sampling . word2vec model has 300 X 10,000 weight values in total. For each training sample, all these weights will get tweaked very slightly. This will happen for all the word-pairs generated in our text. That is, only for one word (ground truth) the output should be 1 and for the rest of the thousand words it should be 0. This would make the training very slow and is not even required since each training sample would not affect the a large fraction of weights significantly. Hence, for each training sample, we choose 5 negative words that we are not present in the input word&#39;s context. Weights are tweaked only for the label or the ground truth present in the word pair and these 5 negative words. . Global Vectors (GloVe) . GloVe follows a more principled approach in calculating word-embeddings. The major difference between word2vec and GloVe is that the latter does not use a neural net for the task. The authors develop a strong mathematical model to learn the embeddings. GloVe also overcomes the drawbacks of previous techniques used to calculate word-embeddings. Before word2vec, statistical methods like Latent Semantic Analysis (LSA) were used to approximate embeddings for terms in a document. These methods took into account global count statistics of the dataset. However, the vectors derived from such methods did not capture the meaning of the words like word2vec does and hence performed poorly in tasks like syntactic and semantic similarity and word analogies like &quot;king - queen + woman = man&quot;. Meanwhile word2vec took into consideration the local context of words but failed to account for the global count satistics of the dataset. The main aim of GloVe was to combine the two approaches to learn word-vectors. Quoting from the paper, . The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word representations, and although many such methods now exist, the question still remains as to how meaning is generated from these statistics, and how the resulting word vectors might represent that meaning. . Our model efﬁciently leverages statistical information by training only on the non-zero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. . As highlighted above, GloVe derives its training data by calculating a word-word co-occurence matrix. We&#39;ll show how this is calculated below with a toy example. But its important to understand the difference between this method and that used by word2vec. Word2vec generated training samples by forming word-pairs for all the words in a local context window completely ignoring the global statistics of the words that occur in the context window. . Co-occurence matrix . Consider the sentence - &quot;Winter is coming and it is here.&quot;. We construct a co-occurence matrix for this sentence with a context window size of 2. For &quot;winter&quot;, the context words are &quot;is&quot; and &quot;coming&quot;. Hence, we put 1 in the respective box. We also assume that the word itself is part of its context and hence increment the count in the diagonal of the matrix. . Derivation . We&#39;ll now get into the derivation of the GloVe model. The following section contains equations taken from the paper directly. Although I&#39;ll try my best to explain the significance and meaning of each equation, the derivation can get a bit daunting. Let&#39;s start by defining some notations. . Let the word-word co-occurrence table be denoted by $X$. Each entry in the table $X_{ij}$ denotes the number of times word $j$ occurs in the context of word $i$. . | Let $X_{i}$ be the number of times any word appears in the context of word $i$. That is the total number of distinct words throughout the corpus that appear in word $i$&#39;s context. Therefore, $X_{i}$ = $ sum_{k}$$X_{ik}$ where $k$ are the different words that appear in $i$&#39;s context throughout the dataset. . | Let $P_{ij}$ = $P (j | i)$ = $X_{ij}$ / $X_{i}$. This defines the probability that word $j$ appears in the context of word $i$. $P_{ij}$ is also called as the co-occurrence probability. . By calculating this probability, we are actually incorporating the global statistics of the dataset. Word2vec ignores the denominator of $P_{ij}$ calculated above. . | . Consider the following calculations for a particular dataset. The following example will help us understand how can we derive meaning of words by the ratio of probabilities calculated above. . The above matrix shows raw probabilities and ratios for two words viz. ice and steam. These words are topically concerned with the thermodynamic phases of water. The first 2 rows show calculations of the the probability $P_{ik}$ where $k$ are some words from the dataset called probe words. The last row shows the ratio of probabilities calculated in the first two rows. The hypothesis is as follows: . The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$. . Among the probe words, &quot;solid&quot; is closely related to the word &quot;ice&quot; (word $i$) and &quot;gas&quot; is closely related to &quot;steam&quot; (word $j$). As can be seen in the table above, the ratio $P_{ik}$/$P_{jk}$ is much greater than 1 (8.9) for words that are similar to &quot;ice&quot; ($i$) and much smaller than 1 (0.085) for words that are related to &quot;steam&quot; ($j$). Moreover, for words that are equally related to both &quot;ice&quot; and &quot;steam&quot; like &quot;water&quot; and words that not related to any off the two words (like &quot;fashion&quot;) have the ratio around 1. Therefore, the hypothesis stated above is true in the sense that ratios of co-occurrence probabilities gives a better sense about the meaning of a word than raw probabilities. . Therefore, this seems to be a good starting point for learning word vectors. We need to learn a function $F$, parameterized by 3 word vectors $w_{i}$, $w_{j}$ and $ tilde w_{k}$ such that, | . $w_{i}$, $w_{j}$ and $ tilde w_{k}$ belong to $R^{ d}$. $ tilde w_{k}$ represent separate context words or the probe words as discussed above. . There are a large number of possibilities for the function $F$. So lets impose some constraints on the model. The purpose of this model is to learn embeddings or feature vectors for words. Once learned, these vectors can be projected into a vector space just like we project 2 and 3 dimensional vectors in a cartesian plane. These vector spaces are inherently linear in nature because a vector is ultimately just a line. The above discussion leads us to our first constraint. The most common way to compare two linear structures is to calculate their difference. Thus we can restrict our function as, | . In the above equation, the right hand side is a ratio which is scalar and the arguments are vectors. While, learning a complex function $F$ that does the above mapping is possible but that would introduce non-linearities in our model because that&#39;s what neural nets do. We don&#39;t wish to obscure the linear structure that our model captures. Therefore, we take a dot product of the arguments on the left hand side to make it a scalar. | . The next constraint is that of symmetry. The distinction between a word and the context words is arbitary and hence they can be exchanged with each other. This is easy to understand. If &quot;water&quot; and &quot;gas&quot; can be used as context words for &quot;steam&quot;, then &quot;steam&quot; can also be used as a context word for &quot;water&quot; and &quot;gas&quot;. This symmetry is also evident in our co-occurrence matrix $X$ which is symmetric about the diagonal. Symmetry implies that $w$ $&lt;-&gt;$ $ tilde w$. To restore this symmetry, we require that the function $F$ is a homomorphism between groups $(R, +)$ and $(R_{&gt;0}, *)$. | . Let&#39;s define a group and homomorphism. A group $(G, *)$ is set of elements, that is closed under a particular operation $*$. That is if $x$ and $y$ belong to $G$ then, $z$ = $x$ $y$ also belongs to $G$. Each group has an inverse such that for all $x$, $x^{-1}x$ = $e$. Also, for all elements in a group, there exists an identity such that $xe = ex$ = $x$. Function $F$ is a homomorphism between 2 groups $(G, )$ and $(H, @)$ such that $F:G -&gt;H$ if $F(xy) = F(x) @ F(y)$. . Coming back to the equation of the model, homomorphism for $F$ can be explained as . begin{equation} F((w^{T}_{i} - w^{T}_{j}) tilde w_{k}) = F(w^{T}_{i} tilde w_{k} + (-w^{T}_{j} tilde w_{k})) end{equation} begin{equation} F(w^{T}_{i} tilde w_{k} + (-w^{T}_{j} tilde w_{k})) = F(w_{i}^{T} tilde w_{k}) * F(-w_{j}^{T} tilde w_{k}) end{equation} begin{equation} F(w_{i}^{T} tilde w_{k}) * F(-w_{j}^{T} tilde w_{k}) = F(w_{i}^{T} tilde w_{k}) * F(w_{j}^{T} tilde w_{k})^{-1} end{equation} The above equation gives the following result, . The above equation is solved by equation $(3)$, . And the exponential function is the solution for equation $(4)$, therefore we have the following, . . he symmetry is still not restored because of the $log(X_{i})$ term on the right hand side. Since, this term is independent of $k$, we can absorb it into bias $b_{i}$. Also introducing a bias $ tilde b_{k}$ for $ tilde w_{k}$, we finally have the following equation, . . The above equation gives us a cost function for a new weighted least squares regression model. The weighting function $f(X_{ij})$ takes into consideration the co-occurrences of words. . References and Acknowledgements . This post is heavily derived from the respective research papers of the techniques. Figures and equations are taken from the papers and blogs referenced below. Chris McCormick&#39;s posts on word2vec are still the best explanations for the topic and I strongly recommend the reader to read those. . https://nlp.stanford.edu/pubs/glove.pdf | https://arxiv.org/abs/1301.3781 | https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf | http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ | http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ | https://math.stackexchange.com/questions/2580647/what-does-homomorphism-mean-in-the-glove-paper/2580648 | https://datascience.stackexchange.com/questions/27042/glove-vector-representation-homomorphism-question | https://www.youtube.com/watch?v=g7L_r6zw4-c | https://towardsdatascience.com/word-embedding-part-ii-intuition-and-some-maths-to-understand-end-to-end-glove-model-9b08e6bf5c06 |",
            "url": "https://kushalj001.github.io/black-box-ml/word2vec/glove/word-embeddings/nlp/2019/11/13/Understanding-Word-Embeddings.html",
            "relUrl": "/word2vec/glove/word-embeddings/nlp/2019/11/13/Understanding-Word-Embeddings.html",
            "date": " • Nov 13, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an incoming MS CS grad student at University of California, San Diego. I completed my undergrad in Computer Science from Mumbai University, India and have been working on ML/NLP problems for over 3 years now. I am currently working as an applied research scientist at NeuralSpace AI. I am an avid football and Arsenal follower. I also like to read about Indian history and finding out about events and various philosophies that have shaped the world we currently live in. My research interests lie broadly in NLP. I have briefly explained my work and research experience below. . Publications/Research . Indic-Transformers: An Analysis of Transformer Language Models for Indian Languages . Accepted and presented at ML-RSA@NeurIPS 2020. | Presents an exhaustive analysis and evaluation of transformer-based models on Indian languages. | Analyzes three experimental setups which involve pre-training the language models, fine-tuning multilingual models with data from one language and directly evaluating multilingual models on Indian languages. | . Stance Detection using Transformer Architectures and Temporal Convolutional Networks . Accepted and presented at IC4S 2019 (http://www.ic4s.org/). Published in Springer series “Advances in Computer, Communication and Computational Sciences”. | Implemented task-specific LSTM architectures with BERT and XLNet weights using independent and conditional encoding of input text. Also developed a novel architecture that uses Temporal CNNs for stance detection. | . Question Answering using Pytorch . Implemented 3 important papers for the task of Question Answering viz. DrQA, BiDAF, and QANet. | Each implementation is in the form of a tutorial with detailed explanations about each component/layer. | Has been starred 165 times on Github. | . Work Experience . NeuralSpace (October 2020-Present)- Applied Research Scientist . I am currently helping the company build an NLP SaaS platform from scratch with focus on low-resource languages. Our platform currently supports 55 languages which includes indic, south-east asian, african, scandinavian and middle-east languages. | During this, I worked on all the components that are required to build a modern ML/NLP system, right from basic text processing, POS-Tagging to distributed training of models and deploying them at scale. | Closely worked on natural language understanding (NLU) systems. If your business involves NLU use-cases, reach out to NeuralSpace https://neuralspace.ai for a demo. We are continuously adding a lot of features and have a very exciting plan ahead of us. | . NeuralSpace (August 2019-October 2020)- Research Intern . Developed a document analysis library from scratch. Involves a pipeline that takes in an image and returns all named-entities present in the image. Currently supports template-matching-based entity recognition for any type of PDF document. Also supports the analysis of documents with tabular data. | Developed a multilingual text summarization model that works for 21 languages including 11 Indian languages.  | Worked on machine translation of low resource languages to English. | . Neebal Technologies (June 2018-August 2018)- Software Engineering Intern . Integrated a handwriting recognition engine into an android app. | Worked with various Android APIs like BLE, JobScheduler, etc.  | . Unicode (January 2018-June 2020)- Python/ Android developer . Since my sophomore year, I have been a part of a tiered student organization at my college called Unicode where we develop open source software projects. | As a project guide, I mentored sophomores to build an android-based attendance application for my department.  | .",
          "url": "https://kushalj001.github.io/black-box-ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kushalj001.github.io/black-box-ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}